{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gzip\n",
    "\n",
    "\n",
    " \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Task B\n",
    "\n",
    "For this analysis we will be creating a linear regression model to predict the hardness score. The hardness score represent the difficulty of the question being asked. Therefore, it is independent of the main dataset (conversation data) or the response embedding data in the auxilliary dataset. To predict the hardness score we will make the assumption that each row of embedding data from the prompt embeddings corresponds to each row of the topic_and_hardness dataset.\n",
    "\n",
    "The problem statement for task B states that we must use linear regression to determine the hardness score. Therefore, any linear model from the sklearn library would meet this criteria. Therefore, we will perform an analysis and return the results of the best performing models. We will then select the top two models for hyperparameter tuning to create our final models, then the best model of the tuned models will be the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Datasets\n",
    "\n",
    "# Prompt embedding Data -- we will use this data in the \"Embedding Data\" section\n",
    "prompt_embeddings = np.load(\n",
    "    \"../training_data/chatbot-arena-prompts-embeddings.npy\"\n",
    ")\n",
    "\n",
    "# Topic Modeling and Hardness Score Data -- we will use this data in the \"Topic Modeling and Hardness Score Data\" section\n",
    "topic_and_hardness = pd.read_json(\n",
    "    \"../training_data/chatbot-arena-gpt3-scores.jsonl.gz\",\n",
    "    lines=True,\n",
    "    compression=\"gzip\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_247</th>\n",
       "      <th>feature_248</th>\n",
       "      <th>feature_249</th>\n",
       "      <th>feature_250</th>\n",
       "      <th>feature_251</th>\n",
       "      <th>feature_252</th>\n",
       "      <th>feature_253</th>\n",
       "      <th>feature_254</th>\n",
       "      <th>feature_255</th>\n",
       "      <th>feature_256</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.123763</td>\n",
       "      <td>-0.117352</td>\n",
       "      <td>0.045677</td>\n",
       "      <td>0.015849</td>\n",
       "      <td>0.085833</td>\n",
       "      <td>-0.027624</td>\n",
       "      <td>0.003787</td>\n",
       "      <td>-0.08236</td>\n",
       "      <td>0.088994</td>\n",
       "      <td>-0.00169</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024708</td>\n",
       "      <td>-0.114236</td>\n",
       "      <td>0.034814</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.015938</td>\n",
       "      <td>0.059344</td>\n",
       "      <td>-0.162139</td>\n",
       "      <td>-0.024396</td>\n",
       "      <td>-0.03724</td>\n",
       "      <td>-0.043807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0  -0.123763  -0.117352   0.045677   0.015849   0.085833  -0.027624   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_247  feature_248  \\\n",
       "0   0.003787   -0.08236   0.088994    -0.00169  ...    -0.024708    -0.114236   \n",
       "\n",
       "   feature_249  feature_250  feature_251  feature_252  feature_253  \\\n",
       "0     0.034814     0.006923     0.015938     0.059344    -0.162139   \n",
       "\n",
       "   feature_254  feature_255  feature_256  \n",
       "0    -0.024396     -0.03724    -0.043807  \n",
       "\n",
       "[1 rows x 256 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the prompt embeddings data into a pandas dataframe\n",
    "num_features = prompt_embeddings.shape[1]\n",
    "column_names = [f\"feature_{i+1}\" for i in range(num_features)]\n",
    "df_prompt = pd.DataFrame(prompt_embeddings, columns = column_names)\n",
    "df_prompt.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>openai_scores_raw_choices_nested</th>\n",
       "      <th>topic_modeling_1</th>\n",
       "      <th>score_reason_1</th>\n",
       "      <th>score_value_1</th>\n",
       "      <th>topic_modeling_2</th>\n",
       "      <th>score_reason_2</th>\n",
       "      <th>score_value_2</th>\n",
       "      <th>topic_modeling_3</th>\n",
       "      <th>score_reason_3</th>\n",
       "      <th>score_value_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58210e39b3fd4441a2bd4a518bb44c2d</td>\n",
       "      <td>What is the difference between OpenCL and CUDA?</td>\n",
       "      <td>[{'finish_reason': 'stop', 'index': 0, 'logpro...</td>\n",
       "      <td>Technical Comparison</td>\n",
       "      <td>This prompt requires the AI to accurately comp...</td>\n",
       "      <td>9</td>\n",
       "      <td>Software Comparison</td>\n",
       "      <td>This prompt assesses the AI's factual accuracy...</td>\n",
       "      <td>8</td>\n",
       "      <td>Comparison, Technology</td>\n",
       "      <td>This prompt requires the AI to demonstrate kno...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question_id  \\\n",
       "0  58210e39b3fd4441a2bd4a518bb44c2d   \n",
       "\n",
       "                                            prompt  \\\n",
       "0  What is the difference between OpenCL and CUDA?   \n",
       "\n",
       "                    openai_scores_raw_choices_nested      topic_modeling_1  \\\n",
       "0  [{'finish_reason': 'stop', 'index': 0, 'logpro...  Technical Comparison   \n",
       "\n",
       "                                      score_reason_1 score_value_1  \\\n",
       "0  This prompt requires the AI to accurately comp...             9   \n",
       "\n",
       "      topic_modeling_2                                     score_reason_2  \\\n",
       "0  Software Comparison  This prompt assesses the AI's factual accuracy...   \n",
       "\n",
       "  score_value_2        topic_modeling_3  \\\n",
       "0             8  Comparison, Technology   \n",
       "\n",
       "                                      score_reason_3 score_value_3  \n",
       "0  This prompt requires the AI to demonstrate kno...             9  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_and_hardness.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question_id', 'prompt', 'topic_modeling_1', 'score_value_1',\n",
       "       'topic_modeling_2', 'score_value_2', 'topic_modeling_3',\n",
       "       'score_value_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_and_hardness = topic_and_hardness.drop(columns = [\"score_reason_1\", \n",
    "                                                        \"score_reason_2\", \n",
    "                                                        \"score_reason_3\",\n",
    "                                                        \"openai_scores_raw_choices_nested\"], axis = 1)\n",
    "topic_and_hardness.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for prompt length\n",
    "topic_and_hardness[\"prompt_length\"] = topic_and_hardness[\"prompt\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the score data\n",
    "for i in range(3):\n",
    "    topic_and_hardness[f\"score_value_{i+1}\"] = topic_and_hardness[f\"score_value_{i+1}\"].apply(\n",
    "        # Clean nested list element into an int\n",
    "        lambda x: x[0][0] if isinstance(x, list) and len(x) == 1 and isinstance(x[0], list) and len(x[0]) == 1 else (\n",
    "            # Else clean the list element into an int\n",
    "            x[0] if isinstance(x, list) and len(x) == 1 and isinstance(x[0], (int, float)) \n",
    "            # Else leave it alone\n",
    "            else x\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: score_value_1, dtype: float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_and_hardness[\"score_value_1\"][\n",
    "    topic_and_hardness[\"score_value_1\"].apply(lambda x: isinstance(x, list))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the topic modeling data\n",
    "for i in range(3):\n",
    "    topic_and_hardness[f\"topic_modeling_{i+1}\"] = topic_and_hardness[f\"topic_modeling_{i+1}\"].apply(\n",
    "        # Clean list element into an string\n",
    "        lambda x: \", \".join(x) if isinstance(x, list) else x\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: topic_modeling_3, dtype: object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_and_hardness[\"topic_modeling_3\"][\n",
    "    topic_and_hardness[\"topic_modeling_3\"].apply(lambda x: isinstance(x, list))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_modeling_1</th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>score_value_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3D Modeling</td>\n",
       "      <td>42</td>\n",
       "      <td>7.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3D Modeling</td>\n",
       "      <td>57</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3D Modeling</td>\n",
       "      <td>102</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3D Modeling</td>\n",
       "      <td>103</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3D Printing</td>\n",
       "      <td>158</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21263</th>\n",
       "      <td>troubleshooting, technology</td>\n",
       "      <td>39</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21264</th>\n",
       "      <td>urban forestry</td>\n",
       "      <td>71</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21265</th>\n",
       "      <td>videogames, recommendation</td>\n",
       "      <td>54</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21266</th>\n",
       "      <td>weather forecast</td>\n",
       "      <td>122</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21267</th>\n",
       "      <td>worldbuilding, storytelling</td>\n",
       "      <td>185</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21268 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  topic_modeling_1  prompt_length  score_value_1\n",
       "0                      3D Modeling             42       7.333333\n",
       "1                      3D Modeling             57       7.000000\n",
       "2                      3D Modeling            102       8.000000\n",
       "3                      3D Modeling            103       9.000000\n",
       "4                      3D Printing            158       7.000000\n",
       "...                            ...            ...            ...\n",
       "21263  troubleshooting, technology             39       7.000000\n",
       "21264               urban forestry             71       8.000000\n",
       "21265   videogames, recommendation             54       7.000000\n",
       "21266             weather forecast            122       7.000000\n",
       "21267  worldbuilding, storytelling            185       9.000000\n",
       "\n",
       "[21268 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_1 = topic_and_hardness.groupby([\"topic_modeling_1\", \"prompt_length\"])[\"score_value_1\"].mean().reset_index()\n",
    "group_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the average hardness score for each topic for each response\n",
    "#topic_1_avg_scores = topic_and_hardness.groupby([\"topic_modeling_1\"])[\"score_value_1\"].mean().to_dict()\n",
    "topic_2_avg_scores = topic_and_hardness.groupby([\"topic_modeling_2\"])[\"score_value_2\"].mean().to_dict()\n",
    "topic_3_avg_scores = topic_and_hardness.groupby([\"topic_modeling_3\"])[\"score_value_3\"].mean().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>assumed_score_2</th>\n",
       "      <th>assumed_score_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47</td>\n",
       "      <td>6.75</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>7.50</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>7.50</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prompt_length  assumed_score_2  assumed_score_3\n",
       "0             47             6.75         8.666667\n",
       "1             49             7.50         8.000000\n",
       "2             32             7.50         2.000000\n",
       "3             35             8.00         8.000000\n",
       "4             17             2.00         2.000000"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.DataFrame()\n",
    "df_train[\"prompt_length\"] = topic_and_hardness[\"prompt_length\"]\n",
    "#df_train[\"assumed_score_1\"] = topic_and_hardness[\"topic_modeling_1\"].map(topic_1_avg_scores)\n",
    "df_train[\"assumed_score_2\"] = topic_and_hardness[\"topic_modeling_2\"].map(topic_2_avg_scores)\n",
    "df_train[\"assumed_score_3\"] = topic_and_hardness[\"topic_modeling_3\"].map(topic_3_avg_scores)\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25282, 265)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the training data with the prompt embeddings\n",
    "# Create the modelling data\n",
    "df_train = pd.concat([df_prompt, df_train], axis = 1)\n",
    "# Concatenate the score values\n",
    "data = topic_and_hardness[[\"score_value_1\", \"score_value_2\", \"score_value_3\"]].copy()\n",
    "df_train = pd.concat([data, df_train], axis = 1)\n",
    "data_2 = topic_and_hardness[[\"topic_modeling_1\", \"topic_modeling_2\", \"topic_modeling_3\"]].copy()\n",
    "df_train = pd.concat([data_2, df_train], axis = 1)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25256, 265)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with missing values\n",
    "df_train.drop(df_train[df_train[\"score_value_1\"].isnull() == True].index, inplace = True)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the valid prompt embeddings back into an array\n",
    "embedding_columns = [col for col in df_train.columns if col.startswith(\"feature_\")]\n",
    "valid_prompt_embeddings = df_train[embedding_columns].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_modeling_1</th>\n",
       "      <th>topic_modeling_2</th>\n",
       "      <th>topic_modeling_3</th>\n",
       "      <th>score_value_1</th>\n",
       "      <th>score_value_2</th>\n",
       "      <th>score_value_3</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_250</th>\n",
       "      <th>feature_251</th>\n",
       "      <th>feature_252</th>\n",
       "      <th>feature_253</th>\n",
       "      <th>feature_254</th>\n",
       "      <th>feature_255</th>\n",
       "      <th>feature_256</th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>assumed_score_2</th>\n",
       "      <th>assumed_score_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Technical Comparison</td>\n",
       "      <td>Software Comparison</td>\n",
       "      <td>Comparison, Technology</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.123763</td>\n",
       "      <td>-0.117352</td>\n",
       "      <td>0.045677</td>\n",
       "      <td>0.015849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.015938</td>\n",
       "      <td>0.059344</td>\n",
       "      <td>-0.162139</td>\n",
       "      <td>-0.024396</td>\n",
       "      <td>-0.03724</td>\n",
       "      <td>-0.043807</td>\n",
       "      <td>47</td>\n",
       "      <td>6.75</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 265 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic_modeling_1     topic_modeling_2        topic_modeling_3  \\\n",
       "0  Technical Comparison  Software Comparison  Comparison, Technology   \n",
       "\n",
       "   score_value_1  score_value_2  score_value_3  feature_1  feature_2  \\\n",
       "0            9.0            8.0            9.0  -0.123763  -0.117352   \n",
       "\n",
       "   feature_3  feature_4  ...  feature_250  feature_251  feature_252  \\\n",
       "0   0.045677   0.015849  ...     0.006923     0.015938     0.059344   \n",
       "\n",
       "   feature_253  feature_254  feature_255  feature_256  prompt_length  \\\n",
       "0    -0.162139    -0.024396     -0.03724    -0.043807             47   \n",
       "\n",
       "   assumed_score_2  assumed_score_3  \n",
       "0             6.75         8.666667  \n",
       "\n",
       "[1 rows x 265 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing Libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Model Building Libraries\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,\n",
    "    Ridge, \n",
    "    Lasso,\n",
    "    ElasticNet,\n",
    "    SGDRegressor,\n",
    "    BayesianRidge,\n",
    "    ARDRegression\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X and y\n",
    "X = df_train.drop(columns = [\"score_value_1\", \n",
    "                             \"score_value_2\", \n",
    "                             \"score_value_3\", \n",
    "                             \"topic_modeling_1\",\n",
    "                             \"topic_modeling_2\",\n",
    "                             \"topic_modeling_3\"])\n",
    "y = df_train[\"score_value_1\"]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train_1, y_test_1 = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "\n",
    "# Scale the data.\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation MSE on Training Data:\n",
      "LinearRegression: 1.8087580633521618\n",
      "Ridge: 1.8087495020499365\n",
      "Lasso: 3.082820353682629\n",
      "ElasticNet: 2.406350268373025\n",
      "SGDRegressor: 1.8712538153226486\n",
      "BayesianRidge: 1.805899958380353\n",
      "ARDRegression: 1.8099263905939207\n",
      "\n",
      "MSE on Testing Data:\n",
      "LinearRegression: 1.9593823412960274\n",
      "Ridge: 1.9593823412960274\n",
      "Lasso: 3.5043473670318073\n",
      "ElasticNet: 2.615024415995777\n",
      "SGDRegressor: 2.0223360168932296\n",
      "BayesianRidge: 1.9508037481852976\n",
      "ARDRegression: 1.9563468391183845\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store all of the models for testing\n",
    "models = []\n",
    "\n",
    "# Append models into the list\\\n",
    "models.append((\"LinearRegression\", LinearRegression()))\n",
    "models.append((\"Ridge\", Ridge()))\n",
    "models.append((\"Lasso\", Lasso()))\n",
    "models.append((\"ElasticNet\",ElasticNet()))\n",
    "models.append((\"SGDRegressor\", SGDRegressor()))\n",
    "models.append((\"BayesianRidge\", BayesianRidge()))\n",
    "models.append((\"ARDRegression\", ARDRegression()))\n",
    "\n",
    "# Create lists to store the output of the training loop\n",
    "model_names = []\n",
    "train_MSE = []\n",
    "test_MSE = []\n",
    "\n",
    "# Loop through the models to obtain mean cross-validated MSE scores\n",
    "for name, model in models:\n",
    "\n",
    "    # Add the model name to the list for this iteration\n",
    "    model_names.append(name)\n",
    "\n",
    "    # Set training parameters\n",
    "    scoring = \"neg_mean_squared_error\"\n",
    "    kfold = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "\n",
    "    # Get the mean cross-validated MSE score on the training data\n",
    "    train_cv_result = cross_val_score(estimator = model, X = X_train_s, y = y_train_1, cv = kfold, scoring = scoring)\n",
    "    avg_train_MSE = -train_cv_result.mean()\n",
    "    train_MSE.append(avg_train_MSE)\n",
    "    \n",
    "\n",
    "    # Get the MSE score on the test data\n",
    "    model.fit(X_train_s, y_train_1)\n",
    "    y_pred = model.predict(X_test_s)\n",
    "    y_pred_int = np.round(y_pred).astype(int) # Round predictions to nearest integer\n",
    "    comp_MSE = mean_squared_error(y_test_1, y_pred_int)\n",
    "    test_MSE.append(comp_MSE)\n",
    "\n",
    "# Print Results\n",
    "print(\"\\n\" \"Cross-Validation MSE on Training Data:\")\n",
    "for i in range(len(model_names)):\n",
    "    print(\"{}: {}\".format(model_names[i], train_MSE[i]))\n",
    "\n",
    "print(\"\\n\" \"MSE on Testing Data:\")\n",
    "for i in range(len(model_names)):\n",
    "    print(\"{}: {}\".format(model_names[i], test_MSE[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above analysis you can see that the models perform similarly on the testing and training data, but there are some slight differences. \n",
    "\n",
    "The Lasso and ElasticNet models have the highest MSE at ~3.50 and ~2.61. The SGDRegressor model is clearly the fifth place candidate based on its MSE of ~2.02 on the testing data.\n",
    "\n",
    "The remaining models have similar MSE scores on the test data. By obersving the Ridge and Linear Regression models and applying some critical thinking, we can conclude that when using mean_squared_error as the loss metric for the Linear Regression model it effectively becomes a Ridge model. Therefore the top three candidates are: Ridge, ARDRegression, and BayesianRidge.\n",
    "\n",
    "We will choose the Ridge model and the ARDRegression model as our top two models to perform hyperparameter tuning on.\n",
    "\n",
    "Note: The Ridge model only has one parameter for hyperparameter tuning, alpha. The BayesianRidge model has four parameters for tuning: alpha_1, alpha_2, lambda_1, lambda_2.\n",
    "\n",
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best alpha is {'alpha': 0.001}\n",
      "The cross-validated MSE for the best Ridge model is 1.8107634057869089\n",
      "The MSE of the best Ridge model versus the test data is 1.8789399686767878\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for Ridge model\n",
    "\n",
    "# Set up the parameter grid\n",
    "param_grid = {\n",
    "    \"alpha\" : [1E-10, 1E-9, 1E-8, 1E-7, 1E-6, 1E-5, 1E-4, 1E-3]}\n",
    "scoring = 'neg_mean_squared_error'\n",
    "\n",
    "# Perform GridSearchCV across the parameter grid\n",
    "grid_search = GridSearchCV(estimator = Ridge(), \n",
    "                           param_grid = param_grid, \n",
    "                           cv = 5,\n",
    "                           scoring = 'neg_mean_squared_error',\n",
    "                           return_train_score = True # Return the MSE for each alpha in .cv_results_\n",
    "                           )\n",
    "grid_search.fit(X_train_s, y_train_1)\n",
    "\n",
    "# Obtain the best model from grid search\n",
    "best_ridge_model = grid_search.best_estimator_\n",
    "best_ridge_param = grid_search.best_params_\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred = best_ridge_model.predict(X_test_s)\n",
    "test_MSE = mean_squared_error(y_test_1, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"The best alpha is {best_ridge_param}\")\n",
    "print(f\"The cross-validated MSE for the best Ridge model is {-grid_search.best_score_}\")\n",
    "print(f\"The MSE of the best Ridge model versus the test data is {test_MSE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross-validated MSE for the best BayesianRidge model is 1.8074948488675253\n",
      "The MSE of the best BayesianRidge versus the test data is 1.8759702652390209\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for BayesianRidge model\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Set up the parameter grid\n",
    "param_grid = {\n",
    "    \"alpha_1\" : [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"alpha_2\" : [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"lambda_1\" : [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"lambda_2\" : [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \n",
    "}\n",
    "scoring = 'neg_mean_squared_error'\n",
    "\n",
    "# Perform RandomizedSearchCV across the parameter grid\n",
    "rand_search = RandomizedSearchCV(estimator = BayesianRidge(), \n",
    "                           param_distributions = param_grid, \n",
    "                           cv = 5,\n",
    "                           scoring = 'neg_mean_squared_error',\n",
    "                           return_train_score = True, # Return the MSE for each alpha in .cv_results_\n",
    "                           n_iter = 100,\n",
    "                           random_state = 42, \n",
    "                           n_jobs = -1\n",
    "                           )\n",
    "rand_search.fit(X_train_s, y_train_1)\n",
    "\n",
    "# Obtain the best model and parameters\n",
    "best_BAY_model = rand_search.best_estimator_\n",
    "best_params = rand_search.best_params_\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred = best_BAY_model.predict(X_test_s)\n",
    "test_MSE = mean_squared_error(y_test_1, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"The cross-validated MSE for the best BayesianRidge model is {-rand_search.best_score_}\")\n",
    "print(f\"The MSE of the best BayesianRidge versus the test data is {test_MSE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_250</th>\n",
       "      <th>feature_251</th>\n",
       "      <th>feature_252</th>\n",
       "      <th>feature_253</th>\n",
       "      <th>feature_254</th>\n",
       "      <th>feature_255</th>\n",
       "      <th>feature_256</th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>assumed_score_2</th>\n",
       "      <th>assumed_score_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.123763</td>\n",
       "      <td>-0.117352</td>\n",
       "      <td>0.045677</td>\n",
       "      <td>0.015849</td>\n",
       "      <td>0.085833</td>\n",
       "      <td>-0.027624</td>\n",
       "      <td>0.003787</td>\n",
       "      <td>-0.08236</td>\n",
       "      <td>0.088994</td>\n",
       "      <td>-0.00169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.015938</td>\n",
       "      <td>0.059344</td>\n",
       "      <td>-0.162139</td>\n",
       "      <td>-0.024396</td>\n",
       "      <td>-0.03724</td>\n",
       "      <td>-0.043807</td>\n",
       "      <td>47</td>\n",
       "      <td>6.75</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 259 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0  -0.123763  -0.117352   0.045677   0.015849   0.085833  -0.027624   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_250  feature_251  \\\n",
       "0   0.003787   -0.08236   0.088994    -0.00169  ...     0.006923     0.015938   \n",
       "\n",
       "   feature_252  feature_253  feature_254  feature_255  feature_256  \\\n",
       "0     0.059344    -0.162139    -0.024396     -0.03724    -0.043807   \n",
       "\n",
       "   prompt_length  assumed_score_2  assumed_score_3  \n",
       "0             47             6.75         8.666667  \n",
       "\n",
       "[1 rows x 259 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating against the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Datasets\n",
    "\n",
    "# Embedding Data -- we will use this data in the \"Embedding Data\" section\n",
    "test_embeddings = np.load(\n",
    "    \"../testing_data/arena-test-set-prompts-embeddings.npy\"\n",
    ")\n",
    "\n",
    "# Topic Modeling and Hardness Score Data -- we will use this data in the \"Topic Modeling and Hardness Score Data\" section\n",
    "test_hardness = pd.read_json(\n",
    "    \"../testing_data/arena-test-set-topic-modeling.jsonl.gz\",\n",
    "    lines=True,\n",
    "    compression=\"gzip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_247</th>\n",
       "      <th>feature_248</th>\n",
       "      <th>feature_249</th>\n",
       "      <th>feature_250</th>\n",
       "      <th>feature_251</th>\n",
       "      <th>feature_252</th>\n",
       "      <th>feature_253</th>\n",
       "      <th>feature_254</th>\n",
       "      <th>feature_255</th>\n",
       "      <th>feature_256</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.055131</td>\n",
       "      <td>-0.115709</td>\n",
       "      <td>0.055225</td>\n",
       "      <td>0.050576</td>\n",
       "      <td>0.010953</td>\n",
       "      <td>-0.004206</td>\n",
       "      <td>0.062269</td>\n",
       "      <td>0.064194</td>\n",
       "      <td>0.06936</td>\n",
       "      <td>0.016847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035079</td>\n",
       "      <td>-0.044518</td>\n",
       "      <td>0.043931</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>-0.076169</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.008611</td>\n",
       "      <td>-0.01545</td>\n",
       "      <td>-0.033905</td>\n",
       "      <td>-0.057479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0  -0.055131  -0.115709   0.055225   0.050576   0.010953  -0.004206   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_247  feature_248  \\\n",
       "0   0.062269   0.064194    0.06936    0.016847  ...     0.035079    -0.044518   \n",
       "\n",
       "   feature_249  feature_250  feature_251  feature_252  feature_253  \\\n",
       "0     0.043931     0.000368    -0.076169     0.002721     0.008611   \n",
       "\n",
       "   feature_254  feature_255  feature_256  \n",
       "0     -0.01545    -0.033905    -0.057479  \n",
       "\n",
       "[1 rows x 256 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the prompt embeddings data into a pandas dataframe\n",
    "num_features = test_embeddings.shape[1]\n",
    "column_names = [f\"feature_{i+1}\" for i in range(num_features)]\n",
    "df_test_prompt = pd.DataFrame(test_embeddings, columns = column_names)\n",
    "df_test_prompt.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200, 256)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_prompt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for prompt length\n",
    "test_hardness[\"prompt_length\"] = test_hardness[\"prompt\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the topic modeling data\n",
    "for i in range(3):\n",
    "    test_hardness[f\"topic_modeling_{i+1}\"] = test_hardness[f\"topic_modeling_{i+1}\"].apply(\n",
    "        # Clean list element into an string\n",
    "        lambda x: \", \".join(x) if isinstance(x, list) else x\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>topic_modeling_1</th>\n",
       "      <th>topic_modeling_2</th>\n",
       "      <th>topic_modeling_3</th>\n",
       "      <th>prompt_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4f332ebd8cdc4ff2be74aa8828ff20d5</td>\n",
       "      <td>what do you think about the future of iran?</td>\n",
       "      <td>Future Prediction</td>\n",
       "      <td>Future Prediction</td>\n",
       "      <td>Future Prediction</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question_id  \\\n",
       "0  4f332ebd8cdc4ff2be74aa8828ff20d5   \n",
       "\n",
       "                                        prompt   topic_modeling_1  \\\n",
       "0  what do you think about the future of iran?  Future Prediction   \n",
       "\n",
       "    topic_modeling_2   topic_modeling_3  prompt_length  \n",
       "0  Future Prediction  Future Prediction             43  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hardness.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "895 1130\n"
     ]
    }
   ],
   "source": [
    "counter_in = 0\n",
    "counter_out = 0\n",
    "for topic in test_hardness[\"topic_modeling_1\"].unique():\n",
    "    if topic in (topic_and_hardness[\"topic_modeling_1\"].unique()):\n",
    "        counter_in += 1\n",
    "    else:\n",
    "        counter_out += 1\n",
    "\n",
    "print(counter_in, counter_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that out of the unique topics in the test data, only 895 of them are in the training data. The majority of topics in the test data are not in the training data. How can we find the assumed score? First, we can determine the cosine similarity between the embedding for the test prompt versus all of the training prompts. Then we can assume that the topic of the test prompt is the same topic as the most similar training prompt. Once we have a new assumed topic we can assume the hardness score is the average hardness score for that topic. This will allow us to have an assumed hardness score for every prompt embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create a function to determine the topic of the most similar training prompt.\n",
    "def assumed_topic(test_prompt_embedding, all_training_embeddings, topic_modeling):\n",
    "    # Reshape the test prompt embedding\n",
    "    test_prompt_embedding = np.array(test_prompt_embedding).reshape(1, -1)\n",
    "    # Determine the cosine similarity between the test prompt and all training prompts\n",
    "    test_similarity = cosine_similarity(test_prompt_embedding, all_training_embeddings).flatten()\n",
    "\n",
    "    # Determine the index of the most similar training prompt\n",
    "    index = np.argmax(test_similarity)\n",
    "\n",
    "    # Return the most similar training prompt\n",
    "    return topic_modeling.iloc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question_id', 'prompt', 'topic_modeling_1', 'topic_modeling_2',\n",
       "       'topic_modeling_3', 'prompt_length'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hardness.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_modeling_1</th>\n",
       "      <th>topic_modeling_2</th>\n",
       "      <th>topic_modeling_3</th>\n",
       "      <th>prompt_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Future Prediction</td>\n",
       "      <td>Future Prediction</td>\n",
       "      <td>Future Prediction</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic_modeling_1   topic_modeling_2   topic_modeling_3  prompt_length\n",
       "0  Future Prediction  Future Prediction  Future Prediction             43"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hardness = test_hardness.drop(columns = ['question_id', 'prompt'])\n",
    "test_hardness.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_251</th>\n",
       "      <th>feature_252</th>\n",
       "      <th>feature_253</th>\n",
       "      <th>feature_254</th>\n",
       "      <th>feature_255</th>\n",
       "      <th>feature_256</th>\n",
       "      <th>topic_modeling_1</th>\n",
       "      <th>topic_modeling_2</th>\n",
       "      <th>topic_modeling_3</th>\n",
       "      <th>prompt_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.055131</td>\n",
       "      <td>-0.115709</td>\n",
       "      <td>0.055225</td>\n",
       "      <td>0.050576</td>\n",
       "      <td>0.010953</td>\n",
       "      <td>-0.004206</td>\n",
       "      <td>0.062269</td>\n",
       "      <td>0.064194</td>\n",
       "      <td>0.06936</td>\n",
       "      <td>0.016847</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076169</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.008611</td>\n",
       "      <td>-0.01545</td>\n",
       "      <td>-0.033905</td>\n",
       "      <td>-0.057479</td>\n",
       "      <td>Future Prediction</td>\n",
       "      <td>Future Prediction</td>\n",
       "      <td>Future Prediction</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0  -0.055131  -0.115709   0.055225   0.050576   0.010953  -0.004206   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_251  feature_252  \\\n",
       "0   0.062269   0.064194    0.06936    0.016847  ...    -0.076169     0.002721   \n",
       "\n",
       "   feature_253  feature_254  feature_255  feature_256   topic_modeling_1  \\\n",
       "0     0.008611     -0.01545    -0.033905    -0.057479  Future Prediction   \n",
       "\n",
       "    topic_modeling_2   topic_modeling_3  prompt_length  \n",
       "0  Future Prediction  Future Prediction             43  \n",
       "\n",
       "[1 rows x 260 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the training data with the prompt embeddings\n",
    "df_test = pd.concat([df_test_prompt, test_hardness], axis = 1)\n",
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_250</th>\n",
       "      <th>feature_251</th>\n",
       "      <th>feature_252</th>\n",
       "      <th>feature_253</th>\n",
       "      <th>feature_254</th>\n",
       "      <th>feature_255</th>\n",
       "      <th>feature_256</th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>assumed_score_2</th>\n",
       "      <th>assumed_score_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.055131</td>\n",
       "      <td>-0.115709</td>\n",
       "      <td>0.055225</td>\n",
       "      <td>0.050576</td>\n",
       "      <td>0.010953</td>\n",
       "      <td>-0.004206</td>\n",
       "      <td>0.062269</td>\n",
       "      <td>0.064194</td>\n",
       "      <td>0.06936</td>\n",
       "      <td>0.016847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>-0.076169</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.008611</td>\n",
       "      <td>-0.01545</td>\n",
       "      <td>-0.033905</td>\n",
       "      <td>-0.057479</td>\n",
       "      <td>43</td>\n",
       "      <td>7.5</td>\n",
       "      <td>6.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 259 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0  -0.055131  -0.115709   0.055225   0.050576   0.010953  -0.004206   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_250  feature_251  \\\n",
       "0   0.062269   0.064194    0.06936    0.016847  ...     0.000368    -0.076169   \n",
       "\n",
       "   feature_252  feature_253  feature_254  feature_255  feature_256  \\\n",
       "0     0.002721     0.008611     -0.01545    -0.033905    -0.057479   \n",
       "\n",
       "   prompt_length  assumed_score_2  assumed_score_3  \n",
       "0             43              7.5         6.833333  \n",
       "\n",
       "[1 rows x 259 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Introduce the prediction heuristic for topic_modeling_2\n",
    "assumed_scores_2 = []\n",
    "assumed_scores_3 = []\n",
    "\n",
    "for i in range(test_hardness.shape[0]):\n",
    "    topic_2 = test_hardness[\"topic_modeling_2\"].iloc[i]\n",
    "    topic_3 = test_hardness[\"topic_modeling_3\"].iloc[i]\n",
    "    test_embedding = test_embeddings[i]\n",
    "\n",
    "    if topic_2 in topic_2_avg_scores:\n",
    "        # If topic is in the training data, use the average score for the topic\n",
    "        assumed_score_2 = topic_2_avg_scores[topic_2]\n",
    "    else:\n",
    "        # If the topic is not in the training data, use the average score for the topic of the most similar prompt.\n",
    "        most_similar_topic = assumed_topic(test_embedding,\n",
    "                                           valid_prompt_embeddings,\n",
    "                                           df_train[\"topic_modeling_2\"])\n",
    "        assumed_score_2 = topic_2_avg_scores[most_similar_topic]\n",
    "    assumed_scores_2.append(assumed_score_2)\n",
    "\n",
    "    if topic_3 in topic_3_avg_scores:\n",
    "        # If topic is in the training data, use the average score for the topic\n",
    "        assumed_score_3 = topic_3_avg_scores[topic_3]\n",
    "    else:\n",
    "        # If the topic is not in the training data, use the average score for the topic of the most similar prompt.\n",
    "        most_similar_topic = assumed_topic(test_embedding,\n",
    "                                           valid_prompt_embeddings,\n",
    "                                           df_train[\"topic_modeling_3\"])\n",
    "        assumed_score_3 = topic_3_avg_scores[most_similar_topic]\n",
    "    \n",
    "    assumed_scores_3.append(assumed_score_3)\n",
    "\n",
    "# Create the test dataframe\n",
    "df_test = pd.DataFrame()\n",
    "df_test[\"prompt_length\"] = test_hardness[\"prompt_length\"]\n",
    "df_test[\"assumed_score_2\"] = assumed_score_2\n",
    "df_test[\"assumed_score_3\"] = assumed_score_3\n",
    "# Concatenate the training data with the prompt embeddings\n",
    "df_test = pd.concat([df_test_prompt, df_test], axis = 1)\n",
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 6, 7, ..., 7, 6, 7])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale the data before making predictions\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(df_test)\n",
    "\n",
    "# Make Predictions\n",
    "y_pred = best_BAY_model.predict(X_train_s)\n",
    "y_pred.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>winner</th>\n",
       "      <th>hardness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4f332ebd8cdc4ff2be74aa8828ff20d5</td>\n",
       "      <td>model_b</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f2be6f13e5ed40e5b81443223996494c</td>\n",
       "      <td>model_b</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5fafefb8a0c54243afb52d2892946cea</td>\n",
       "      <td>model_b</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7834f572267f40709ecebb273a2b346b</td>\n",
       "      <td>model_a</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1ccc7e58290245c4bd5457fce45f8640</td>\n",
       "      <td>model_a</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>eb08f8a7f20840c99efe9fc8c03f1c13</td>\n",
       "      <td>model_a</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>4baca918f1f5440599ae9edb3bfa8cc1</td>\n",
       "      <td>model_b</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>a787ce60dc1440f39455ab20e3bffe33</td>\n",
       "      <td>model_b</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>3dc09f20eedb405ab3dc980cf7bff5d0</td>\n",
       "      <td>model_a</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>75b64403f4654bb9ad4311f4952e9571</td>\n",
       "      <td>model_b</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           question_id   winner  hardness_score\n",
       "0     4f332ebd8cdc4ff2be74aa8828ff20d5  model_b               9\n",
       "1     f2be6f13e5ed40e5b81443223996494c  model_b               9\n",
       "2     5fafefb8a0c54243afb52d2892946cea  model_b               9\n",
       "3     7834f572267f40709ecebb273a2b346b  model_a               9\n",
       "4     1ccc7e58290245c4bd5457fce45f8640  model_a               9\n",
       "...                                ...      ...             ...\n",
       "3195  eb08f8a7f20840c99efe9fc8c03f1c13  model_a               9\n",
       "3196  4baca918f1f5440599ae9edb3bfa8cc1  model_b               9\n",
       "3197  a787ce60dc1440f39455ab20e3bffe33  model_b               9\n",
       "3198  3dc09f20eedb405ab3dc980cf7bff5d0  model_a               9\n",
       "3199  75b64403f4654bb9ad4311f4952e9571  model_b               9\n",
       "\n",
       "[3200 rows x 3 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.read_csv(\"submission_20241129_084701.csv\")\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>winner</th>\n",
       "      <th>hardness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4f332ebd8cdc4ff2be74aa8828ff20d5</td>\n",
       "      <td>model_b</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f2be6f13e5ed40e5b81443223996494c</td>\n",
       "      <td>model_b</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5fafefb8a0c54243afb52d2892946cea</td>\n",
       "      <td>model_b</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7834f572267f40709ecebb273a2b346b</td>\n",
       "      <td>model_a</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1ccc7e58290245c4bd5457fce45f8640</td>\n",
       "      <td>model_a</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>eb08f8a7f20840c99efe9fc8c03f1c13</td>\n",
       "      <td>model_a</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>4baca918f1f5440599ae9edb3bfa8cc1</td>\n",
       "      <td>model_b</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>a787ce60dc1440f39455ab20e3bffe33</td>\n",
       "      <td>model_b</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>3dc09f20eedb405ab3dc980cf7bff5d0</td>\n",
       "      <td>model_a</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>75b64403f4654bb9ad4311f4952e9571</td>\n",
       "      <td>model_b</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           question_id   winner  hardness_score\n",
       "0     4f332ebd8cdc4ff2be74aa8828ff20d5  model_b               7\n",
       "1     f2be6f13e5ed40e5b81443223996494c  model_b               6\n",
       "2     5fafefb8a0c54243afb52d2892946cea  model_b               7\n",
       "3     7834f572267f40709ecebb273a2b346b  model_a               6\n",
       "4     1ccc7e58290245c4bd5457fce45f8640  model_a               7\n",
       "...                                ...      ...             ...\n",
       "3195  eb08f8a7f20840c99efe9fc8c03f1c13  model_a               7\n",
       "3196  4baca918f1f5440599ae9edb3bfa8cc1  model_b               7\n",
       "3197  a787ce60dc1440f39455ab20e3bffe33  model_b               7\n",
       "3198  3dc09f20eedb405ab3dc980cf7bff5d0  model_a               6\n",
       "3199  75b64403f4654bb9ad4311f4952e9571  model_b               7\n",
       "\n",
       "[3200 rows x 3 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df[\"hardness_score\"] = y_pred.astype(int)\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msse-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
