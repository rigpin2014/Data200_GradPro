{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gzip\n",
    " \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Task B\n",
    "\n",
    "For this analysis we will be creating a linear regression model to predict the hardness score. The hardness score represent the difficulty of the question being asked. Therefore, it is independent of the main dataset (conversation data) or the response embedding data in the auxilliary dataset. To predict the hardness score we will make the assumption that each row of embedding data from the prompt embeddings corresponds to each row of the topic_and_hardness dataset.\n",
    "\n",
    "The problem statement for task B states that we must use linear regression to determine the hardness score. Therefore, any linear model from the sklearn library would meet this criteria. Therefore, we will perform an analysis and return the results of the best performing models. We will then select the top two models for hyperparameter tuning to create our final models, then the best model of the tuned models will be the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../training_data/chatbot-arena-prompts-embeddings.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Auxiliary Datasets\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Embedding Data -- we will use this data in the \"Embedding Data\" section\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m prompt_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../training_data/chatbot-arena-prompts-embeddings.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Topic Modeling and Hardness Score Data -- we will use this data in the \"Topic Modeling and Hardness Score Data\" section\u001b[39;00m\n\u001b[0;32m      9\u001b[0m topic_and_hardness \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../training_data/chatbot-arena-gpt3-scores.jsonl.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     12\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\sebme\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../training_data/chatbot-arena-prompts-embeddings.npy'"
     ]
    }
   ],
   "source": [
    "# Auxiliary Datasets\n",
    "\n",
    "# Embedding Data -- we will use this data in the \"Embedding Data\" section\n",
    "prompt_embeddings = np.load(\n",
    "    \"../training_data/chatbot-arena-prompts-embeddings.npy\"\n",
    ")\n",
    "\n",
    "# Topic Modeling and Hardness Score Data -- we will use this data in the \"Topic Modeling and Hardness Score Data\" section\n",
    "topic_and_hardness = pd.read_json(\n",
    "    \"../training_data/chatbot-arena-gpt3-scores.jsonl.gz\",\n",
    "    lines=True,\n",
    "    compression=\"gzip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25282, 256)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25282, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_and_hardness.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>openai_scores_raw_choices_nested</th>\n",
       "      <th>topic_modeling_1</th>\n",
       "      <th>score_reason_1</th>\n",
       "      <th>score_value_1</th>\n",
       "      <th>topic_modeling_2</th>\n",
       "      <th>score_reason_2</th>\n",
       "      <th>score_value_2</th>\n",
       "      <th>topic_modeling_3</th>\n",
       "      <th>score_reason_3</th>\n",
       "      <th>score_value_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58210e39b3fd4441a2bd4a518bb44c2d</td>\n",
       "      <td>What is the difference between OpenCL and CUDA?</td>\n",
       "      <td>[{'finish_reason': 'stop', 'index': 0, 'logpro...</td>\n",
       "      <td>Technical Comparison</td>\n",
       "      <td>This prompt requires the AI to accurately comp...</td>\n",
       "      <td>9</td>\n",
       "      <td>Software Comparison</td>\n",
       "      <td>This prompt assesses the AI's factual accuracy...</td>\n",
       "      <td>8</td>\n",
       "      <td>Comparison, Technology</td>\n",
       "      <td>This prompt requires the AI to demonstrate kno...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question_id  \\\n",
       "0  58210e39b3fd4441a2bd4a518bb44c2d   \n",
       "\n",
       "                                            prompt  \\\n",
       "0  What is the difference between OpenCL and CUDA?   \n",
       "\n",
       "                    openai_scores_raw_choices_nested      topic_modeling_1  \\\n",
       "0  [{'finish_reason': 'stop', 'index': 0, 'logpro...  Technical Comparison   \n",
       "\n",
       "                                      score_reason_1 score_value_1  \\\n",
       "0  This prompt requires the AI to accurately comp...             9   \n",
       "\n",
       "      topic_modeling_2                                     score_reason_2  \\\n",
       "0  Software Comparison  This prompt assesses the AI's factual accuracy...   \n",
       "\n",
       "  score_value_2        topic_modeling_3  \\\n",
       "0             8  Comparison, Technology   \n",
       "\n",
       "                                      score_reason_3 score_value_3  \n",
       "0  This prompt requires the AI to demonstrate kno...             9  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_and_hardness.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.unique of 0             Technical Comparison\n",
       "1               Reasoning, Emotion\n",
       "2                Camera comparison\n",
       "3                    Chatbot Arena\n",
       "4                       Time Query\n",
       "                   ...            \n",
       "25277     Mathematics, Measurement\n",
       "25278        Information Retrieval\n",
       "25279    Training, Hyperparameters\n",
       "25280            Language Modeling\n",
       "25281          Workflow Automation\n",
       "Name: topic_modeling_1, Length: 25282, dtype: object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_and_hardness[\"topic_modeling_1\"].unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above analysis, majority of the data in the topic_and_hardness dataframe is not useful for analysis. Therefore our method will be to create an ensemble type approach where we train a model to target each of the three score values, then average the result from the three models to obtain the final predicted hardness score. But first we must find out which model performs best on our data.\n",
    "\n",
    "Let us first create our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question_id', 'prompt', 'openai_scores_raw_choices_nested',\n",
       "       'topic_modeling_1', 'score_reason_1', 'score_value_1',\n",
       "       'topic_modeling_2', 'score_reason_2', 'score_value_2',\n",
       "       'topic_modeling_3', 'score_reason_3', 'score_value_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_and_hardness.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score_value_1</th>\n",
       "      <th>score_value_2</th>\n",
       "      <th>score_value_3</th>\n",
       "      <th>prompt_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  score_value_1 score_value_2 score_value_3  prompt_length\n",
       "0             9             8             9             47"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the score value columns\n",
    "data = topic_and_hardness[[\"score_value_1\", \"score_value_2\", \"score_value_3\"]].copy()\n",
    "data[\"prompt_length\"] = topic_and_hardness[\"prompt\"].apply(len)\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score_value_1    26\n",
       "score_value_2    26\n",
       "score_value_3    26\n",
       "prompt_length     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_247</th>\n",
       "      <th>feature_248</th>\n",
       "      <th>feature_249</th>\n",
       "      <th>feature_250</th>\n",
       "      <th>feature_251</th>\n",
       "      <th>feature_252</th>\n",
       "      <th>feature_253</th>\n",
       "      <th>feature_254</th>\n",
       "      <th>feature_255</th>\n",
       "      <th>feature_256</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.123763</td>\n",
       "      <td>-0.117352</td>\n",
       "      <td>0.045677</td>\n",
       "      <td>0.015849</td>\n",
       "      <td>0.085833</td>\n",
       "      <td>-0.027624</td>\n",
       "      <td>0.003787</td>\n",
       "      <td>-0.08236</td>\n",
       "      <td>0.088994</td>\n",
       "      <td>-0.00169</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024708</td>\n",
       "      <td>-0.114236</td>\n",
       "      <td>0.034814</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.015938</td>\n",
       "      <td>0.059344</td>\n",
       "      <td>-0.162139</td>\n",
       "      <td>-0.024396</td>\n",
       "      <td>-0.03724</td>\n",
       "      <td>-0.043807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0  -0.123763  -0.117352   0.045677   0.015849   0.085833  -0.027624   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_247  feature_248  \\\n",
       "0   0.003787   -0.08236   0.088994    -0.00169  ...    -0.024708    -0.114236   \n",
       "\n",
       "   feature_249  feature_250  feature_251  feature_252  feature_253  \\\n",
       "0     0.034814     0.006923     0.015938     0.059344    -0.162139   \n",
       "\n",
       "   feature_254  feature_255  feature_256  \n",
       "0    -0.024396     -0.03724    -0.043807  \n",
       "\n",
       "[1 rows x 256 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the prompt embeddings data into a pandas dataframe\n",
    "num_features = prompt_embeddings.shape[1]\n",
    "column_names = [f\"feature_{i+1}\" for i in range(num_features)]\n",
    "embeddings = pd.DataFrame(prompt_embeddings, columns = column_names)\n",
    "embeddings.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_251</th>\n",
       "      <th>feature_252</th>\n",
       "      <th>feature_253</th>\n",
       "      <th>feature_254</th>\n",
       "      <th>feature_255</th>\n",
       "      <th>feature_256</th>\n",
       "      <th>score_value_1</th>\n",
       "      <th>score_value_2</th>\n",
       "      <th>score_value_3</th>\n",
       "      <th>prompt_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.123763</td>\n",
       "      <td>-0.117352</td>\n",
       "      <td>0.045677</td>\n",
       "      <td>0.015849</td>\n",
       "      <td>0.085833</td>\n",
       "      <td>-0.027624</td>\n",
       "      <td>0.003787</td>\n",
       "      <td>-0.082360</td>\n",
       "      <td>0.088994</td>\n",
       "      <td>-0.001690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015938</td>\n",
       "      <td>0.059344</td>\n",
       "      <td>-0.162139</td>\n",
       "      <td>-0.024396</td>\n",
       "      <td>-0.037240</td>\n",
       "      <td>-0.043807</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006028</td>\n",
       "      <td>0.028436</td>\n",
       "      <td>-0.091022</td>\n",
       "      <td>0.039573</td>\n",
       "      <td>-0.080445</td>\n",
       "      <td>-0.053600</td>\n",
       "      <td>-0.046251</td>\n",
       "      <td>-0.026352</td>\n",
       "      <td>-0.081835</td>\n",
       "      <td>0.045040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022912</td>\n",
       "      <td>-0.082866</td>\n",
       "      <td>0.055752</td>\n",
       "      <td>0.085062</td>\n",
       "      <td>-0.053332</td>\n",
       "      <td>0.001854</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.035222</td>\n",
       "      <td>-0.109402</td>\n",
       "      <td>-0.022247</td>\n",
       "      <td>-0.037604</td>\n",
       "      <td>0.037931</td>\n",
       "      <td>-0.049936</td>\n",
       "      <td>-0.011818</td>\n",
       "      <td>0.033610</td>\n",
       "      <td>0.032769</td>\n",
       "      <td>0.022925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032092</td>\n",
       "      <td>0.055308</td>\n",
       "      <td>-0.035479</td>\n",
       "      <td>-0.141167</td>\n",
       "      <td>0.004774</td>\n",
       "      <td>0.004169</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.050530</td>\n",
       "      <td>-0.004413</td>\n",
       "      <td>0.090092</td>\n",
       "      <td>0.029821</td>\n",
       "      <td>-0.037979</td>\n",
       "      <td>-0.095112</td>\n",
       "      <td>-0.016179</td>\n",
       "      <td>0.006698</td>\n",
       "      <td>-0.063790</td>\n",
       "      <td>0.046847</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013089</td>\n",
       "      <td>0.060516</td>\n",
       "      <td>0.032741</td>\n",
       "      <td>-0.034432</td>\n",
       "      <td>0.045946</td>\n",
       "      <td>-0.063517</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.038406</td>\n",
       "      <td>0.045207</td>\n",
       "      <td>0.061096</td>\n",
       "      <td>0.051551</td>\n",
       "      <td>0.046493</td>\n",
       "      <td>-0.016303</td>\n",
       "      <td>0.058638</td>\n",
       "      <td>0.054352</td>\n",
       "      <td>-0.065154</td>\n",
       "      <td>-0.023475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070126</td>\n",
       "      <td>-0.039035</td>\n",
       "      <td>-0.083557</td>\n",
       "      <td>-0.045493</td>\n",
       "      <td>0.012152</td>\n",
       "      <td>-0.010252</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0  -0.123763  -0.117352   0.045677   0.015849   0.085833  -0.027624   \n",
       "1   0.006028   0.028436  -0.091022   0.039573  -0.080445  -0.053600   \n",
       "2  -0.035222  -0.109402  -0.022247  -0.037604   0.037931  -0.049936   \n",
       "3  -0.050530  -0.004413   0.090092   0.029821  -0.037979  -0.095112   \n",
       "4  -0.038406   0.045207   0.061096   0.051551   0.046493  -0.016303   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_251  feature_252  \\\n",
       "0   0.003787  -0.082360   0.088994   -0.001690  ...     0.015938     0.059344   \n",
       "1  -0.046251  -0.026352  -0.081835    0.045040  ...    -0.022912    -0.082866   \n",
       "2  -0.011818   0.033610   0.032769    0.022925  ...     0.032092     0.055308   \n",
       "3  -0.016179   0.006698  -0.063790    0.046847  ...    -0.013089     0.060516   \n",
       "4   0.058638   0.054352  -0.065154   -0.023475  ...     0.070126    -0.039035   \n",
       "\n",
       "   feature_253  feature_254  feature_255  feature_256  score_value_1  \\\n",
       "0    -0.162139    -0.024396    -0.037240    -0.043807              9   \n",
       "1     0.055752     0.085062    -0.053332     0.001854              9   \n",
       "2    -0.035479    -0.141167     0.004774     0.004169              2   \n",
       "3     0.032741    -0.034432     0.045946    -0.063517              8   \n",
       "4    -0.083557    -0.045493     0.012152    -0.010252              2   \n",
       "\n",
       "   score_value_2  score_value_3  prompt_length  \n",
       "0              8              9             47  \n",
       "1              8              8             49  \n",
       "2              6              2             32  \n",
       "3              8              8             35  \n",
       "4              2              2             17  \n",
       "\n",
       "[5 rows x 260 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the modelling data\n",
    "m_data = pd.concat([embeddings, data], axis = 1)\n",
    "m_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "m_data.drop(list(m_data[m_data[\"score_value_1\"].isnull() == True].index), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "                ..\n",
       "feature_256      0\n",
       "score_value_1    0\n",
       "score_value_2    0\n",
       "score_value_3    0\n",
       "prompt_length    0\n",
       "Length: 260, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "for i in range(3):\n",
    "    m_data[f\"score_value_{i+1}\"] = m_data[f\"score_value_{i+1}\"].apply(\n",
    "        # Clean nested list element into an int\n",
    "        lambda x: x[0][0] if isinstance(x, list) and len(x) == 1 and isinstance(x[0], list) and len(x[0]) == 1 else (\n",
    "            # Else clean the list element into an int\n",
    "            x[0] if isinstance(x, list) and len(x) == 1 and isinstance(x[0], (int, float)) \n",
    "            # Else leave it alone\n",
    "            else x\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_data[\"avg_score\"] = m_data[[\"score_value_1\", \"score_value_2\", \"score_value_3\"]].mean(axis=1)\n",
    "m_data[\"score_variance\"] = m_data[[\"score_value_1\", \"score_value_2\", \"score_value_3\"]].var(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing Libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Model Building Libraries\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,\n",
    "    Ridge, \n",
    "    Lasso,\n",
    "    ElasticNet,\n",
    "    SGDRegressor,\n",
    "    BayesianRidge,\n",
    "    ARDRegression\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "# Split the data into X and y\n",
    "X = m_data.drop(columns = [\"score_value_1\", \n",
    "                           \"score_value_2\", \n",
    "                           \"score_value_3\"], axis = 1)\n",
    "y = m_data[\"score_value_1\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# Scale the data.\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation MSE on Training Data:\n",
      "LinearRegression: 0.7154841161293884\n",
      "Ridge: 0.7154805999722897\n",
      "Lasso: 1.7031024973365334\n",
      "ElasticNet: 1.5165029862803967\n",
      "SGDRegressor: 0.747374858123635\n",
      "BayesianRidge: 0.7153709749900037\n",
      "ARDRegression: 0.7045579751307461\n",
      "\n",
      "MSE on Testing Data:\n",
      "LinearRegression: 0.7822145968061238\n",
      "Ridge: 0.7822145968061238\n",
      "Lasso: 1.9302679160617657\n",
      "ElasticNet: 1.5904236505213145\n",
      "SGDRegressor: 0.8057067440939686\n",
      "BayesianRidge: 0.7811587699617263\n",
      "ARDRegression: 0.7708644582288505\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store all of the models for testing\n",
    "models = []\n",
    "\n",
    "# Append models into the list\\\n",
    "models.append((\"LinearRegression\", LinearRegression()))\n",
    "models.append((\"Ridge\", Ridge()))\n",
    "models.append((\"Lasso\", Lasso()))\n",
    "models.append((\"ElasticNet\",ElasticNet()))\n",
    "models.append((\"SGDRegressor\", SGDRegressor()))\n",
    "models.append((\"BayesianRidge\", BayesianRidge()))\n",
    "models.append((\"ARDRegression\", ARDRegression()))\n",
    "\n",
    "# Create lists to store the output of the training loop\n",
    "model_names = []\n",
    "train_MSE = []\n",
    "test_MSE = []\n",
    "\n",
    "# Loop through the models to obtain mean cross-validated MSE scores\n",
    "for name, model in models:\n",
    "\n",
    "    # Add the model name to the list for this iteration\n",
    "    model_names.append(name)\n",
    "\n",
    "    # Set training parameters\n",
    "    scoring = \"neg_mean_squared_error\"\n",
    "    kfold = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "\n",
    "    # Get the mean cross-validated MSE score on the training data\n",
    "    train_cv_result = cross_val_score(estimator = model, X = X_train_s, y = y_train, cv = kfold, scoring = scoring)\n",
    "    avg_train_MSE = -train_cv_result.mean()\n",
    "    train_MSE.append(avg_train_MSE)\n",
    "    \n",
    "\n",
    "    # Get the MSE score on the test data\n",
    "    model.fit(X_train_s, y_train)\n",
    "    y_pred = model.predict(X_test_s)\n",
    "    y_pred_int = np.round(y_pred).astype(int) # Round predictions to nearest integer\n",
    "    comp_MSE = mean_squared_error(y_test, y_pred_int)\n",
    "    test_MSE.append(comp_MSE)\n",
    "\n",
    "# Print Results\n",
    "print(\"\\n\" \"Cross-Validation MSE on Training Data:\")\n",
    "for i in range(len(model_names)):\n",
    "    print(\"{}: {}\".format(model_names[i], train_MSE[i]))\n",
    "\n",
    "print(\"\\n\" \"MSE on Testing Data:\")\n",
    "for i in range(len(model_names)):\n",
    "    print(\"{}: {}\".format(model_names[i], test_MSE[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above analysis you can see that the models perform similarly on the testing and training data, but there are some slight differences. \n",
    "\n",
    "The Lasso and ElasticNet models have the highest MSE at ~1.93 and ~1.59. The SGDRegressor model is clearly the fifth place candidate based on its MSE of ~0.8 on the testing data.\n",
    "\n",
    "The remaining models have similar MSE scores on the test data. By obersving the Ridge and Linear Regression models and applying some critical thinking, we can conclude that when using mean_squared_error as the loss metric for the Linear Regression model it effectively becomes a Ridge model. Therefore the top three candidates are: Ridge, ARDRegression, and BayesianRidge.\n",
    "\n",
    "We will choose the Ridge model and the ARDRegression model as our top two models to perform hyperparameter tuning on.\n",
    "\n",
    "Note: The Ridge model only has one parameter for hyperparameter tuning, alpha. The ARDRegression model has four parameters for tuning: alpha_1, alpha_2, lambda_1, lambda_2.\n",
    "\n",
    "## Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best alpha is 1e-10\n",
      "The cross-validated MSE for the best Ridge model is 0.7181150145397839\n",
      "The MSE of the best Ridge model versus the test data is 0.7822145968061238\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for Ridge model\n",
    "\n",
    "# Set up the parameter grid\n",
    "param_grid = {\n",
    "    \"alpha\" : [1E-10, 1E-9, 1E-8, 1E-7, 1E-6, 1E-5, 1E-4, 1E-3]}\n",
    "scoring = 'neg_mean_squared_error'\n",
    "\n",
    "# Perform GridSearchCV across the parameter grid\n",
    "grid_search = GridSearchCV(estimator = Ridge(), \n",
    "                           param_grid = param_grid, \n",
    "                           cv = 5,\n",
    "                           scoring = 'neg_mean_squared_error',\n",
    "                           return_train_score = True # Return the MSE for each alpha in .cv_results_\n",
    "                           )\n",
    "grid_search.fit(X_train_s, y_train)\n",
    "\n",
    "# Create empty variables to store the best model\n",
    "best_ridge_model = []\n",
    "best_alpha = []\n",
    "best_ridge_train_MSE = None\n",
    "best_ridge_test_MSE = float(\"inf\")\n",
    "\n",
    "# Loop through each alpha in the parameter grid\n",
    "for i, param in enumerate(grid_search.cv_results_[\"params\"]):\n",
    "\n",
    "    # Obtain the train_MSE for the iteration\n",
    "    train_MSE = -grid_search.cv_results_[\"mean_test_score\"][i]\n",
    "\n",
    "    # Train the Ridge model on the alpha for this iteration\n",
    "    model = Ridge(alpha = param[\"alpha\"])\n",
    "    model.fit(X_train_s, y_train)\n",
    "    y_pred = model.predict(X_test_s)\n",
    "    y_pred_int = np.round(y_pred).astype(int)\n",
    "\n",
    "    # Obtain the test_MSE for the iteration\n",
    "    test_MSE = mean_squared_error(y_test, y_pred_int)\n",
    "\n",
    "    if test_MSE < best_ridge_test_MSE:\n",
    "        best_ridge_model = model\n",
    "        best_alpha = param[\"alpha\"]\n",
    "        best_ridge_train_MSE = train_MSE\n",
    "        best_ridge_test_MSE = test_MSE\n",
    "\n",
    "# Print the results\n",
    "print(f\"The best alpha is {best_alpha}\")\n",
    "print(f\"The cross-validated MSE for the best Ridge model is {best_ridge_train_MSE}\")\n",
    "print(f\"The MSE of the best Ridge model versus the test data is {best_ridge_test_MSE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross-validated MSE for the best ARDRegression model is 0.7058298591553831\n",
      "The MSE of the best ARDRegression model versus the test data is 0.7699406097400027\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for BayesianRidge model\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Set up the parameter grid\n",
    "param_grid = {\n",
    "    \"alpha_1\" : [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"alpha_2\" : [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"lambda_1\" : [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"lambda_2\" : [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \n",
    "}\n",
    "scoring = 'neg_mean_squared_error'\n",
    "\n",
    "# Perform GridSearchCV across the parameter grid\n",
    "rand_search = RandomizedSearchCV(estimator = ARDRegression(), \n",
    "                           param_distributions = param_grid, \n",
    "                           cv = 5,\n",
    "                           scoring = 'neg_mean_squared_error',\n",
    "                           return_train_score = True, # Return the MSE for each alpha in .cv_results_\n",
    "                           n_iter = 200,\n",
    "                           random_state = 42, \n",
    "                           n_jobs = -1\n",
    "                           )\n",
    "rand_search.fit(X_train_s, y_train)\n",
    "\n",
    "# Create empty variables to store the best model\n",
    "best_params = []\n",
    "best_ARD_train_MSE = None\n",
    "best_ARD_test_MSE = float(\"inf\")\n",
    "\n",
    "# Loop through each alpha in the parameter grid\n",
    "for i, param in enumerate(rand_search.cv_results_[\"params\"]):\n",
    "\n",
    "    # Obtain the train_MSE for the iteration\n",
    "    train_MSE = -rand_search.cv_results_[\"mean_test_score\"][i]\n",
    "\n",
    "    # Train the Ridge model on the alpha for this iteration\n",
    "    model = ARDRegression(**param)  # Use ** for every combination of possible parameters\n",
    "    model.fit(X_train_s, y_train)\n",
    "    y_pred = model.predict(X_test_s)\n",
    "    y_pred_int = np.round(y_pred).astype(int)\n",
    "\n",
    "    # Obtain the test_MSE for the iteration\n",
    "    test_MSE = mean_squared_error(y_test, y_pred_int)\n",
    "\n",
    "    if test_MSE < best_ARD_test_MSE:\n",
    "        best_ARD_model = model\n",
    "        best_params = param\n",
    "        best_ARD_train_MSE = train_MSE\n",
    "        best_ARD_test_MSE = test_MSE\n",
    "\n",
    "# Print the results\n",
    "print(f\"The cross-validated MSE for the best ARDRegression model is {best_ARD_train_MSE}\")\n",
    "print(f\"The MSE of the best ARDRegression model versus the test data is {best_ARD_test_MSE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lambda_2': 0.0001, 'lambda_1': 1, 'alpha_2': 1e-06, 'alpha_1': 1e-05}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the best parameters for the final ARDRegression model\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have determined our best model, let us employ our ensemble method to obtain the most accurate prediction given the probabalistic responses of GPT3.5. To do this we are going to train three models on each of the score values generated by GPT3.5 and then average the results of the models to obtain our final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_253</th>\n",
       "      <th>feature_254</th>\n",
       "      <th>feature_255</th>\n",
       "      <th>feature_256</th>\n",
       "      <th>score_value_1</th>\n",
       "      <th>score_value_2</th>\n",
       "      <th>score_value_3</th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>score_variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.123763</td>\n",
       "      <td>-0.117352</td>\n",
       "      <td>0.045677</td>\n",
       "      <td>0.015849</td>\n",
       "      <td>0.085833</td>\n",
       "      <td>-0.027624</td>\n",
       "      <td>0.003787</td>\n",
       "      <td>-0.082360</td>\n",
       "      <td>0.088994</td>\n",
       "      <td>-0.001690</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.162139</td>\n",
       "      <td>-0.024396</td>\n",
       "      <td>-0.037240</td>\n",
       "      <td>-0.043807</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8</td>\n",
       "      <td>9.0</td>\n",
       "      <td>47</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006028</td>\n",
       "      <td>0.028436</td>\n",
       "      <td>-0.091022</td>\n",
       "      <td>0.039573</td>\n",
       "      <td>-0.080445</td>\n",
       "      <td>-0.053600</td>\n",
       "      <td>-0.046251</td>\n",
       "      <td>-0.026352</td>\n",
       "      <td>-0.081835</td>\n",
       "      <td>0.045040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055752</td>\n",
       "      <td>0.085062</td>\n",
       "      <td>-0.053332</td>\n",
       "      <td>0.001854</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>49</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.035222</td>\n",
       "      <td>-0.109402</td>\n",
       "      <td>-0.022247</td>\n",
       "      <td>-0.037604</td>\n",
       "      <td>0.037931</td>\n",
       "      <td>-0.049936</td>\n",
       "      <td>-0.011818</td>\n",
       "      <td>0.033610</td>\n",
       "      <td>0.032769</td>\n",
       "      <td>0.022925</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035479</td>\n",
       "      <td>-0.141167</td>\n",
       "      <td>0.004774</td>\n",
       "      <td>0.004169</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>5.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.050530</td>\n",
       "      <td>-0.004413</td>\n",
       "      <td>0.090092</td>\n",
       "      <td>0.029821</td>\n",
       "      <td>-0.037979</td>\n",
       "      <td>-0.095112</td>\n",
       "      <td>-0.016179</td>\n",
       "      <td>0.006698</td>\n",
       "      <td>-0.063790</td>\n",
       "      <td>0.046847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032741</td>\n",
       "      <td>-0.034432</td>\n",
       "      <td>0.045946</td>\n",
       "      <td>-0.063517</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>35</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.038406</td>\n",
       "      <td>0.045207</td>\n",
       "      <td>0.061096</td>\n",
       "      <td>0.051551</td>\n",
       "      <td>0.046493</td>\n",
       "      <td>-0.016303</td>\n",
       "      <td>0.058638</td>\n",
       "      <td>0.054352</td>\n",
       "      <td>-0.065154</td>\n",
       "      <td>-0.023475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083557</td>\n",
       "      <td>-0.045493</td>\n",
       "      <td>0.012152</td>\n",
       "      <td>-0.010252</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 262 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0  -0.123763  -0.117352   0.045677   0.015849   0.085833  -0.027624   \n",
       "1   0.006028   0.028436  -0.091022   0.039573  -0.080445  -0.053600   \n",
       "2  -0.035222  -0.109402  -0.022247  -0.037604   0.037931  -0.049936   \n",
       "3  -0.050530  -0.004413   0.090092   0.029821  -0.037979  -0.095112   \n",
       "4  -0.038406   0.045207   0.061096   0.051551   0.046493  -0.016303   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_253  feature_254  \\\n",
       "0   0.003787  -0.082360   0.088994   -0.001690  ...    -0.162139    -0.024396   \n",
       "1  -0.046251  -0.026352  -0.081835    0.045040  ...     0.055752     0.085062   \n",
       "2  -0.011818   0.033610   0.032769    0.022925  ...    -0.035479    -0.141167   \n",
       "3  -0.016179   0.006698  -0.063790    0.046847  ...     0.032741    -0.034432   \n",
       "4   0.058638   0.054352  -0.065154   -0.023475  ...    -0.083557    -0.045493   \n",
       "\n",
       "   feature_255  feature_256  score_value_1  score_value_2  score_value_3  \\\n",
       "0    -0.037240    -0.043807            9.0              8            9.0   \n",
       "1    -0.053332     0.001854            9.0              8            8.0   \n",
       "2     0.004774     0.004169            2.0              6            2.0   \n",
       "3     0.045946    -0.063517            8.0              8            8.0   \n",
       "4     0.012152    -0.010252            2.0              2            2.0   \n",
       "\n",
       "   prompt_length  avg_score  score_variance  \n",
       "0             47   8.666667        0.333333  \n",
       "1             49   8.333333        0.333333  \n",
       "2             32   3.333333        5.333333  \n",
       "3             35   8.000000        0.000000  \n",
       "4             17   2.000000        0.000000  \n",
       "\n",
       "[5 rows x 262 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE of the ARDRegression model predicting 'score_value_1' is 0.7035689059689595.\n",
      "The MSE of the ARDRegression model predicting 'score_value_2' is 0.7247891585771813.\n",
      "The MSE of the ARDRegression model predicting 'score_value_3' is 0.7101071992500068.\n",
      "The MSE of the ARDRegression Ensemble agains the test data for 'score_value_1' is: 0.7762755708063878.\n",
      "The MSE of the ARDRegression Ensemble agains the test data for 'score_value_2' is: 0.8230170252078659.\n",
      "The MSE of the ARDRegression Ensemble agains the test data for 'score_value_3' is: 0.821471558664379.\n"
     ]
    }
   ],
   "source": [
    "# Split the data for y2 and y3 on indices to align with y\n",
    "y2 = m_data[\"score_value_2\"]\n",
    "y3 = m_data[\"score_value_3\"]\n",
    "\n",
    "_, _, y2_train, y2_test = train_test_split(X, y2, test_size = 0.3, random_state = 42)\n",
    "_, _, y3_train, y3_test = train_test_split(X, y3, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# Initialize the models\n",
    "model1 = ARDRegression(alpha_1 = 1E-5, alpha_2 = 1E-6, lambda_1 = 1, lambda_2 = 0.0001)\n",
    "model2 = ARDRegression(alpha_1 = 1E-5, alpha_2 = 1E-6, lambda_1 = 1, lambda_2 = 0.0001)\n",
    "model3 = ARDRegression(alpha_1 = 1E-5, alpha_2 = 1E-6, lambda_1 = 1, lambda_2 = 0.0001)\n",
    "\n",
    "# Set training parameters\n",
    "scoring = \"neg_mean_squared_error\"\n",
    "kfold1 = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "kfold2 = KFold(n_splits = 5, shuffle = True, random_state = 16)\n",
    "kfold3 = KFold(n_splits = 5, shuffle = True, random_state = 6)\n",
    "\n",
    "# Get the mean cross-validated MSE score on the training data\n",
    "train_cv_result_1 = cross_val_score(estimator = model1, X = X_train_s, y = y_train, cv = kfold1, scoring = scoring)\n",
    "train_cv_result_2 = cross_val_score(estimator = model2, X = X_train_s, y = y2_train, cv = kfold2, scoring = scoring)\n",
    "train_cv_result_3 = cross_val_score(estimator = model3, X = X_train_s, y = y3_train, cv = kfold3, scoring = scoring)\n",
    "\n",
    "avg_train_MSE_1 = -train_cv_result_1.mean()\n",
    "avg_train_MSE_2 = -train_cv_result_2.mean()\n",
    "avg_train_MSE_3 = -train_cv_result_3.mean()    \n",
    "\n",
    "# Fit the models and obtain predictions\n",
    "model1.fit(X_train_s, y_train)\n",
    "y1_pred = model.predict(X_test_s)\n",
    "y1_pred_int = np.round(y1_pred).astype(int) # Round predictions to nearest integer\n",
    "\n",
    "model2.fit(X_train_s, y2_train)\n",
    "y2_pred = model.predict(X_test_s)\n",
    "y2_pred_int = np.round(y2_pred).astype(int) # Round predictions to nearest integer\n",
    "\n",
    "model3.fit(X_train_s, y3_train)\n",
    "y3_pred = model.predict(X_test_s)\n",
    "y3_pred_int = np.round(y3_pred).astype(int) # Round predictions to nearest integer\n",
    "\n",
    "# Print the MSE of the training data\n",
    "print(f\"The MSE of the ARDRegression model predicting 'score_value_1' is {avg_train_MSE_1}.\")\n",
    "print(f\"The MSE of the ARDRegression model predicting 'score_value_2' is {avg_train_MSE_2}.\")\n",
    "print(f\"The MSE of the ARDRegression model predicting 'score_value_3' is {avg_train_MSE_3}.\")\n",
    "\n",
    "# Add results to a dataframe and obtain the average prediction\n",
    "df_final = pd.DataFrame()\n",
    "df_final[\"model_1_pred\"] = y1_pred_int\n",
    "df_final[\"model_2_pred\"] = y2_pred_int\n",
    "df_final[\"model_3_pred\"] = y3_pred_int\n",
    "df_final[\"avg_pred\"] = np.round(df_final[[\"model_1_pred\", \"model_2_pred\", \"model_3_pred\"]].mean(axis = 1)).astype(int)\n",
    "\n",
    "# Obtain the MSE on the test data using the average prediction\n",
    "test_MSE_1  = mean_squared_error(y_test, df_final[\"avg_pred\"])\n",
    "test_MSE_2  = mean_squared_error(y2_test, df_final[\"avg_pred\"])\n",
    "test_MSE_3  = mean_squared_error(y3_test, df_final[\"avg_pred\"])\n",
    "print(f\"The MSE of the ARDRegression Ensemble agains the test data for 'score_value_1' is: {test_MSE_1}.\")\n",
    "print(f\"The MSE of the ARDRegression Ensemble agains the test data for 'score_value_2' is: {test_MSE_2}.\")\n",
    "print(f\"The MSE of the ARDRegression Ensemble agains the test data for 'score_value_3' is: {test_MSE_3}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
