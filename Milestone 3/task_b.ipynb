{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gzip\n",
    "\n",
    "\n",
    " \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Task B\n",
    "\n",
    "For this analysis we will be creating a linear regression model to predict the hardness score. The hardness score represent the difficulty of the question being asked. Therefore, it is independent of the main dataset (conversation data) or the response embedding data in the auxilliary dataset. To predict the hardness score we will make the assumption that each row of embedding data from the prompt embeddings corresponds to each row of the topic_and_hardness dataset.\n",
    "\n",
    "The problem statement for task B states that we must use linear regression to determine the hardness score. Therefore, any linear model from the sklearn library would meet this criteria. Therefore, we will perform an analysis and return the results of the best performing models. We will then select the top two models for hyperparameter tuning to create our final models, then the best model of the tuned models will be the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Datasets\n",
    "\n",
    "# Prompt embedding Data -- we will use this data in the \"Embedding Data\" section\n",
    "prompt_embeddings = np.load(\n",
    "    \"../training_data/chatbot-arena-prompts-embeddings.npy\"\n",
    ")\n",
    "\n",
    "# Topic Modeling and Hardness Score Data -- we will use this data in the \"Topic Modeling and Hardness Score Data\" section\n",
    "topic_and_hardness = pd.read_json(\n",
    "    \"../training_data/chatbot-arena-gpt3-scores.jsonl.gz\",\n",
    "    lines=True,\n",
    "    compression=\"gzip\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_247</th>\n",
       "      <th>feature_248</th>\n",
       "      <th>feature_249</th>\n",
       "      <th>feature_250</th>\n",
       "      <th>feature_251</th>\n",
       "      <th>feature_252</th>\n",
       "      <th>feature_253</th>\n",
       "      <th>feature_254</th>\n",
       "      <th>feature_255</th>\n",
       "      <th>feature_256</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.123763</td>\n",
       "      <td>-0.117352</td>\n",
       "      <td>0.045677</td>\n",
       "      <td>0.015849</td>\n",
       "      <td>0.085833</td>\n",
       "      <td>-0.027624</td>\n",
       "      <td>0.003787</td>\n",
       "      <td>-0.08236</td>\n",
       "      <td>0.088994</td>\n",
       "      <td>-0.00169</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024708</td>\n",
       "      <td>-0.114236</td>\n",
       "      <td>0.034814</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.015938</td>\n",
       "      <td>0.059344</td>\n",
       "      <td>-0.162139</td>\n",
       "      <td>-0.024396</td>\n",
       "      <td>-0.03724</td>\n",
       "      <td>-0.043807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0  -0.123763  -0.117352   0.045677   0.015849   0.085833  -0.027624   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_247  feature_248  \\\n",
       "0   0.003787   -0.08236   0.088994    -0.00169  ...    -0.024708    -0.114236   \n",
       "\n",
       "   feature_249  feature_250  feature_251  feature_252  feature_253  \\\n",
       "0     0.034814     0.006923     0.015938     0.059344    -0.162139   \n",
       "\n",
       "   feature_254  feature_255  feature_256  \n",
       "0    -0.024396     -0.03724    -0.043807  \n",
       "\n",
       "[1 rows x 256 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the prompt embeddings data into a pandas dataframe\n",
    "num_features = prompt_embeddings.shape[1]\n",
    "column_names = [f\"feature_{i+1}\" for i in range(num_features)]\n",
    "df_prompt = pd.DataFrame(prompt_embeddings, columns = column_names)\n",
    "df_prompt.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>openai_scores_raw_choices_nested</th>\n",
       "      <th>topic_modeling_1</th>\n",
       "      <th>score_reason_1</th>\n",
       "      <th>score_value_1</th>\n",
       "      <th>topic_modeling_2</th>\n",
       "      <th>score_reason_2</th>\n",
       "      <th>score_value_2</th>\n",
       "      <th>topic_modeling_3</th>\n",
       "      <th>score_reason_3</th>\n",
       "      <th>score_value_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58210e39b3fd4441a2bd4a518bb44c2d</td>\n",
       "      <td>What is the difference between OpenCL and CUDA?</td>\n",
       "      <td>[{'finish_reason': 'stop', 'index': 0, 'logpro...</td>\n",
       "      <td>Technical Comparison</td>\n",
       "      <td>This prompt requires the AI to accurately comp...</td>\n",
       "      <td>9</td>\n",
       "      <td>Software Comparison</td>\n",
       "      <td>This prompt assesses the AI's factual accuracy...</td>\n",
       "      <td>8</td>\n",
       "      <td>Comparison, Technology</td>\n",
       "      <td>This prompt requires the AI to demonstrate kno...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question_id  \\\n",
       "0  58210e39b3fd4441a2bd4a518bb44c2d   \n",
       "\n",
       "                                            prompt  \\\n",
       "0  What is the difference between OpenCL and CUDA?   \n",
       "\n",
       "                    openai_scores_raw_choices_nested      topic_modeling_1  \\\n",
       "0  [{'finish_reason': 'stop', 'index': 0, 'logpro...  Technical Comparison   \n",
       "\n",
       "                                      score_reason_1 score_value_1  \\\n",
       "0  This prompt requires the AI to accurately comp...             9   \n",
       "\n",
       "      topic_modeling_2                                     score_reason_2  \\\n",
       "0  Software Comparison  This prompt assesses the AI's factual accuracy...   \n",
       "\n",
       "  score_value_2        topic_modeling_3  \\\n",
       "0             8  Comparison, Technology   \n",
       "\n",
       "                                      score_reason_3 score_value_3  \n",
       "0  This prompt requires the AI to demonstrate kno...             9  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_and_hardness.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question_id', 'prompt', 'topic_modeling_1', 'score_value_1',\n",
       "       'topic_modeling_2', 'score_value_2', 'topic_modeling_3',\n",
       "       'score_value_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_and_hardness = topic_and_hardness.drop(columns = [\"score_reason_1\", \n",
    "                                                        \"score_reason_2\", \n",
    "                                                        \"score_reason_3\",\n",
    "                                                        \"openai_scores_raw_choices_nested\",\n",
    "                                                        \"question_id\",\n",
    "                                                        \"topic_modeling_1\",\n",
    "                                                        \"topic_modeling_2\",\n",
    "                                                        \"topic_modeling_3\"], axis = 1)\n",
    "topic_and_hardness.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for prompt length\n",
    "topic_and_hardness[\"prompt_length\"] = topic_and_hardness[\"prompt\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the score data\n",
    "for i in range(3):\n",
    "    topic_and_hardness[f\"score_value_{i+1}\"] = topic_and_hardness[f\"score_value_{i+1}\"].apply(\n",
    "        # Clean nested list element into an int\n",
    "        lambda x: x[0][0] if isinstance(x, list) and len(x) == 1 and isinstance(x[0], list) and len(x[0]) == 1 else (\n",
    "            # Else clean the list element into an int\n",
    "            x[0] if isinstance(x, list) and len(x) == 1 and isinstance(x[0], (int, float)) \n",
    "            # Else leave it alone\n",
    "            else x\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: score_value_1, dtype: float64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_and_hardness[\"score_value_1\"][\n",
    "    topic_and_hardness[\"score_value_1\"].apply(lambda x: isinstance(x, list))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the topic modeling data\n",
    "for i in range(3):\n",
    "    topic_and_hardness[f\"topic_modeling_{i+1}\"] = topic_and_hardness[f\"topic_modeling_{i+1}\"].apply(\n",
    "        # Clean list element into an string\n",
    "        lambda x: \", \".join(x) if isinstance(x, list) else x\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: topic_modeling_3, dtype: object)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_and_hardness[\"topic_modeling_3\"][\n",
    "    topic_and_hardness[\"topic_modeling_3\"].apply(lambda x: isinstance(x, list))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_modeling_1</th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>score_value_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3D Modeling</td>\n",
       "      <td>42</td>\n",
       "      <td>7.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3D Modeling</td>\n",
       "      <td>57</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3D Modeling</td>\n",
       "      <td>102</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3D Modeling</td>\n",
       "      <td>103</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3D Printing</td>\n",
       "      <td>158</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21263</th>\n",
       "      <td>troubleshooting, technology</td>\n",
       "      <td>39</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21264</th>\n",
       "      <td>urban forestry</td>\n",
       "      <td>71</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21265</th>\n",
       "      <td>videogames, recommendation</td>\n",
       "      <td>54</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21266</th>\n",
       "      <td>weather forecast</td>\n",
       "      <td>122</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21267</th>\n",
       "      <td>worldbuilding, storytelling</td>\n",
       "      <td>185</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21268 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  topic_modeling_1  prompt_length  score_value_1\n",
       "0                      3D Modeling             42       7.333333\n",
       "1                      3D Modeling             57       7.000000\n",
       "2                      3D Modeling            102       8.000000\n",
       "3                      3D Modeling            103       9.000000\n",
       "4                      3D Printing            158       7.000000\n",
       "...                            ...            ...            ...\n",
       "21263  troubleshooting, technology             39       7.000000\n",
       "21264               urban forestry             71       8.000000\n",
       "21265   videogames, recommendation             54       7.000000\n",
       "21266             weather forecast            122       7.000000\n",
       "21267  worldbuilding, storytelling            185       9.000000\n",
       "\n",
       "[21268 rows x 3 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_1 = topic_and_hardness.groupby([\"topic_modeling_1\", \"prompt_length\"])[\"score_value_1\"].mean().reset_index()\n",
    "group_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>topic_modeling_1</th>\n",
       "      <th>score_value_1</th>\n",
       "      <th>topic_modeling_2</th>\n",
       "      <th>score_value_2</th>\n",
       "      <th>topic_modeling_3</th>\n",
       "      <th>score_value_3</th>\n",
       "      <th>prompt_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58210e39b3fd4441a2bd4a518bb44c2d</td>\n",
       "      <td>What is the difference between OpenCL and CUDA?</td>\n",
       "      <td>Technical Comparison</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Software Comparison</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Comparison, Technology</td>\n",
       "      <td>9.0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question_id  \\\n",
       "0  58210e39b3fd4441a2bd4a518bb44c2d   \n",
       "\n",
       "                                            prompt      topic_modeling_1  \\\n",
       "0  What is the difference between OpenCL and CUDA?  Technical Comparison   \n",
       "\n",
       "   score_value_1     topic_modeling_2  score_value_2        topic_modeling_3  \\\n",
       "0            9.0  Software Comparison            8.0  Comparison, Technology   \n",
       "\n",
       "   score_value_3  prompt_length  \n",
       "0            9.0             47  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_and_hardness.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = topic_and_hardness.drop(columns = [\"question_id\", \"topic_modeling_1\", \"topic_modeling_2\", \"topic_modeling_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score_value_1</th>\n",
       "      <th>score_value_2</th>\n",
       "      <th>score_value_3</th>\n",
       "      <th>prompt_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score_value_1  score_value_2  score_value_3  prompt_length\n",
       "0            9.0            8.0            9.0             47"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_251</th>\n",
       "      <th>feature_252</th>\n",
       "      <th>feature_253</th>\n",
       "      <th>feature_254</th>\n",
       "      <th>feature_255</th>\n",
       "      <th>feature_256</th>\n",
       "      <th>score_value_1</th>\n",
       "      <th>score_value_2</th>\n",
       "      <th>score_value_3</th>\n",
       "      <th>prompt_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.123763</td>\n",
       "      <td>-0.117352</td>\n",
       "      <td>0.045677</td>\n",
       "      <td>0.015849</td>\n",
       "      <td>0.085833</td>\n",
       "      <td>-0.027624</td>\n",
       "      <td>0.003787</td>\n",
       "      <td>-0.08236</td>\n",
       "      <td>0.088994</td>\n",
       "      <td>-0.00169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015938</td>\n",
       "      <td>0.059344</td>\n",
       "      <td>-0.162139</td>\n",
       "      <td>-0.024396</td>\n",
       "      <td>-0.03724</td>\n",
       "      <td>-0.043807</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0  -0.123763  -0.117352   0.045677   0.015849   0.085833  -0.027624   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_251  feature_252  \\\n",
       "0   0.003787   -0.08236   0.088994    -0.00169  ...     0.015938     0.059344   \n",
       "\n",
       "   feature_253  feature_254  feature_255  feature_256  score_value_1  \\\n",
       "0    -0.162139    -0.024396     -0.03724    -0.043807            9.0   \n",
       "\n",
       "   score_value_2  score_value_3  prompt_length  \n",
       "0            8.0            9.0             47  \n",
       "\n",
       "[1 rows x 260 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the training data with the prompt embeddings\n",
    "# Create the modelling data\n",
    "df_train = pd.concat([df_prompt, df_train], axis = 1)\n",
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1         0\n",
       "feature_2         0\n",
       "feature_3         0\n",
       "feature_4         0\n",
       "feature_5         0\n",
       "                 ..\n",
       "feature_256       0\n",
       "score_value_1    26\n",
       "score_value_2    26\n",
       "score_value_3    26\n",
       "prompt_length     0\n",
       "Length: 260, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25256, 260)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with missing values\n",
    "df_train.drop(df_train[df_train[\"score_value_1\"].isnull() == True].index, inplace = True)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "                ..\n",
       "feature_256      0\n",
       "score_value_1    0\n",
       "score_value_2    0\n",
       "score_value_3    0\n",
       "prompt_length    0\n",
       "Length: 260, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the valid prompt embeddings back into an array\n",
    "embedding_columns = [col for col in df_train.columns if col.startswith(\"feature_\")]\n",
    "valid_prompt_embeddings = df_train[embedding_columns].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_251</th>\n",
       "      <th>feature_252</th>\n",
       "      <th>feature_253</th>\n",
       "      <th>feature_254</th>\n",
       "      <th>feature_255</th>\n",
       "      <th>feature_256</th>\n",
       "      <th>score_value_1</th>\n",
       "      <th>score_value_2</th>\n",
       "      <th>score_value_3</th>\n",
       "      <th>prompt_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.123763</td>\n",
       "      <td>-0.117352</td>\n",
       "      <td>0.045677</td>\n",
       "      <td>0.015849</td>\n",
       "      <td>0.085833</td>\n",
       "      <td>-0.027624</td>\n",
       "      <td>0.003787</td>\n",
       "      <td>-0.08236</td>\n",
       "      <td>0.088994</td>\n",
       "      <td>-0.00169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015938</td>\n",
       "      <td>0.059344</td>\n",
       "      <td>-0.162139</td>\n",
       "      <td>-0.024396</td>\n",
       "      <td>-0.03724</td>\n",
       "      <td>-0.043807</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0  -0.123763  -0.117352   0.045677   0.015849   0.085833  -0.027624   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_251  feature_252  \\\n",
       "0   0.003787   -0.08236   0.088994    -0.00169  ...     0.015938     0.059344   \n",
       "\n",
       "   feature_253  feature_254  feature_255  feature_256  score_value_1  \\\n",
       "0    -0.162139    -0.024396     -0.03724    -0.043807            9.0   \n",
       "\n",
       "   score_value_2  score_value_3  prompt_length  \n",
       "0            8.0            9.0             47  \n",
       "\n",
       "[1 rows x 260 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fold based feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([    0,     1,     2, ..., 25253, 25254, 25255]),\n",
       "  array([   17,    29,    30, ..., 25226, 25239, 25240])),\n",
       " (array([    1,     2,     5, ..., 25252, 25254, 25255]),\n",
       "  array([    0,     3,     4, ..., 25247, 25251, 25253])),\n",
       " (array([    0,     1,     2, ..., 25252, 25253, 25254]),\n",
       "  array([    5,     7,     8, ..., 25249, 25250, 25255])),\n",
       " (array([    0,     3,     4, ..., 25253, 25254, 25255]),\n",
       "  array([    1,     2,    10, ..., 25205, 25210, 25229])),\n",
       " (array([    0,     1,     2, ..., 25251, 25253, 25255]),\n",
       "  array([    9,    11,    13, ..., 25245, 25252, 25254]))]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Split the data into folds\n",
    "kfolds = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "folds = list(kfolds.split(df_train))\n",
    "folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    df_train[f\"similarity_{i+1}\"] = None\n",
    "    for j in range(3):\n",
    "        df_train[f\"sim_{i+1}_score_{j+1}\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>sim_3_score_2</th>\n",
       "      <th>sim_3_score_3</th>\n",
       "      <th>similarity_4</th>\n",
       "      <th>sim_4_score_1</th>\n",
       "      <th>sim_4_score_2</th>\n",
       "      <th>sim_4_score_3</th>\n",
       "      <th>similarity_5</th>\n",
       "      <th>sim_5_score_1</th>\n",
       "      <th>sim_5_score_2</th>\n",
       "      <th>sim_5_score_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.123763</td>\n",
       "      <td>-0.117352</td>\n",
       "      <td>0.045677</td>\n",
       "      <td>0.015849</td>\n",
       "      <td>0.085833</td>\n",
       "      <td>-0.027624</td>\n",
       "      <td>0.003787</td>\n",
       "      <td>-0.08236</td>\n",
       "      <td>0.088994</td>\n",
       "      <td>-0.00169</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 280 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0  -0.123763  -0.117352   0.045677   0.015849   0.085833  -0.027624   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  sim_3_score_2  \\\n",
       "0   0.003787   -0.08236   0.088994    -0.00169  ...           None   \n",
       "\n",
       "   sim_3_score_3  similarity_4  sim_4_score_1  sim_4_score_2  sim_4_score_3  \\\n",
       "0           None          None           None           None           None   \n",
       "\n",
       "   similarity_5  sim_5_score_1  sim_5_score_2  sim_5_score_3  \n",
       "0          None           None           None           None  \n",
       "\n",
       "[1 rows x 280 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Loop through the folds\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(folds):\n",
    "    # Index the dataset (Note: the indices of training data / val data match df_train)\n",
    "    training_data = df_train.iloc[train_idx]\n",
    "    val_data = df_train.iloc[val_idx]\n",
    "\n",
    "    # Obtain prompt embeddings\n",
    "    training_prompt_indices = [col for col in training_data.columns if col.startswith(\"feature_\")]\n",
    "    val_prompt_indices = [col for col in val_data.columns if col.startswith(\"feature_\")]\n",
    "    training_prompt_embeddings = training_data[training_prompt_indices].to_numpy()\n",
    "    val_prompt_embeddings = val_data[val_prompt_indices].to_numpy()\n",
    "\n",
    "    # Obtain the similarity of every prompt in the validation data to every prompt in the training data\n",
    "    for idx, prompt_embedding in zip(val_idx, val_prompt_embeddings):\n",
    "        prompt_embedding = prompt_embedding.reshape(1, -1)\n",
    "        similarity = cosine_similarity(prompt_embedding, training_prompt_embeddings).flatten()\n",
    "\n",
    "        # Obtain the similarity scores and the index of the top five most similar prompts\n",
    "        top_five_indices = np.argsort(similarity)[-5:][::-1]\n",
    "        top_five_similarities = similarity[top_five_indices]\n",
    "\n",
    "        # Add this data to the dataframe\n",
    "        for order, (idy, sim_score) in enumerate(zip(top_five_indices, top_five_similarities), start = 1):\n",
    "            # Add the similarity scores for the top five most similar prompts to the dataframe\n",
    "            df_train.loc[idx, f\"similarity_{order}\"] = sim_score\n",
    "\n",
    "            # Add the scores from the similar prompts to the data\n",
    "            for j in range(3):\n",
    "                df_train.loc[idx, f\"sim_{order}_score_{j+1}\"] = training_data.iloc[idy][f\"score_value_{j+1}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        26\n",
       "feature_2        26\n",
       "feature_3        26\n",
       "feature_4        26\n",
       "feature_5        26\n",
       "                 ..\n",
       "sim_4_score_3    26\n",
       "similarity_5     26\n",
       "sim_5_score_1    26\n",
       "sim_5_score_2    26\n",
       "sim_5_score_3    26\n",
       "Length: 280, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25256, 280)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with missing values\n",
    "df_train.drop(df_train[df_train[\"score_value_1\"].isnull() == True].index, inplace = True)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1         0\n",
       "feature_2         0\n",
       "feature_3         0\n",
       "feature_4         0\n",
       "feature_5         0\n",
       "                 ..\n",
       "sim_4_score_3    26\n",
       "similarity_5     26\n",
       "sim_5_score_1    26\n",
       "sim_5_score_2    26\n",
       "sim_5_score_3    26\n",
       "Length: 280, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25230, 280)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with missing values\n",
    "df_train.drop(df_train[df_train[\"sim_5_score_3\"].isnull() == True].index, inplace = True)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "                ..\n",
       "sim_4_score_2    0\n",
       "sim_4_score_3    0\n",
       "sim_5_score_1    0\n",
       "sim_5_score_2    0\n",
       "sim_5_score_3    0\n",
       "Length: 295, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>sim_3_score_2</th>\n",
       "      <th>sim_3_score_3</th>\n",
       "      <th>similarity_4</th>\n",
       "      <th>sim_4_score_1</th>\n",
       "      <th>sim_4_score_2</th>\n",
       "      <th>sim_4_score_3</th>\n",
       "      <th>similarity_5</th>\n",
       "      <th>sim_5_score_1</th>\n",
       "      <th>sim_5_score_2</th>\n",
       "      <th>sim_5_score_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.123763</td>\n",
       "      <td>-0.117352</td>\n",
       "      <td>0.045677</td>\n",
       "      <td>0.015849</td>\n",
       "      <td>0.085833</td>\n",
       "      <td>-0.027624</td>\n",
       "      <td>0.003787</td>\n",
       "      <td>-0.08236</td>\n",
       "      <td>0.088994</td>\n",
       "      <td>-0.00169</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.487742</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.475612</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 280 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0  -0.123763  -0.117352   0.045677   0.015849   0.085833  -0.027624   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  sim_3_score_2  \\\n",
       "0   0.003787   -0.08236   0.088994    -0.00169  ...            7.0   \n",
       "\n",
       "   sim_3_score_3  similarity_4  sim_4_score_1  sim_4_score_2  sim_4_score_3  \\\n",
       "0            7.0      0.487742            9.0            9.0            9.0   \n",
       "\n",
       "   similarity_5  sim_5_score_1  sim_5_score_2  sim_5_score_3  \n",
       "0      0.475612            9.0            8.0            8.0  \n",
       "\n",
       "[1 rows x 280 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"score_value_1\"] = df_train[\"score_value_1\"].fillna(0).astype(int)\n",
    "df_train[\"score_value_2\"] = df_train[\"score_value_2\"].fillna(0).astype(int)\n",
    "df_train[\"score_value_3\"] = df_train[\"score_value_3\"].fillna(0).astype(int)\n",
    "df_train[\"score_value_1\"] = df_train[\"score_value_1\"].astype(int)\n",
    "df_train[\"score_value_2\"] = df_train[\"score_value_2\"].astype(int)\n",
    "df_train[\"score_value_3\"] = df_train[\"score_value_3\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing Libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Model Building Libraries\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,\n",
    "    Ridge, \n",
    "    Lasso,\n",
    "    ElasticNet,\n",
    "    SGDRegressor,\n",
    "    BayesianRidge,\n",
    "    ARDRegression\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    "    BaggingClassifier,\n",
    ")\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X and y\n",
    "X = df_train.drop(columns = [\"score_value_1\", \n",
    "                             \"score_value_2\", \n",
    "                             \"score_value_3\", ])\n",
    "y = df_train[[\"score_value_1\", \"score_value_2\", \"score_value_3\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# Scale the data.\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 36\u001b[0m\n\u001b[1;32m     25\u001b[0m kfold \u001b[38;5;241m=\u001b[39m KFold(n_splits \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Get the mean cross-validated MSE score on the training data\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#cv_result = cross_val_score(estimator = model, \u001b[39;00m\n\u001b[1;32m     29\u001b[0m                                   \u001b[38;5;66;03m#X = X_train_s, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Evaluate the model on MSE\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train_s, y_train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore_value_1\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     37\u001b[0m y_pred_train \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train_s)\n\u001b[1;32m     38\u001b[0m y_pred_test \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_s)\n",
      "File \u001b[0;32m~/miniconda3/envs/msse-python/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:538\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stages(\n\u001b[1;32m    539\u001b[0m     X,\n\u001b[1;32m    540\u001b[0m     y,\n\u001b[1;32m    541\u001b[0m     raw_predictions,\n\u001b[1;32m    542\u001b[0m     sample_weight,\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng,\n\u001b[1;32m    544\u001b[0m     X_val,\n\u001b[1;32m    545\u001b[0m     y_val,\n\u001b[1;32m    546\u001b[0m     sample_weight_val,\n\u001b[1;32m    547\u001b[0m     begin_at_stage,\n\u001b[1;32m    548\u001b[0m     monitor,\n\u001b[1;32m    549\u001b[0m )\n\u001b[1;32m    551\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/msse-python/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:615\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    608\u001b[0m     old_oob_score \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[1;32m    609\u001b[0m         y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    610\u001b[0m         raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    611\u001b[0m         sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    612\u001b[0m     )\n\u001b[1;32m    614\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[0;32m--> 615\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stage(\n\u001b[1;32m    616\u001b[0m     i,\n\u001b[1;32m    617\u001b[0m     X,\n\u001b[1;32m    618\u001b[0m     y,\n\u001b[1;32m    619\u001b[0m     raw_predictions,\n\u001b[1;32m    620\u001b[0m     sample_weight,\n\u001b[1;32m    621\u001b[0m     sample_mask,\n\u001b[1;32m    622\u001b[0m     random_state,\n\u001b[1;32m    623\u001b[0m     X_csc,\n\u001b[1;32m    624\u001b[0m     X_csr,\n\u001b[1;32m    625\u001b[0m )\n\u001b[1;32m    627\u001b[0m \u001b[38;5;66;03m# track deviance (= loss)\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[0;32m~/miniconda3/envs/msse-python/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:257\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    254\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    256\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[0;32m--> 257\u001b[0m tree\u001b[38;5;241m.\u001b[39mfit(X, residual, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[1;32m    260\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[1;32m    261\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[1;32m    262\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m    270\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/msse-python/lib/python3.11/site-packages/sklearn/tree/_classes.py:1247\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \n\u001b[1;32m   1221\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1247\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m   1248\u001b[0m         X,\n\u001b[1;32m   1249\u001b[0m         y,\n\u001b[1;32m   1250\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m   1251\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[1;32m   1252\u001b[0m     )\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/msse-python/lib/python3.11/site-packages/sklearn/tree/_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    370\u001b[0m         splitter,\n\u001b[1;32m    371\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    377\u001b[0m     )\n\u001b[0;32m--> 379\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create an empty list to store all of the models for testing\n",
    "models = []\n",
    "\n",
    "# Append models into the list\\\n",
    "models.append((\"Adaboost\", AdaBoostClassifier()))\n",
    "models.append((\"GradientBoosting\", GradientBoostingClassifier()))\n",
    "models.append((\"RandomForest\", RandomForestClassifier(n_jobs = -1)))\n",
    "models.append((\"Bagging\",BaggingClassifier(n_jobs = -1)))\n",
    "models.append((\"XGBClassifier\", XGBClassifier(eval_metric = \"logloss\", n_jobs = -1)))\n",
    "\n",
    "# Create lists to store the output of the training loop\n",
    "model_names = []\n",
    "train_MSE = []\n",
    "test_MSE = []\n",
    "trained_models = []\n",
    "\n",
    "# Loop through the models to obtain mean cross-validated MSE scores\n",
    "for name, model in models:\n",
    "\n",
    "    # Add the model name to the list for this iteration\n",
    "    model_names.append(name)\n",
    "\n",
    "    # Set training parameters\n",
    "    scoring = \"neg_mean_squared_error\"\n",
    "    kfold = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "\n",
    "    # Get the mean cross-validated MSE score on the training data\n",
    "    #cv_result = cross_val_score(estimator = model, \n",
    "                                      #X = X_train_s, \n",
    "                                      #y = y_train[\"score_value_1\"], \n",
    "                                      #cv = kfold, \n",
    "                                      #scoring = scoring\n",
    "                                      #)\n",
    "    \n",
    "    # Evaluate the model on MSE\n",
    "    model.fit(X_train_s, y_train[\"score_value_1\"])\n",
    "    y_pred_train = model.predict(X_train_s)\n",
    "    y_pred_test = model.predict(X_test_s)\n",
    "\n",
    "    train_mse = mean_squared_error(y_train[\"score_value_1\"], y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test[\"score_value_1\"], y_pred_test)\n",
    "\n",
    "    train_MSE.append(train_mse)\n",
    "    test_MSE.append(test_mse)\n",
    "\n",
    "# Print Results\n",
    "print(\"\\n\" \"Cross-Validation MSE on Training Data:\")\n",
    "for i in range(len(model_names)):\n",
    "        print(\"{}: {}\".format(model_names[i], train_MSE[i]))\n",
    "\n",
    "print(\"\\n\" \"MSE on Testing Data:\")\n",
    "for i in range(len(model_names)):\n",
    "    print(\"{}: {}\".format(model_names[i], test_MSE[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation MSE on Training Data:\n",
      "LinearRegression: 2.5262352808967656\n",
      "Ridge: 2.526235288661006\n",
      "Lasso: 3.627914308214588\n",
      "ElasticNet: 3.627914308214588\n",
      "SGDRegressor: 2.621268220513191\n",
      "BayesianRidge: 2.531605310033661\n",
      "ARDRegression: 2.540252394038225\n",
      "\n",
      "MSE on Testing Data:\n",
      "LinearRegression: 2.7758840886070373\n",
      "Ridge: 2.7758400493239974\n",
      "Lasso: 3.6570220636808037\n",
      "ElasticNet: 3.6570220636808037\n",
      "SGDRegressor: 2.850574712643678\n",
      "BayesianRidge: 2.7739023208702167\n",
      "ARDRegression: 2.782489981063108\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store all of the models for testing\n",
    "models = []\n",
    "\n",
    "# Append models into the list\\\n",
    "models.append((\"LinearRegression\", LinearRegression()))\n",
    "models.append((\"Ridge\", Ridge()))\n",
    "models.append((\"Lasso\", Lasso()))\n",
    "models.append((\"ElasticNet\",ElasticNet()))\n",
    "models.append((\"SGDRegressor\", SGDRegressor()))\n",
    "models.append((\"BayesianRidge\", BayesianRidge()))\n",
    "models.append((\"ARDRegression\", ARDRegression()))\n",
    "\n",
    "# Create lists to store the output of the training loop\n",
    "model_names = []\n",
    "train_MSE = []\n",
    "test_MSE = []\n",
    "trained_models = []\n",
    "\n",
    "# Loop through the models to obtain mean cross-validated MSE scores\n",
    "for name, model in models:\n",
    "\n",
    "    # Add the model name to the list for this iteration\n",
    "    model_names.append(name)\n",
    "\n",
    "    # Set training parameters\n",
    "    scoring = \"neg_mean_squared_error\"\n",
    "    kfold = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "\n",
    "    # Get the mean cross-validated MSE score on the training data\n",
    "    train_cv_result = cross_validate(estimator = model, \n",
    "                                      X = X_train_s, \n",
    "                                      y = y_train, \n",
    "                                      cv = kfold, \n",
    "                                      scoring = scoring,\n",
    "                                      return_train_score = True,\n",
    "                                      return_estimator = True)\n",
    "    avg_train_MSE = -train_cv_result['train_score']\n",
    "    min_score_index = np.argmin(avg_train_MSE)\n",
    "\n",
    "    train_MSE.append(avg_train_MSE[min_score_index])\n",
    "    cv_model = train_cv_result['estimator'][min_score_index]\n",
    "    trained_models.append(cv_model)\n",
    "\n",
    "    # Get the MSE score on the test data\n",
    "    y_pred = cv_model.predict(X_test_s)\n",
    "    y_pred_int = np.round(y_pred).astype(int) # Round predictions to nearest integer\n",
    "    comp_MSE = mean_squared_error(y_test, y_pred_int)\n",
    "    test_MSE.append(comp_MSE)\n",
    "\n",
    "# Print Results\n",
    "print(\"\\n\" \"Cross-Validation MSE on Training Data:\")\n",
    "for i in range(len(model_names)):\n",
    "        print(\"{}: {}\".format(model_names[i], train_MSE[i]))\n",
    "\n",
    "print(\"\\n\" \"MSE on Testing Data:\")\n",
    "for i in range(len(model_names)):\n",
    "    print(\"{}: {}\".format(model_names[i], test_MSE[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above analysis you can see that the models perform similarly on the testing and training data, but there are some slight differences. \n",
    "\n",
    "The Lasso and ElasticNet models have the highest MSE at ~3.50 and ~2.61. The SGDRegressor model is clearly the fifth place candidate based on its MSE of ~2.02 on the testing data.\n",
    "\n",
    "The remaining models have similar MSE scores on the test data. By obersving the Ridge and Linear Regression models and applying some critical thinking, we can conclude that when using mean_squared_error as the loss metric for the Linear Regression model it effectively becomes a Ridge model. Therefore the top three candidates are: Ridge, ARDRegression, and BayesianRidge.\n",
    "\n",
    "We will choose the Ridge model and the ARDRegression model as our top two models to perform hyperparameter tuning on.\n",
    "\n",
    "Note: The Ridge model only has one parameter for hyperparameter tuning, alpha. The BayesianRidge model has four parameters for tuning: alpha_1, alpha_2, lambda_1, lambda_2.\n",
    "\n",
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best alpha is {'alpha': 0.001}\n",
      "The cross-validated MSE for the best Ridge model is 2.6664906606232335\n",
      "The MSE of the best Ridge model versus the test data is 2.6720156459706694\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for Ridge model\n",
    "\n",
    "# Set up the parameter grid\n",
    "param_grid = {\n",
    "    \"alpha\" : [1E-10, 1E-9, 1E-8, 1E-7, 1E-6, 1E-5, 1E-4, 1E-3]}\n",
    "scoring = 'neg_mean_squared_error'\n",
    "\n",
    "# Perform GridSearchCV across the parameter grid\n",
    "grid_search = GridSearchCV(estimator = Ridge(), \n",
    "                           param_grid = param_grid, \n",
    "                           cv = 5,\n",
    "                           scoring = 'neg_mean_squared_error',\n",
    "                           return_train_score = True # Return the MSE for each alpha in .cv_results_\n",
    "                           )\n",
    "grid_search.fit(X_train_s, y_train)\n",
    "\n",
    "# Obtain the best model from grid search\n",
    "best_ridge_model = grid_search.best_estimator_\n",
    "best_ridge_param = grid_search.best_params_\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred = best_ridge_model.predict(X_test_s)\n",
    "test_MSE = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"The best alpha is {best_ridge_param}\")\n",
    "print(f\"The cross-validated MSE for the best Ridge model is {-grid_search.best_score_}\")\n",
    "print(f\"The MSE of the best Ridge model versus the test data is {test_MSE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross-validated MSE for the best BayesianRidge model is 2.6888014314823026\n",
      "The MSE of the best BayesianRidge versus the test data is 2.613380166701588\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for BayesianRidge model\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Set up the parameter grid\n",
    "param_grid = {\n",
    "    \"alpha_1\" : [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"alpha_2\" : [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"lambda_1\" : [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"lambda_2\" : [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \n",
    "}\n",
    "scoring = 'neg_mean_squared_error'\n",
    "\n",
    "# Perform RandomizedSearchCV across the parameter grid\n",
    "rand_search = RandomizedSearchCV(estimator = BayesianRidge(), \n",
    "                           param_distributions = param_grid, \n",
    "                           cv = 5,\n",
    "                           scoring = 'neg_mean_squared_error',\n",
    "                           return_train_score = True, # Return the MSE for each alpha in .cv_results_\n",
    "                           n_iter = 100,\n",
    "                           random_state = 42, \n",
    "                           n_jobs = -1\n",
    "                           )\n",
    "rand_search.fit(X_train_s, y_train[\"score_value_1\"])\n",
    "\n",
    "# Obtain the best model and parameters\n",
    "best_BAY_model = rand_search.best_estimator_\n",
    "best_params = rand_search.best_params_\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred = best_BAY_model.predict(X_test_s)\n",
    "test_MSE = mean_squared_error(y_test[\"score_value_1\"], y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"The cross-validated MSE for the best BayesianRidge model is {-rand_search.best_score_}\")\n",
    "print(f\"The MSE of the best BayesianRidge versus the test data is {test_MSE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_250</th>\n",
       "      <th>feature_251</th>\n",
       "      <th>feature_252</th>\n",
       "      <th>feature_253</th>\n",
       "      <th>feature_254</th>\n",
       "      <th>feature_255</th>\n",
       "      <th>feature_256</th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>assumed_score_2</th>\n",
       "      <th>assumed_score_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.123763</td>\n",
       "      <td>-0.117352</td>\n",
       "      <td>0.045677</td>\n",
       "      <td>0.015849</td>\n",
       "      <td>0.085833</td>\n",
       "      <td>-0.027624</td>\n",
       "      <td>0.003787</td>\n",
       "      <td>-0.08236</td>\n",
       "      <td>0.088994</td>\n",
       "      <td>-0.00169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.015938</td>\n",
       "      <td>0.059344</td>\n",
       "      <td>-0.162139</td>\n",
       "      <td>-0.024396</td>\n",
       "      <td>-0.03724</td>\n",
       "      <td>-0.043807</td>\n",
       "      <td>47</td>\n",
       "      <td>6.75</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 259 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0  -0.123763  -0.117352   0.045677   0.015849   0.085833  -0.027624   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_250  feature_251  \\\n",
       "0   0.003787   -0.08236   0.088994    -0.00169  ...     0.006923     0.015938   \n",
       "\n",
       "   feature_252  feature_253  feature_254  feature_255  feature_256  \\\n",
       "0     0.059344    -0.162139    -0.024396     -0.03724    -0.043807   \n",
       "\n",
       "   prompt_length  assumed_score_2  assumed_score_3  \n",
       "0             47             6.75         8.666667  \n",
       "\n",
       "[1 rows x 259 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating against the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Datasets\n",
    "\n",
    "# Embedding Data -- we will use this data in the \"Embedding Data\" section\n",
    "test_embeddings = np.load(\n",
    "    \"../testing_data/arena-test-set-prompts-embeddings.npy\"\n",
    ")\n",
    "\n",
    "# Topic Modeling and Hardness Score Data -- we will use this data in the \"Topic Modeling and Hardness Score Data\" section\n",
    "test_hardness = pd.read_json(\n",
    "    \"../testing_data/arena-test-set-topic-modeling.jsonl.gz\",\n",
    "    lines=True,\n",
    "    compression=\"gzip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_247</th>\n",
       "      <th>feature_248</th>\n",
       "      <th>feature_249</th>\n",
       "      <th>feature_250</th>\n",
       "      <th>feature_251</th>\n",
       "      <th>feature_252</th>\n",
       "      <th>feature_253</th>\n",
       "      <th>feature_254</th>\n",
       "      <th>feature_255</th>\n",
       "      <th>feature_256</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.055131</td>\n",
       "      <td>-0.115709</td>\n",
       "      <td>0.055225</td>\n",
       "      <td>0.050576</td>\n",
       "      <td>0.010953</td>\n",
       "      <td>-0.004206</td>\n",
       "      <td>0.062269</td>\n",
       "      <td>0.064194</td>\n",
       "      <td>0.06936</td>\n",
       "      <td>0.016847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035079</td>\n",
       "      <td>-0.044518</td>\n",
       "      <td>0.043931</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>-0.076169</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.008611</td>\n",
       "      <td>-0.01545</td>\n",
       "      <td>-0.033905</td>\n",
       "      <td>-0.057479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0  -0.055131  -0.115709   0.055225   0.050576   0.010953  -0.004206   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_247  feature_248  \\\n",
       "0   0.062269   0.064194    0.06936    0.016847  ...     0.035079    -0.044518   \n",
       "\n",
       "   feature_249  feature_250  feature_251  feature_252  feature_253  \\\n",
       "0     0.043931     0.000368    -0.076169     0.002721     0.008611   \n",
       "\n",
       "   feature_254  feature_255  feature_256  \n",
       "0     -0.01545    -0.033905    -0.057479  \n",
       "\n",
       "[1 rows x 256 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the prompt embeddings data into a pandas dataframe\n",
    "num_features = test_embeddings.shape[1]\n",
    "column_names = [f\"feature_{i+1}\" for i in range(num_features)]\n",
    "df_test_prompt = pd.DataFrame(test_embeddings, columns = column_names)\n",
    "df_test_prompt.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200, 256)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_prompt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for prompt length\n",
    "test_hardness[\"prompt_length\"] = test_hardness[\"prompt\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the topic modeling data\n",
    "for i in range(3):\n",
    "    test_hardness[f\"topic_modeling_{i+1}\"] = test_hardness[f\"topic_modeling_{i+1}\"].apply(\n",
    "        # Clean list element into an string\n",
    "        lambda x: \", \".join(x) if isinstance(x, list) else x\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>topic_modeling_1</th>\n",
       "      <th>topic_modeling_2</th>\n",
       "      <th>topic_modeling_3</th>\n",
       "      <th>prompt_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4f332ebd8cdc4ff2be74aa8828ff20d5</td>\n",
       "      <td>what do you think about the future of iran?</td>\n",
       "      <td>Future Prediction</td>\n",
       "      <td>Future Prediction</td>\n",
       "      <td>Future Prediction</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question_id  \\\n",
       "0  4f332ebd8cdc4ff2be74aa8828ff20d5   \n",
       "\n",
       "                                        prompt   topic_modeling_1  \\\n",
       "0  what do you think about the future of iran?  Future Prediction   \n",
       "\n",
       "    topic_modeling_2   topic_modeling_3  prompt_length  \n",
       "0  Future Prediction  Future Prediction             43  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hardness.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "895 1130\n"
     ]
    }
   ],
   "source": [
    "counter_in = 0\n",
    "counter_out = 0\n",
    "for topic in test_hardness[\"topic_modeling_1\"].unique():\n",
    "    if topic in (topic_and_hardness[\"topic_modeling_1\"].unique()):\n",
    "        counter_in += 1\n",
    "    else:\n",
    "        counter_out += 1\n",
    "\n",
    "print(counter_in, counter_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that out of the unique topics in the test data, only 895 of them are in the training data. The majority of topics in the test data are not in the training data. How can we find the assumed score? First, we can determine the cosine similarity between the embedding for the test prompt versus all of the training prompts. Then we can assume that the topic of the test prompt is the same topic as the most similar training prompt. Once we have a new assumed topic we can assume the hardness score is the average hardness score for that topic. This will allow us to have an assumed hardness score for every prompt embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create a function to determine the topic of the most similar training prompt.\n",
    "def assumed_topic(test_prompt_embedding, all_training_embeddings, topic_modeling):\n",
    "    # Reshape the test prompt embedding\n",
    "    test_prompt_embedding = np.array(test_prompt_embedding).reshape(1, -1)\n",
    "    # Determine the cosine similarity between the test prompt and all training prompts\n",
    "    test_similarity = cosine_similarity(test_prompt_embedding, all_training_embeddings).flatten()\n",
    "\n",
    "    # Determine the index of the most similar training prompt\n",
    "    index = np.argmax(test_similarity)\n",
    "\n",
    "    # Return the most similar training prompt\n",
    "    return topic_modeling.iloc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question_id', 'prompt', 'topic_modeling_1', 'topic_modeling_2',\n",
       "       'topic_modeling_3', 'prompt_length'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hardness.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_modeling_1</th>\n",
       "      <th>topic_modeling_2</th>\n",
       "      <th>topic_modeling_3</th>\n",
       "      <th>prompt_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Future Prediction</td>\n",
       "      <td>Future Prediction</td>\n",
       "      <td>Future Prediction</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic_modeling_1   topic_modeling_2   topic_modeling_3  prompt_length\n",
       "0  Future Prediction  Future Prediction  Future Prediction             43"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hardness = test_hardness.drop(columns = ['question_id', 'prompt'])\n",
    "test_hardness.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_251</th>\n",
       "      <th>feature_252</th>\n",
       "      <th>feature_253</th>\n",
       "      <th>feature_254</th>\n",
       "      <th>feature_255</th>\n",
       "      <th>feature_256</th>\n",
       "      <th>topic_modeling_1</th>\n",
       "      <th>topic_modeling_2</th>\n",
       "      <th>topic_modeling_3</th>\n",
       "      <th>prompt_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.055131</td>\n",
       "      <td>-0.115709</td>\n",
       "      <td>0.055225</td>\n",
       "      <td>0.050576</td>\n",
       "      <td>0.010953</td>\n",
       "      <td>-0.004206</td>\n",
       "      <td>0.062269</td>\n",
       "      <td>0.064194</td>\n",
       "      <td>0.06936</td>\n",
       "      <td>0.016847</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076169</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.008611</td>\n",
       "      <td>-0.01545</td>\n",
       "      <td>-0.033905</td>\n",
       "      <td>-0.057479</td>\n",
       "      <td>Future Prediction</td>\n",
       "      <td>Future Prediction</td>\n",
       "      <td>Future Prediction</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0  -0.055131  -0.115709   0.055225   0.050576   0.010953  -0.004206   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_251  feature_252  \\\n",
       "0   0.062269   0.064194    0.06936    0.016847  ...    -0.076169     0.002721   \n",
       "\n",
       "   feature_253  feature_254  feature_255  feature_256   topic_modeling_1  \\\n",
       "0     0.008611     -0.01545    -0.033905    -0.057479  Future Prediction   \n",
       "\n",
       "    topic_modeling_2   topic_modeling_3  prompt_length  \n",
       "0  Future Prediction  Future Prediction             43  \n",
       "\n",
       "[1 rows x 260 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the training data with the prompt embeddings\n",
    "df_test = pd.concat([df_test_prompt, test_hardness], axis = 1)\n",
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_250</th>\n",
       "      <th>feature_251</th>\n",
       "      <th>feature_252</th>\n",
       "      <th>feature_253</th>\n",
       "      <th>feature_254</th>\n",
       "      <th>feature_255</th>\n",
       "      <th>feature_256</th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>assumed_score_2</th>\n",
       "      <th>assumed_score_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.055131</td>\n",
       "      <td>-0.115709</td>\n",
       "      <td>0.055225</td>\n",
       "      <td>0.050576</td>\n",
       "      <td>0.010953</td>\n",
       "      <td>-0.004206</td>\n",
       "      <td>0.062269</td>\n",
       "      <td>0.064194</td>\n",
       "      <td>0.06936</td>\n",
       "      <td>0.016847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>-0.076169</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.008611</td>\n",
       "      <td>-0.01545</td>\n",
       "      <td>-0.033905</td>\n",
       "      <td>-0.057479</td>\n",
       "      <td>43</td>\n",
       "      <td>7.5</td>\n",
       "      <td>6.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 259 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0  -0.055131  -0.115709   0.055225   0.050576   0.010953  -0.004206   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_250  feature_251  \\\n",
       "0   0.062269   0.064194    0.06936    0.016847  ...     0.000368    -0.076169   \n",
       "\n",
       "   feature_252  feature_253  feature_254  feature_255  feature_256  \\\n",
       "0     0.002721     0.008611     -0.01545    -0.033905    -0.057479   \n",
       "\n",
       "   prompt_length  assumed_score_2  assumed_score_3  \n",
       "0             43              7.5         6.833333  \n",
       "\n",
       "[1 rows x 259 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Introduce the prediction heuristic for topic_modeling_2\n",
    "assumed_scores_2 = []\n",
    "assumed_scores_3 = []\n",
    "\n",
    "for i in range(test_hardness.shape[0]):\n",
    "    topic_2 = test_hardness[\"topic_modeling_2\"].iloc[i]\n",
    "    topic_3 = test_hardness[\"topic_modeling_3\"].iloc[i]\n",
    "    test_embedding = test_embeddings[i]\n",
    "\n",
    "    if topic_2 in topic_2_avg_scores:\n",
    "        # If topic is in the training data, use the average score for the topic\n",
    "        assumed_score_2 = topic_2_avg_scores[topic_2]\n",
    "    else:\n",
    "        # If the topic is not in the training data, use the average score for the topic of the most similar prompt.\n",
    "        most_similar_topic = assumed_topic(test_embedding,\n",
    "                                           valid_prompt_embeddings,\n",
    "                                           df_train[\"topic_modeling_2\"])\n",
    "        assumed_score_2 = topic_2_avg_scores[most_similar_topic]\n",
    "    assumed_scores_2.append(assumed_score_2)\n",
    "\n",
    "    if topic_3 in topic_3_avg_scores:\n",
    "        # If topic is in the training data, use the average score for the topic\n",
    "        assumed_score_3 = topic_3_avg_scores[topic_3]\n",
    "    else:\n",
    "        # If the topic is not in the training data, use the average score for the topic of the most similar prompt.\n",
    "        most_similar_topic = assumed_topic(test_embedding,\n",
    "                                           valid_prompt_embeddings,\n",
    "                                           df_train[\"topic_modeling_3\"])\n",
    "        assumed_score_3 = topic_3_avg_scores[most_similar_topic]\n",
    "    \n",
    "    assumed_scores_3.append(assumed_score_3)\n",
    "\n",
    "# Create the test dataframe\n",
    "df_test = pd.DataFrame()\n",
    "df_test[\"prompt_length\"] = test_hardness[\"prompt_length\"]\n",
    "df_test[\"assumed_score_2\"] = assumed_score_2\n",
    "df_test[\"assumed_score_3\"] = assumed_score_3\n",
    "# Concatenate the training data with the prompt embeddings\n",
    "df_test = pd.concat([df_test_prompt, df_test], axis = 1)\n",
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 6, 7, ..., 7, 6, 7])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale the data before making predictions\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(df_test)\n",
    "\n",
    "# Make Predictions\n",
    "y_pred = best_BAY_model.predict(X_train_s)\n",
    "y_pred.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>winner</th>\n",
       "      <th>hardness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4f332ebd8cdc4ff2be74aa8828ff20d5</td>\n",
       "      <td>model_b</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f2be6f13e5ed40e5b81443223996494c</td>\n",
       "      <td>model_b</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5fafefb8a0c54243afb52d2892946cea</td>\n",
       "      <td>model_b</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7834f572267f40709ecebb273a2b346b</td>\n",
       "      <td>model_a</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1ccc7e58290245c4bd5457fce45f8640</td>\n",
       "      <td>model_a</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>eb08f8a7f20840c99efe9fc8c03f1c13</td>\n",
       "      <td>model_a</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>4baca918f1f5440599ae9edb3bfa8cc1</td>\n",
       "      <td>model_b</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>a787ce60dc1440f39455ab20e3bffe33</td>\n",
       "      <td>model_b</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>3dc09f20eedb405ab3dc980cf7bff5d0</td>\n",
       "      <td>model_a</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>75b64403f4654bb9ad4311f4952e9571</td>\n",
       "      <td>model_b</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           question_id   winner  hardness_score\n",
       "0     4f332ebd8cdc4ff2be74aa8828ff20d5  model_b               9\n",
       "1     f2be6f13e5ed40e5b81443223996494c  model_b               9\n",
       "2     5fafefb8a0c54243afb52d2892946cea  model_b               9\n",
       "3     7834f572267f40709ecebb273a2b346b  model_a               9\n",
       "4     1ccc7e58290245c4bd5457fce45f8640  model_a               9\n",
       "...                                ...      ...             ...\n",
       "3195  eb08f8a7f20840c99efe9fc8c03f1c13  model_a               9\n",
       "3196  4baca918f1f5440599ae9edb3bfa8cc1  model_b               9\n",
       "3197  a787ce60dc1440f39455ab20e3bffe33  model_b               9\n",
       "3198  3dc09f20eedb405ab3dc980cf7bff5d0  model_a               9\n",
       "3199  75b64403f4654bb9ad4311f4952e9571  model_b               9\n",
       "\n",
       "[3200 rows x 3 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.read_csv(\"submission_20241129_084701.csv\")\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>winner</th>\n",
       "      <th>hardness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4f332ebd8cdc4ff2be74aa8828ff20d5</td>\n",
       "      <td>model_b</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f2be6f13e5ed40e5b81443223996494c</td>\n",
       "      <td>model_b</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5fafefb8a0c54243afb52d2892946cea</td>\n",
       "      <td>model_b</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7834f572267f40709ecebb273a2b346b</td>\n",
       "      <td>model_a</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1ccc7e58290245c4bd5457fce45f8640</td>\n",
       "      <td>model_a</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>eb08f8a7f20840c99efe9fc8c03f1c13</td>\n",
       "      <td>model_a</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>4baca918f1f5440599ae9edb3bfa8cc1</td>\n",
       "      <td>model_b</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>a787ce60dc1440f39455ab20e3bffe33</td>\n",
       "      <td>model_b</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>3dc09f20eedb405ab3dc980cf7bff5d0</td>\n",
       "      <td>model_a</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>75b64403f4654bb9ad4311f4952e9571</td>\n",
       "      <td>model_b</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           question_id   winner  hardness_score\n",
       "0     4f332ebd8cdc4ff2be74aa8828ff20d5  model_b               7\n",
       "1     f2be6f13e5ed40e5b81443223996494c  model_b               6\n",
       "2     5fafefb8a0c54243afb52d2892946cea  model_b               7\n",
       "3     7834f572267f40709ecebb273a2b346b  model_a               6\n",
       "4     1ccc7e58290245c4bd5457fce45f8640  model_a               7\n",
       "...                                ...      ...             ...\n",
       "3195  eb08f8a7f20840c99efe9fc8c03f1c13  model_a               7\n",
       "3196  4baca918f1f5440599ae9edb3bfa8cc1  model_b               7\n",
       "3197  a787ce60dc1440f39455ab20e3bffe33  model_b               7\n",
       "3198  3dc09f20eedb405ab3dc980cf7bff5d0  model_a               6\n",
       "3199  75b64403f4654bb9ad4311f4952e9571  model_b               7\n",
       "\n",
       "[3200 rows x 3 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df[\"hardness_score\"] = y_pred.astype(int)\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.17592709, -0.03469267,  0.15824214, ...,  0.25422472,\n",
       "         0.17422746,  0.24273388]], dtype=float32)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompt_embedding = test_embeddings[25]\n",
    "\n",
    "similarity = cosine_similarity(np.array(test_prompt_embedding).reshape(1, -1), valid_prompt_embeddings)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2308,  8667, 18111, ..., 19321, 10712, 13465]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_five_indices = np.argsort(similarity)[-5:][::-1]\n",
    "top_five_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msse-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
