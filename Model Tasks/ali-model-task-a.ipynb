{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf2e8764-1ae7-48f5-8e82-36ba532ca766",
   "metadata": {},
   "source": [
    "# EDA of Chatbot Arena Dataset \n",
    "\n",
    "The purpose of this notebook aims to explore the Chatbot Arena dataset, where two chatbots answer human questions, and users vote on the best response. The EDA of Chatbot Arena Dataset will cover:\n",
    "- Understanding the dataset structure and contents.\n",
    "- Exploring the distribution of questions, responses, and chatbots.\n",
    "- Identifying patterns in the data to guide future modeling tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbcac9f-fb7a-4322-8e68-997244597049",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "\n",
    "- Conversation Data\n",
    "- Prompts and Models Response Embeddings\n",
    "- Topic Modeling and Hardness Score Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1fdffe-8626-4b06-9d82-043f643d61a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gzip\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e824d5c-ec3a-4c01-8a19-f31a4df0ee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- Main Dataset ----#\n",
    "\n",
    "# Conversation Data\n",
    "conversation = pd.read_json(\n",
    "    \"/home/jovyan/shared/course/data100-shared-readwrite/fa24_grad_project_data/nlp-chatbot-analysis_data/training-set/chatbot-arena-conversations.jsonl.gz\",\n",
    "    lines=True,\n",
    "    compression=\"gzip\"\n",
    ")\n",
    "\n",
    "\n",
    "#---- Auxiliary Datasets ----#\n",
    "\n",
    "# Embedding Data -- we will use this data in the \"Embedding Data\" section\n",
    "prompt_embeddings = np.load(\n",
    "    \"/home/jovyan/shared/course/data100-shared-readwrite/fa24_grad_project_data/nlp-chatbot-analysis_data/training-set/chatbot-arena-prompts-embeddings.npy\"\n",
    ")\n",
    "\n",
    "response_a_embeddings = np.load(\n",
    "    \"/home/jovyan/shared/course/data100-shared-readwrite/fa24_grad_project_data/nlp-chatbot-analysis_data/training-set/chatbot-arena-model_a_response-embeddings.npy\"\n",
    ")\n",
    "\n",
    "response_b_embeddings = np.load(\n",
    "    \"/home/jovyan/shared/course/data100-shared-readwrite/fa24_grad_project_data/nlp-chatbot-analysis_data/training-set/chatbot-arena-model_b_response-embeddings.npy\"\n",
    ")\n",
    "\n",
    "# Topic Modeling and Hardness Score Data -- we will use this data in the \"Topic Modeling and Hardness Score Data\" section\n",
    "topic_and_hardness = pd.read_json(\n",
    "    \"/home/jovyan/shared/course/data100-shared-readwrite/fa24_grad_project_data/nlp-chatbot-analysis_data/training-set/chatbot-arena-gpt3-scores.jsonl.gz\",\n",
    "    lines=True,\n",
    "    compression=\"gzip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae16e06-2290-4ce1-85fd-427e879d63e0",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea5db41-3ce1-4c4d-af6e-9d68a705cec3",
   "metadata": {},
   "source": [
    "## Conversations Dataset EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0615866-df95-434c-8a7d-2b9168f926ee",
   "metadata": {},
   "source": [
    "Let's investigate the conversation data first (`chatbot-arena-conversations.jsonl.gz`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b1e62c-d13f-4d2d-bc38-0771eb45f393",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(conversation.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e60481-5128-4473-ba35-096e887dccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(conversation.info())\n",
    "display(conversation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59faba1-55ec-43d7-ac1b-957fa0669038",
   "metadata": {},
   "source": [
    "#### Granularity and Cleaning\n",
    "**Granularity**: the data is on the question-level. The dataset contains conversations from users with different chatbot models. In a row we can see one prompt from a user and the response from two chatbots models for that prompt. The specific models can be different for each conversation. All the columns contain qualitative nominal variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fc8b20-bcc1-4e13-92f6-2efad9fdc7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in data\n",
    "conversation.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d733e2f4-b2af-4e70-9509-70d5b9f17b24",
   "metadata": {},
   "source": [
    "**Null Values:** there are no null values in the data.\n",
    "\n",
    "Next, lets create a winner_model column for easy analysis. For this, we will map 'tie' and 'tie (bothbad)' to just 'tie' string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a286a0e9-414b-4248-8ae9-eee1b170bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation['winner_model'] = conversation.apply(lambda row: row['model_a'] if row['winner'] == 'model_a' else (row['model_b'] if row['winner'] == 'model_b' else 'tie'), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa075fc-0cc5-46da-b1be-80031d1eb71d",
   "metadata": {},
   "source": [
    "Let's check the structure of the conversation column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacfe2e8-cb26-46f9-955d-53b444f47c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation[\"conversation_a\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03528928-5ad8-4f3c-9917-167b3cb14851",
   "metadata": {},
   "source": [
    "Lets extract out the prompt and chatbot model responses into separate columns. For this we will create separate columns for prompt_a & prompt_b and separate columns for models' reponse_a & response_b. We will also add columns for their length fields and a column for the winner_response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f803bd19-c9ee-4c8a-902a-c793eede4f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation['conversation_a_user_prompt'] = conversation[\"conversation_a\"].str[0].str[\"content\"]\n",
    "conversation['conversation_b_user_prompt'] = conversation[\"conversation_b\"].str[0].str[\"content\"]\n",
    "conversation['user_prompt_length'] = conversation['conversation_a_user_prompt'].apply(len)\n",
    "conversation['model_a_response'] = conversation['conversation_a'].apply(lambda x: x[1]['content'])\n",
    "conversation['model_a_response_length'] = conversation['model_a_response'].apply(len)\n",
    "conversation['model_b_response'] = conversation['conversation_b'].apply(lambda x: x[1]['content'])\n",
    "conversation['model_b_response_length'] = conversation['model_b_response'].apply(len)\n",
    "conversation['winner_response'] = conversation.apply(lambda row: row['model_a_response'] if row['winner'] == 'model_a' else (row['model_b_response'] if row['winner'] == 'model_b' else 'tie'), axis=1)\n",
    "conversation['winner_response_length'] = conversation['winner_response'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfa3e4d-d3a8-4a9a-9654-e7e20140659a",
   "metadata": {},
   "source": [
    "Next, check if the user prompt is the same in all rows. If it is different in any row, we will need to fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5e9852-3254-46f7-bee3-e64d238850ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(conversation['conversation_a_user_prompt'] != conversation['conversation_b_user_prompt']) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8a63fb-1b48-4a92-968f-52b4a6439f41",
   "metadata": {},
   "source": [
    "As we can see the user prompt is the same. That means we don't need both the columns we can drop one of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f75c6ac-423e-4a8c-b035-c3ea20f312c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = conversation.drop(columns=['conversation_b_user_prompt'])\n",
    "conversation = conversation.rename(columns={\"conversation_a_user_prompt\": \"user_prompt\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca268e7-8320-4984-ae99-c13d06be577b",
   "metadata": {},
   "source": [
    "#### Cleaned Dataset\n",
    "Lets look at the claned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d4475-3c0b-4173-8a0e-acfdf167a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c506e6a-1c9c-4ee9-b177-6e763e4276dc",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7e66b6-5f03-4387-a273-badf94d45591",
   "metadata": {},
   "source": [
    "Now view the statistics of the prompt lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9669944-42b1-4fff-a94a-bc831e7e9617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View statistics for the prompt length\n",
    "conversation['user_prompt_length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ede974e-23b9-4d57-a39e-43463b9b584e",
   "metadata": {},
   "source": [
    "Since the median is less than the mean prompt length, this distribution is right skewed. Let's visualize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c867dafa-72ee-4c7f-82d9-7d056e3accc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the length of the prompt\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(conversation[\"user_prompt_length\"], kde=True, color='purple')\n",
    "plt.title(\"Distribution of Prompt Length\")\n",
    "plt.xlabel('User Prompt Length')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5d0ad0-8c37-4299-ab54-80ccc83fa9ed",
   "metadata": {},
   "source": [
    "Let's apply this same process to the reponse length for models a and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28027ebb-d15c-4b5d-b12d-6ea2cfb96787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View statistics for the models' responses\n",
    "\n",
    "display(conversation['model_a_response_length'].describe())\n",
    "\n",
    "display(conversation['model_b_response_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dff682-cc46-451a-a054-e706e60b7c9c",
   "metadata": {},
   "source": [
    "Let's visualize the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1ed08c-9b91-4bb3-b575-ffee43493d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the subplot with 1 row and 2 columns\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot the distribution of the length of the response of model a.\n",
    "sns.histplot(conversation[\"model_a_response_length\"], ax=ax1, color='blue', kde=True)\n",
    "ax1.set_title('Distribution of Response Length of Model A')\n",
    "ax1.set_xlabel('Model A Response Length')\n",
    "\n",
    "# Plot the distribution of the length of the response of model b.\n",
    "sns.histplot(conversation[\"model_b_response_length\"], ax=ax2, color='red', kde=True)\n",
    "ax2.set_title('Distribution of Response Length of Model B')\n",
    "ax2.set_xlabel('Model B Response Length')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc05addd-2c3d-49ac-9efa-a75557f312b3",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "\n",
    "- The distribution of user_prompt_length is heavily right-skewed, with the vast majority of prompts being very short. Most prompts fall below 500 characters, and only a few exceed 1000 characters, indicating that users tend to ask concise questions or make brief statements. The density around low values suggests most users short queries, likely without extensive context.\n",
    "- Both model_a_response_length and model_b_response_length distributions are similarly right-skewed but extend further than the prompt length, reflecting that models tend to generate responses longer than the prompts. Most responses are below 1000 characters, but both models occasionally produce responses up to 4000 characters or more. This indicates that the models adapt to provide detailed responses when needed, though such cases are relatively rare.\n",
    "- The shapes of the distributions for model_a_response_length and model_b_response_length are quite similar, with comparable peaks and tails, suggesting that both models generate responses of similar length distributions. The high count around the shorter lengths (<500 characters) in both models indicates a tendency to provide concise answers for most prompts, aligning with the user’s tendency to submit shorter queries.\n",
    "- Both response distributions have some long-tail behavior, with a few responses exceeding 4000 characters. This likely represents instances where models generated detailed responses, possibly for complex or multi-part questions. The outliers indicate the models’ flexibility in producing longer responses when the prompt demands it, though these instances are rare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257980fc-b863-4cd2-93a1-358b03854514",
   "metadata": {},
   "source": [
    "Next, lets examine the **distribution of different chatbot models** (both model A and B) used in the conversations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d9aee2-0920-44a5-afea-79e2db4f2d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the subplot with 1 row and 2 columns\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# First bar chart\n",
    "sns.countplot(data=conversation, x='model_a', ax=ax1, palette='Blues_d')\n",
    "ax1.set_title('Chatbot Model A Usage (Win/Loss Agnostic)')\n",
    "ax1.set_xlabel('Model A')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=90)\n",
    "\n",
    "# Second bar chart\n",
    "sns.countplot(data=conversation, x='model_b', ax=ax2, palette='Reds_d')\n",
    "ax2.set_title('Chatbot Model B Usage (Win/Loss Agnostic)')\n",
    "ax2.set_xlabel('Model B')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.tick_params(axis='x', rotation=90)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb2f498-c3d7-4bd9-9ccc-a01fc1755ac6",
   "metadata": {},
   "source": [
    "Now lets check the **ranking of models** i.e. the distribution of the winner models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df1240b-d6b9-4d7d-80a3-438d1c242fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "winners = conversation['winner_model'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(winners.index, winners.values, color='lightgreen')\n",
    "plt.title('Number of Times Each Model Won')\n",
    "plt.xlabel(\"Number of Times Won\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db56a223-8301-4e1e-b338-85ef59f1267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "winners_without_tie_df = conversation[conversation['winner_model'] != 'tie']\n",
    "winners_without_tie = winners_without_tie_df['winner_model'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(winners_without_tie.index, winners_without_tie.values, color='darkgreen')\n",
    "plt.title('Ranking of Models: Number of Times Each Model Won (Excluding Ties)')\n",
    "plt.xlabel(\"Number of times won\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e2f752-d9a8-454c-a303-03a4f136cac0",
   "metadata": {},
   "source": [
    "Get the win counts of the top-5 winning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e7492-dc05-4f19-a9b2-4e892ed735d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the top five highest performing models based on win counts\n",
    "top_models = winners_without_tie_df.groupby([\"winner_model\"]).size().reset_index(name = \"win_count\")\n",
    "top_models = top_models.sort_values(by = \"win_count\", ascending = False).head()\n",
    "top_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a47653-b154-40c0-9390-cba7bb803153",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "- GPT-4 has the highest win count overall, showing clear dominance. Vicuna-13b and GPT-3.5-turbo are also strong performers, ranking close behind GPT-4 in win counts. When ties are removed, these top models maintain their strong positions, indicating consistent performance in head-to-head comparisons.\n",
    "- Ties occur frequently, with a significant portion of outcomes marked as ties, indicating that users often find models to be similarly effective.\n",
    "- The top models by win count without ties are GPT-4, Vicuna-13b, GPT-3.5-turbo, Claude-v1, and Koala-13b. These models outperform the others consistently, suggesting that they provide more satisfying or accurate responses across diverse prompts.\n",
    "- Models like GPT4All-13b-snoozy, Llama-13b, and Wizardlm-13b have very low win counts, suggesting they are less competitive in this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35888b6-ff0c-41a6-9b46-a3f7afd45257",
   "metadata": {},
   "source": [
    "Next, let's look at the **judge** column that contains the anonymized usernames who held the converstaion and picked the winner model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b8323-42a9-4784-bed3-fdffd0d02567",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(conversation['judge'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da769afd-05ee-40c2-b6b0-2c5808f0ff97",
   "metadata": {},
   "source": [
    "The dataset contains 10531. Let's check the distribution of number of conversations per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94ab31b-1a6e-437c-ace5-b0856d3c2ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_value_counts = conversation['judge'].value_counts()\n",
    "display(judge_value_counts)\n",
    "\n",
    "display('Total users with more than 50 conversations: ' + str(len(judge_value_counts[judge_value_counts > 50])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34ab72b-74bc-4d61-aef7-a94d19d62534",
   "metadata": {},
   "source": [
    "Out of 10531 users, only 34 users have had more than 50 conversations. So we will get the top 20 users according to the number of conversations held and check the winner models that they frequently picked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05735b5d-b6b4-4836-8f7c-c316e0e89c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "top20_users = conversation['judge'].value_counts()[:20].index\n",
    "models_picked_by_top20_users = conversation[conversation['judge'].isin(top20_users)]\n",
    "models_picked_by_top20_users_pivot = models_picked_by_top20_users.pivot_table(index='winner_model', columns='judge', aggfunc='size', fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(models_picked_by_top20_users_pivot, annot=True, cmap=\"YlGnBu\", fmt=\"d\", linewidths=.5)\n",
    "plt.title(\"Heatmap of Judge Model Preferences In Top-20 Users\")\n",
    "plt.xlabel(\"Judge\")\n",
    "plt.ylabel(\"Winner Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6378e2a9-30eb-463b-a2d0-626733ccd92c",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "- Ties are common, with high counts in each judge’s choices (e.g., 115 ties by arena_user_13210), indicating frequent parity between models in responses.\n",
    "- GPT-4, GPT-3.5-turbo, and Claude-v1 are among the most frequently chosen models, with high counts across several judges (e.g., GPT-4 has 22 wins with arena_user_11473). These models stand out as popular choices across different judges, reinforcing their strong performance.\n",
    "- Some judges show distinct preferences. For example, arena_user_13210 selects Claude-v1 frequently (22 times), suggesting a possible personal bias or specific evaluation criteria. arena_user_15085 prefers GPT-4 and GPT-3.5-turbo more than other models, indicating consistency in these models' appeal.\n",
    "- Models like GPT4All-13b-snoozy and Guanaco-33b are rarely chosen as winners, further highlighting their lower competitiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a77fdc8-42e7-4dd5-9e85-7bb35858d01c",
   "metadata": {},
   "source": [
    "Next, let's check model-pairing performace i.e. within the same pair of models, did some models consistently perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836e224-88de-4b3f-b12d-29f88140781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter where model_a is the winner and create a count table\n",
    "df_model_a_wins = winners_without_tie_df[winners_without_tie_df['winner_model'] == winners_without_tie_df['model_a']]\n",
    "pairing_performance_a_over_b = df_model_a_wins.pivot_table(\n",
    "    index='model_a', \n",
    "    columns='model_b', \n",
    "    aggfunc='size', \n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Filter where model_b is the winner and create a count table\n",
    "df_model_b_wins = winners_without_tie_df[winners_without_tie_df['winner_model'] == winners_without_tie_df['model_b']]\n",
    "pairing_performance_b_over_a = df_model_b_wins.pivot_table(\n",
    "    index='model_b', \n",
    "    columns='model_a', \n",
    "    aggfunc='size', \n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Plot both heatmaps side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Heatmap for model_a wins over model_b\n",
    "sns.heatmap(pairing_performance_a_over_b, annot=True, cmap=\"Blues\", fmt=\"d\", linewidths=.5, ax=ax1)\n",
    "ax1.set_title(\"Model Pairing Performance: Wins for model_a over model_b\")\n",
    "ax1.set_xlabel(\"Model B\")\n",
    "ax1.set_ylabel(\"Model A\")\n",
    "\n",
    "# Heatmap for model_b wins over model_a\n",
    "sns.heatmap(pairing_performance_b_over_a, annot=True, cmap=\"Blues\", fmt=\"d\", linewidths=.5, ax=ax2)\n",
    "ax2.set_title(\"Model Pairing Performance: Wins for model_b over model_a\")\n",
    "ax2.set_xlabel(\"Model A\")\n",
    "ax2.set_ylabel(\"Model B\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff360f92-2e31-44f9-ba0a-4ade2a57ffd2",
   "metadata": {},
   "source": [
    "Let's also visualize the average win rate of each model against all other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e25a08-7bbf-4c9d-8f2c-03900505674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total wins for each model\n",
    "win_counts = winners_without_tie_df['winner_model'].value_counts()\n",
    "\n",
    "# Calculate the total appearances for each model (model_a + model_b counts)\n",
    "model_a_counts = winners_without_tie_df['model_a'].value_counts()\n",
    "model_b_counts = winners_without_tie_df['model_b'].value_counts()\n",
    "total_matches = model_a_counts.add(model_b_counts, fill_value=0)\n",
    "\n",
    "# Ensure both Series (win_counts and total_matches) have the same index\n",
    "total_matches = total_matches.reindex(win_counts.index, fill_value=0)\n",
    "\n",
    "# Compute the average win rate and handle any NaNs or infinities\n",
    "average_win_rate = win_counts / total_matches\n",
    "average_win_rate = average_win_rate.fillna(0)\n",
    "\n",
    "# Prepare the DataFrame for plotting\n",
    "win_rate_df = pd.DataFrame({\n",
    "    'Model': average_win_rate.index,\n",
    "    'Average Win Rate': average_win_rate.values\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by win rate for better visualization\n",
    "win_rate_df = win_rate_df.sort_values(by='Average Win Rate', ascending=False)\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(data=win_rate_df, x='Model', y='Average Win Rate', color='dodgerblue')\n",
    "\n",
    "# Add labels on top of each bar\n",
    "for i in range(len(win_rate_df)):\n",
    "    rate = win_rate_df['Average Win Rate'].iloc[i]\n",
    "    # Only add labels if they are finite values\n",
    "    if np.isfinite(rate):\n",
    "        plt.text(i, rate + 0.01, f\"{rate:.2f}\", ha='center', color='black', fontsize=10)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Average Win Rate Against All Other Models')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Average Win Rate')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6df67d-d30b-4709-b0ad-6f657fb83520",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "\n",
    "We have the following takeaways from the two heatmaps above:\n",
    "\n",
    "1. Consistently Strong Models \n",
    "- GPT-4: This model stands out with high win counts against a wide range of opponents. For example:\n",
    "    - GPT-4 vs GPT-3.5-turbo: GPT-4 won 115 times when it was model_a and 95 times as model_b. This suggests a strong advantage over GPT-3.5-turbo in head-to-head comparisons.\n",
    "    - GPT-4 vs Claude-v1: Similarly, GPT-4 won 89 times as model_a and 78 times as model_b, indicating it consistently outperformed Claude-v1 regardless of its role.\n",
    "- Vicuna-13b: Another high-performing model, Vicuna-13b shows significant wins against many models.\n",
    "    - Vicuna-13b vs GPT-3.5-turbo: Vicuna-13b won 85 times as model_a and 73 times as model_b, illustrating its reliability in this matchup.\n",
    "    - Vicuna-13b vs Claude-v1: This model also performed well against Claude-v1, winning 61 times as model_a and 58 times as model_b.\n",
    "\n",
    "3. Consistently Weaker Models: GPT4All-13b-Snoozy and Guanaco-33b have some of the lowest win counts across both heatmaps, indicating that they are generally less competitive.\n",
    "- GPT4All-13b-Snoozy: Wins only occasionally, such as 6 times against Claude-v1 when it was model_b, and 5 times against GPT-3.5-turbo as model_a. Overall, it struggles against most models.\n",
    "- Guanaco-33b Shows similar weaknesses, with wins as low as 3 against GPT-4 as model_b, and only 5 times against Claude-instant-v1 as model_a. This low win count across pairings suggests it may lack robustness or versatility.\n",
    "\n",
    "4. Models From The Same Series\n",
    "- PT-3.5-turbo vs GPT-4: This pairing is particularly interesting, as GPT-4 shows a clear advantage, but GPT-3.5-turbo still manages some wins. As model_b GPT-3.5-turbo won 63 times against GPT-4, indicating it can occasionally outperform GPT-4 depending on the context or prompt. This matchup suggests that while GPT-4 is generally stronger, GPT-3.5-turbo may have certain contexts or types of conversations where it competes well.\n",
    "- Claude-v1 vs Claude-instant-v1: Both models from the Claude family, but Claude-v1 shows consistently higher win counts. Claude-v1 won 55 times as model_a against Claude-instant-v1 and 45 times as model_b. This suggests that Claude-v1 might be a more robust model than Claude-instant-v1, even though they belong to the same family.\n",
    "\n",
    "5. Asymmetrical Pairing Performance: the asymmetrical results suggest that some models might perform differently depending on their position (model_a vs. model_b). This could be due to variations in prompt handling, response strategies, or even random fluctuations in performance. we will explore this further later in our EDA.\n",
    "- laude-v1 vs Vicuna-13b: In this pairing, Claude-v1 performs relatively well when it’s model_b, winning 58 times, compared to 61 wins for Vicuna-13b. However, when Claude-v1 is model_a, Vicuna-13b wins significantly more frequently, with 85 wins compared to 40 for Claude-v1.\n",
    "- GPT-3.5-turbo vs Claude-instant-v1: When GPT-3.5-turbo is model_a, it wins 42 times compared to 18 wins for Claude-instant-v1. But when Claude-instant-v1 is model_a, it wins more often, with 33 wins versus 30 for GPT-3.5-turbo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16aa320-8302-40d3-aac8-e79881d8fff4",
   "metadata": {},
   "source": [
    "Next, let's analyze the response of the top-5 winnings models and of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7475c38-0579-43be-9737-c598daeb46f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the top models to only include the top five models\n",
    "top_five = list(top_models[\"winner_model\"])\n",
    "df_top_five = winners_without_tie_df[winners_without_tie_df[\"winner_model\"].isin(top_five)]\n",
    "best_model = winners_without_tie_df[winners_without_tie_df[\"winner_model\"] == 'gpt-4']\n",
    "\n",
    "# Set up the subplot with 1 row and 2 columns\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# First histogram\n",
    "sns.histplot(data = df_top_five, x = \"winner_response_length\", hue = \"winner_model\", kde = True, ax=ax1)\n",
    "ax1.set_title(\"Distribution of Top Five Winning Models\")\n",
    "ax1.set_xlabel(\"Winning Response Length\")\n",
    "\n",
    "# Second histogram\n",
    "sns.histplot(best_model[\"winner_response_length\"], kde=True, ax=ax2)\n",
    "ax2.set_title(\"Distribution of Response Length of the Best Model\")\n",
    "ax2.set_xlabel(\"Winning Response Length\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aa7531-ad77-48a4-a9f8-9eb6594a6df7",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "- All top 5 winning models have right-skewed distributions, with most responses clustering below 1000 characters. This indicates a user preference for concise answers across models. However, to confirm that we still need to check the distribution of the loser model responses to see if there is a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251bcfba-5b3f-43e9-aad9-561945247e15",
   "metadata": {},
   "source": [
    "Let's also check the summary statistics of winner and loser models side-by-side w.r.t the response lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafc851e-55e2-4629-8c1d-2fe5b5cc125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "violit_plot_conversation_copy = conversation.copy()\n",
    "\n",
    "# Create a column indicating whether each response is from the winning or losing model\n",
    "violit_plot_conversation_copy['model_a_win'] = violit_plot_conversation_copy['winner_model'] == violit_plot_conversation_copy['model_a']\n",
    "violit_plot_conversation_copy['model_b_win'] = violit_plot_conversation_copy['winner_model'] == violit_plot_conversation_copy['model_b']\n",
    "\n",
    "# Add columns indicating if the response is from the winner or loser\n",
    "violit_plot_conversation_copy['model_a_response_type'] = violit_plot_conversation_copy['model_a_win'].apply(lambda x: 'winner' if x else 'loser')\n",
    "violit_plot_conversation_copy['model_b_response_type'] = violit_plot_conversation_copy['model_b_win'].apply(lambda x: 'winner' if x else 'loser')\n",
    "\n",
    "# Reshape the DataFrame for plotting\n",
    "lengths_df = pd.DataFrame({\n",
    "    'Length': violit_plot_conversation_copy['model_a_response_length'].tolist() + violit_plot_conversation_copy['model_b_response_length'].tolist(),\n",
    "    'Type': violit_plot_conversation_copy['model_a_response_type'].tolist() + violit_plot_conversation_copy['model_b_response_type'].tolist(),\n",
    "    'Model': ['Model A'] * len(violit_plot_conversation_copy) + ['Model B'] * len(violit_plot_conversation_copy)\n",
    "})\n",
    "\n",
    "# Plotting the violin plot with response lengths separated by winner/loser status\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(x='Model', y='Length', hue='Type', data=lengths_df, palette='Set2', split=True)\n",
    "plt.yscale('log')  # Log scale to accommodate length variability\n",
    "plt.title(\"Distribution of Response Lengths for Winning and Losing Models\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Text Length (characters, log scale)\")\n",
    "plt.legend(title=\"Response Type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3c93a-00de-4204-9b4a-110980861924",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "- This negates our earlier belief that only winner models have shorter response length. We can see that Both Model A and Model B show overlapping distributions for winners and losers, indicating that response length alone does not determine winning or losing. The models produce similar-length responses regardless of outcome.\n",
    "- However, we do see that winners have a slightly wider spread in length, especially in the higher range (10^3 characters or more). This suggests that winning responses may sometimes be more elaborate or detailed but not consistently so.\n",
    "- The similarity in distribution shape for both Model A and Model B implies that both models respond with comparable verbosity, regardless of whether they win or lose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bd5198-82e9-4a14-aaba-81e7ce8c3721",
   "metadata": {},
   "source": [
    "### Key Observations and Takeaways From Conversation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e881bc0-56bf-4485-b1b9-1ae73d2705ed",
   "metadata": {},
   "source": [
    "- **Do arena users ask long or short questions? What is the average length of prompts users give to the chatbots?**\n",
    "Most users ask pretty short questions, with prompt lengths heavily skewed toward the shorter end—under 500 characters. Very few prompts go beyond 1000 characters, which suggests that users tend to ask straightforward questions or make simple statements without too much detail.\n",
    "\n",
    "- **Describe the length of the response for models in general. In general, what is the average response length for a model?**\n",
    "Both `model_a_response_length` and `model_b_response_length` show similar right-skewed distributions, generally producing responses longer than the prompts. Most responses are under 1000 characters, but some reach up to 4000 characters, indicating that the models are capable of generating more detailed responses when needed.\n",
    "\n",
    "- **What are the top five models based on winning from either model a or model b?**\n",
    "The top five winning models are GPT-4, Vicuna-13b, GPT-3.5-turbo, Claude-v1, and Koala-13b. These models consistently outperform others in head-to-head comparisons, showing they’re able to handle diverse prompts well.\n",
    "\n",
    "- **Describe the distribution of the top five models based solely on wins. What is the average response length of the top five models compared to models in general?**\n",
    "or the top five models, response lengths show a similar right-skewed pattern, with most responses under 1000 characters. GPT-4, however, stands out a bit with a wider range, showing it can produce more detailed responses when needed, which may contribute to its high win rate.\n",
    "\n",
    "- **What is the best model based solely on wins? Describe its distribution and compare its statistics versus the general case of the top five models.**\n",
    "GPT-4 is identified as the best model based on win count. Its response length distribution is similar to the other top models but shows a broader range, especially on the longer side. This flexibility in producing both concise and detailed answers may be a factor in its consistent wins.\n",
    "\n",
    "- **Does response length alone determine winning or losing?**\n",
    "Response length by itself doesn’t seem to make or break a win. Both `Model A` and `Model B` show similar response length patterns for winners and losers, suggesting that other factors—like response quality—probably play a bigger role. That said, winning responses sometimes have a slightly wider spread in length, especially at the upper end.\n",
    "\n",
    "- **How frequently do ties occur, and what might this indicate about model performance?**\n",
    "Ties are fairly common, with certain judges choosing \"tie\" often. This points to a lot of instances where models perform similarly, suggesting that both may be providing equally useful or relevant responses.\n",
    "\n",
    "- **Are there judge-specific preferences or biases?**\n",
    "Yes, some judges do show clear favorites. For example, `arena_user_13210` often picks Claude-v1, while `arena_user_15085` leans towards GPT-4 and GPT-3.5-turbo. This could hint at personal biases or just consistent preferences for certain response styles based on the topics they're interested in\n",
    "\n",
    "- **Which models are consistently strong or weak across different pairings?**\n",
    "GPT-4** and Vicuna-13b are strong performers, winning against many other models across the board. On the flip side, models like GPT4All-13b-snoozy and Guanaco-33b rarely win, which suggests they struggle to keep up with the competition. GPT-4 stands out with strong results against GPT-3.5-turbo and Claude-v1, showing a clear competitive edge.\n",
    "\n",
    "- **Do models from the same family (e.g., Claude-v1 vs Claude-instant-v1) perform similarly?**\n",
    "Not always. While Claude-v1 and Claude-instant-v1 come from the same family, Claude-v1 consistently performs better, hinting at differences in capability even among related models.\n",
    "\n",
    "- **Are there any notable asymmetries in model performance based on their position as `model_a` or `model_b`?**\n",
    "Yes, in some cases, models perform differently depending on whether they’re in the `model_a` or `model_b` position. For example, Claude-v1 does better as `model_b` against Vicuna-13b, while GPT-3.5-turbo has more wins as `model_a` against Claude-instant-v1. These differences could be due to varying prompt-handling techniques or just random performance fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b8abee-7be1-4c92-83a7-da1bade51491",
   "metadata": {},
   "source": [
    "## Auxilliary Data Set: Embeddings - EDA\n",
    "\n",
    "Text embedding models transform natural language text into numerical vectors. The vectors are generated in such a way that semantically similar text are close to each other in the vector space. In the real world, these embeddings to find similar questions or to cluster the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772f176b-7ea0-4229-8350-5471776e9abc",
   "metadata": {},
   "source": [
    "#### Prompt Embedding Data (`./chatbot-arena-prompts-embeddings.npy`)\n",
    "This dataset contains 256-dimensional text embeddings for each of the user prompts, each embedding is a 256-dimensional vector that represents the semantic meaning of a prompt. These vectors allow us to compare prompts based on their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf68cbe-01ff-4473-ab3b-1cd8051f271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the data\n",
    "prompt_embeddings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3dce96-10f7-4630-a0ab-4598b9e9b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the data\n",
    "prompt_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb19a3-d1a4-435a-84f8-f89dab12c665",
   "metadata": {},
   "source": [
    "**Shape:** The embeddings array has a shape of `(25282, 256)`, meaning there are 25,282 embeddings (one for each prompt), and each embedding is a 256-dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1407a6e3-386d-41b0-a157-6c82ea22bbc5",
   "metadata": {},
   "source": [
    "Next, we will explore the embeddings by themselves without combining with the main dataset:\n",
    "1. Take a sample of the embeddings to reduce computation time.\n",
    "2. Compute the dot product between the embeddings to measure similarity.\n",
    "3. Retrieve the most similar prompts to a chosen source prompt.\n",
    "4. Output the top 5 similar prompts.\n",
    "\n",
    "Start by taking a sample of the embeddings and calculating the similarity between them using the dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ffac22-102f-41a6-9f0d-268f4f70ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the first 1000 prompts.\n",
    "embeddings_sample = prompt_embeddings[:1000]\n",
    "\n",
    "# Measure the similarity of the embeddings by computing the dot product.\n",
    "dot_product = np.dot(embeddings_sample, embeddings_sample.T)\n",
    "\n",
    "# Check the shape of the dot product\n",
    "dot_product.shape\n",
    "dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3bc593-fd0b-41b2-bc47-776e22a43bc5",
   "metadata": {},
   "source": [
    "Given the above output, the dot product matrix has a shape of (1000, 1000), meaning we have similarity scores between all pairs of prompts in our sample of 1000.\n",
    "\n",
    "Next, let's choose a prompt and find the top 5 most similar prompts based on the computed similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d1b472-98e0-43c8-9073-0b952bf9f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a prompt from the dataframe based on integer indexing.\n",
    "source_prompt_idx = 16\n",
    "source_prompt = conversation.iloc[source_prompt_idx]['user_prompt']\n",
    "source_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b656a-72cd-401e-b98f-ad3b7a96404c",
   "metadata": {},
   "source": [
    "The prompt we're using as a reference (index 16) is:\n",
    "\n",
    "`'could you explain quantum mechanics for me?'`\n",
    "\n",
    "Now let's find the top 5 most similar prompts to this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87136e91-4046-4bd2-8bfb-2e79347e7194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a value for the number of similar prompts to find based on the integer indexed prompt\n",
    "top_k = 5\n",
    "\n",
    "# Determine the index of similar prompts using argsort to compute the similarity of the 1000 \n",
    "# sampled prompts versus the integer indexed prompt, sorted in descending order. Then slice the \n",
    "# top_k elements from the bottom of the list and return them in descending order by similarity.\n",
    "similar_promts_idx = np.argsort(dot_product[source_prompt_idx])[-top_k:][::-1]\n",
    "\n",
    "# Locate the similar prompts by integer indexing\n",
    "similar_promts = conversation.iloc[similar_promts_idx]['user_prompt']\n",
    "\n",
    "# Output the similar prompts in a list.\n",
    "similar_promts.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9695740-09cf-42be-af00-d7dc4bed65d6",
   "metadata": {},
   "source": [
    "As you can see, these prompts are all related to quantum mechanics. This shows how embeddings can group semantically similar questions together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01991090-4b0b-47d4-a584-16948fa41860",
   "metadata": {},
   "source": [
    "#### Auxilliary Data Set - Model A Response Embeddings\n",
    "Now let's investigate the embedding data of model A responses (`./chatbot-arena-model_a_response-embeddings.npy`). This dataset contains 256-dimensional text embeddings for each of model A responses. This is the embedding from `model_a_response` created by extracting the second half of the column `conversation_a` from the main data set. Similar to above, each embedding is a 256-dimensional vector that represents the semantic meaning of a prompt. These vectors allow us to compare prompts based on their content.\n",
    "\n",
    "Similar to what we did with prompt embeddings, in this section we will be computing the similarity between responses from Model A using the precomputed embeddings. The goal is to find responses that are similar to a given response based on their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d66103a-4faf-4c20-b106-37898e3c58b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the data\n",
    "response_a_embeddings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd974bb-dcda-47a9-84b7-e17011f542e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_a_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a7f976-ae99-4dc8-8f60-1e6cd8be9b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to find the closest prompt to a given prompt\n",
    "embeddings_sample = response_a_embeddings[:1000]\n",
    "\n",
    "# Compute the dot product between the embeddings\n",
    "dot_product = np.dot(embeddings_sample, embeddings_sample.T)\n",
    "dot_product.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0981b6-3dc2-493f-9132-e5cb603a0968",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_prompt_idx = 16\n",
    "response_prompt = conversation.iloc[response_prompt_idx]['model_a_response']\n",
    "response_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c876bef5-8e2f-4fa2-bb6d-6f7192aedb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "similar_promts_idx = np.argsort(dot_product[response_prompt_idx])[-top_k:][::-1]\n",
    "similar_promts = conversation.iloc[similar_promts_idx]['model_a_response']\n",
    "similar_promts.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5a43e6-6c2c-468d-8e1f-c832a77efcf5",
   "metadata": {},
   "source": [
    "**All these responses are closely related to quantum mechanics.** Again, this shows how embeddings can group semantically similar responses together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e47960-be0a-4fdd-8803-087247e5eaef",
   "metadata": {},
   "source": [
    "#### Auxilliary Data Set - Model B Response Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f72729-b00c-42ab-b4aa-97dc323d9c79",
   "metadata": {},
   "source": [
    "Let's investigate the response embedding data for model B (`./chatbot-arena-model_b_response-embeddings.npy`). This dataset contains 256-dimensional text embeddings for each of model B responses. This is the embedding from `model_b_response` created by extracting the second half of the column `conversation_b` from the main data set. Similar to above, each embedding is a 256-dimensional vector that represents the semantic meaning of a prompt. These vectors allow us to compare prompts based on their content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe9fe8c-92b4-4cea-8831-2be81fe5b150",
   "metadata": {},
   "source": [
    "As a sanity check, we will repeat the same exercise as above and find some similar embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9eaebc-a54f-4af6-881d-5b302e6f271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(response_a_embeddings.shape)\n",
    "display(response_b_embeddings[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514ebb6a-b050-432f-9da6-4f2c5bf9b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to find the closest prompt to a given prompt\n",
    "embeddings_sample = response_b_embeddings[:1000]\n",
    "dot_product = np.dot(embeddings_sample, embeddings_sample.T)\n",
    "\n",
    "\n",
    "response_prompt_idx = 16\n",
    "response_prompt = conversation.iloc[response_prompt_idx]['model_b_response']\n",
    "response_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4cbd22-5fb2-43c4-9f4e-d51978761c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "similar_promts_idx = np.argsort(dot_product[response_prompt_idx])[-top_k:][::-1]\n",
    "similar_promts = conversation.iloc[similar_promts_idx]['model_b_response']\n",
    "similar_promts.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd910d1-4a82-4154-9933-80515df86f72",
   "metadata": {},
   "source": [
    "**All these responses are closely related to quantum mechanics.** Again, this shows how embeddings can group semantically similar responses together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a6b298-60c5-44d7-8da8-0695f0d341dd",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d875ba69-5231-4130-ada3-0e5ffe74e796",
   "metadata": {},
   "source": [
    "First, lets combine all three embeddings datasets (prompt embeddings, model A response embeddings, model B response embeddings) with the main conversations dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728c493d-96d2-4356-a604-0a27b4875139",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embeddings_list = prompt_embeddings.tolist()\n",
    "conversation['user_prompt_embedding'] = prompt_embeddings_list\n",
    "\n",
    "response_a_embeddings_list = response_a_embeddings.tolist()\n",
    "conversation['model_a_response_embedding'] = response_a_embeddings_list\n",
    "\n",
    "response_b_embeddings_list = response_b_embeddings.tolist()\n",
    "conversation['model_b_response_embedding'] = response_b_embeddings_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9ab860-32b3-4c7b-b887-e376ede12be3",
   "metadata": {},
   "source": [
    "Now let's check what the conversation dataset looks like now. It should have the new embeddings columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e246c8-21f1-479a-b520-fcd0b3733ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.iloc[:5, [0, 1, 2, 3, 4, 7, 8, 10, 12, 16, 17, 18]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ef301f-4102-47cd-9713-0ec2ff91290d",
   "metadata": {},
   "source": [
    "Now lets take a **random sample** of 1000 rows to do our analysis in a computationally efficient manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf48c98a-91a6-45bc-bd3d-b80167dc037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1000 rows from the main conversation DataFrame\n",
    "sample_conversation = conversation.sample(n=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fff6a88-3130-49fe-aa24-7e59d9fa224e",
   "metadata": {},
   "source": [
    "**Model Performance on Clusters of Similar Prompts**: we will now create 10 clusters of similar prompts with k-means clustering and visualize which models did better in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc29b8-1e47-48a0-891b-0c3933d09ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the Prompt Embeddings Using KMeans on the Sample Data\n",
    "n_clusters = 10  # number of clusters\n",
    "sample_prompt_embeddings = np.vstack(sample_conversation['user_prompt_embedding'].values)\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "sample_conversation['prompt_cluster'] = kmeans.fit_predict(sample_prompt_embeddings)\n",
    "\n",
    "# Run t-SNE on the Sample Data for Visualization\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "sample_prompt_embeddings_2d = tsne.fit_transform(sample_prompt_embeddings)\n",
    "\n",
    "# Add t-SNE results to the DataFrame for plotting\n",
    "sample_conversation['tsne_1'] = sample_prompt_embeddings_2d[:, 0]\n",
    "sample_conversation['tsne_2'] = sample_prompt_embeddings_2d[:, 1]\n",
    "\n",
    "# Plot the t-SNE Visualization with Clusters and Model Wins\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(data=sample_conversation, x='tsne_1', y='tsne_2', hue='prompt_cluster', style='winner_model', palette='tab10', alpha=0.7)\n",
    "plt.title('t-SNE Visualization of Sampled User Prompt Clusters with Winning Models')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Generate Heatmaps for Model Wins per Cluster (With and Without Ties)\n",
    "\n",
    "# Heatmap including ties\n",
    "cluster_model_performance_with_tie_sample = sample_conversation.groupby(['prompt_cluster', 'winner_model']).size().unstack(fill_value=0)\n",
    "\n",
    "# Heatmap excluding ties\n",
    "sample_df_without_tie = sample_conversation[sample_conversation['winner_model'] != 'tie']\n",
    "cluster_model_performance_without_tie_sample = sample_df_without_tie.groupby(['prompt_cluster', 'winner_model']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plot both heatmaps side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8), gridspec_kw={'width_ratios': [1, 1]})\n",
    "\n",
    "# Heatmap with ties\n",
    "sns.heatmap(cluster_model_performance_with_tie_sample, annot=True, cmap=\"PuRd\", fmt=\"d\", ax=axes[0])\n",
    "axes[0].set_title(\"Model Wins by Prompt Cluster (Including Ties) - Sample\")\n",
    "axes[0].set_xlabel(\"Winning Model\")\n",
    "axes[0].set_ylabel(\"Prompt Cluster\")\n",
    "\n",
    "# Heatmap without ties\n",
    "sns.heatmap(cluster_model_performance_without_tie_sample, annot=True, cmap=\"PuRd\", fmt=\"d\", ax=axes[1])\n",
    "axes[1].set_title(\"Model Wins by Prompt Cluster (Excluding Ties) - Sample\")\n",
    "axes[1].set_xlabel(\"Winning Model\")\n",
    "axes[1].set_ylabel(\"Prompt Cluster\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7b2b9d-436c-4471-9a14-aa3e7497f255",
   "metadata": {},
   "source": [
    "**Top-5 Models Performance on Clusters of Similar Prompts**: In the above plots we see some patterns but it would be better to focus further on specific models and visualize their performance on the prompt clusters. This time we will also take 5 clusters instead of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab49fb52-c4dc-4803-b0a1-138d56e38c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will take 200 rows per model\n",
    "n_per_model = 200\n",
    "\n",
    "# Perform stratified sampling with an equal count per model\n",
    "sample_conversation = conversation[conversation['winner_model'].isin(top_five)].groupby('winner_model', group_keys=False).apply(lambda x: x.sample(n=n_per_model, random_state=42))\n",
    "\n",
    "# Verify the distribution to ensure it’s balanced\n",
    "print(sample_conversation['winner_model'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdddaf0b-aff5-49f4-90f0-b70ffe355240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the Prompt Embeddings Using KMeans on the Sample Data\n",
    "n_clusters = 5  # number of clusters\n",
    "sample_prompt_embeddings = np.vstack(sample_conversation['user_prompt_embedding'].values)\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "sample_conversation['prompt_cluster'] = kmeans.fit_predict(sample_prompt_embeddings)\n",
    "\n",
    "# Run t-SNE on the Sample Data for Visualization\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "sample_prompt_embeddings_2d = tsne.fit_transform(sample_prompt_embeddings)\n",
    "\n",
    "# Add t-SNE results to the DataFrame for plotting\n",
    "sample_conversation['tsne_1'] = sample_prompt_embeddings_2d[:, 0]\n",
    "sample_conversation['tsne_2'] = sample_prompt_embeddings_2d[:, 1]\n",
    "\n",
    "# Plot the t-SNE Visualization with Clusters and Model Wins\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(data=sample_conversation, x='tsne_1', y='tsne_2', hue='prompt_cluster', style='winner_model', palette='tab10', alpha=0.7)\n",
    "plt.title('t-SNE Visualization of Sampled User Prompt Clusters with Winning Models')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Generate Heatmaps for Model Wins per Cluster (With and Without Ties)\n",
    "\n",
    "# Heatmap including ties\n",
    "cluster_model_performance_with_tie_sample = sample_conversation.groupby(['prompt_cluster', 'winner_model']).size().unstack(fill_value=0)\n",
    "\n",
    "# Heatmap excluding ties\n",
    "sample_df_without_tie = sample_conversation[sample_conversation['winner_model'] != 'tie']\n",
    "cluster_model_performance_without_tie_sample = sample_df_without_tie.groupby(['prompt_cluster', 'winner_model']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plot both heatmaps side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8), gridspec_kw={'width_ratios': [1, 1]})\n",
    "\n",
    "# Heatmap with ties\n",
    "sns.heatmap(cluster_model_performance_with_tie_sample, annot=True, cmap=\"PuRd\", fmt=\"d\", ax=axes[0])\n",
    "axes[0].set_title(\"Model Wins by Prompt Cluster (Including Ties) - Sample\")\n",
    "axes[0].set_xlabel(\"Winning Model\")\n",
    "axes[0].set_ylabel(\"Prompt Cluster\")\n",
    "\n",
    "# Heatmap without ties\n",
    "sns.heatmap(cluster_model_performance_without_tie_sample, annot=True, cmap=\"PuRd\", fmt=\"d\", ax=axes[1])\n",
    "axes[1].set_title(\"Model Wins by Prompt Cluster (Excluding Ties) - Sample\")\n",
    "axes[1].set_xlabel(\"Winning Model\")\n",
    "axes[1].set_ylabel(\"Prompt Cluster\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc726df-2814-4e07-91d4-b2f84f12a909",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "- T-SNE Plot\n",
    "    - The t-SNE plot shows five distinct clusters of prompt embeddings. The separations suggest that the prompt embeddings have enough semantic difference to form identifiable clusters.\n",
    "    - The scatter plot symbols are distributed fairly evenly across clusters, suggesting that models win in various clusters rather than being confined to a single type of question. However, there are slight concentrations of certain models in specific clusters, hinting at possible areas where these models may perform better. For example, GPT-4 has a noticeable presence across clusters, with some denser regions in certain clusters, especially clusters 1 and 4.\n",
    "    - Some clusters appear more densely packed with points (e.g., clusters 1 and 3), while others are more spread out. The denser clusters might contain more similar or thematically related prompts, while the sparser clusters might represent a broader diversity of question types within those clusters\n",
    "    - The dispersion of each model across clusters suggests that most top models can generalize across a range of prompt types rather than being limited to a specific prompt category. This generalization is especially relevant for models like GPT-4, which has a presence in all clusters\n",
    "- Heatmap\n",
    "    - In Cluster 1, GPT-4 has a notably higher win count (50 wins), suggesting that it performs particularly well on prompts within this cluster, even when ties are excluded. Cluster 4 also shows a preference for GPT-4 with 48 wins, indicating that GPT-4 is strong in multiple clusters.\n",
    "    - Claude-v1 has a balanced win count across clusters, showing moderate performance but no particular dominance in any specific cluster. GPT-3.5-turbo performs consistently well across clusters but doesn’t dominate any specific one, suggesting it’s more of a generalist.\n",
    "    - Koala-13b and Vicuna-13b have lower win counts in most clusters, particularly in Cluster 1 and Cluster 4, where GPT-4 dominates. This indicates that these models may struggle with the prompt types represented in these clusters.\n",
    "    - While we can’t directly infer prompt types from the embeddings, the cluster preferences suggest that certain models (especially GPT-4) are better suited for specific types of questions, likely reflected in Clusters 1 and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09c7c52-64eb-4357-a14d-3980ddb42a39",
   "metadata": {},
   "source": [
    "**Top-5 Models Performance on Clusters of Similar Responses:** Next, we will focus on the response embeddings to see the performance of top-5 models on similar responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be76d01e-63a6-4906-8f46-2b937a1ba9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack response embeddings from both model_a and model_b responses for clustering\n",
    "all_response_embeddings = np.vstack([\n",
    "    np.vstack(sample_conversation['model_a_response_embedding'].values),\n",
    "    np.vstack(sample_conversation['model_b_response_embedding'].values)\n",
    "])\n",
    "\n",
    "# Perform clustering (e.g., using 5 clusters)\n",
    "n_clusters = 5\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "response_clusters = kmeans.fit_predict(all_response_embeddings)\n",
    "\n",
    "# Assign clusters back to the original dataframe for each model's response\n",
    "sample_conversation['model_a_response_cluster'] = response_clusters[:len(sample_conversation)]\n",
    "sample_conversation['model_b_response_cluster'] = response_clusters[len(sample_conversation):]\n",
    "\n",
    "# Assign the response cluster based on the winning model's response embedding cluster\n",
    "sample_conversation['winning_response_cluster'] = np.where(\n",
    "    sample_conversation['winner_model'] == sample_conversation['model_a'],\n",
    "    sample_conversation['model_a_response_cluster'],\n",
    "    sample_conversation['model_b_response_cluster']\n",
    ")\n",
    "\n",
    "# Group by response cluster of the winning model and the winning model itself\n",
    "model_wins_by_cluster = sample_conversation.groupby(['winning_response_cluster', 'winner_model']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bd714a-5ae4-4fca-9538-52b7a0ceec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot grouped bar chart\n",
    "model_wins_by_cluster.plot(kind='bar', figsize=(14, 6), colormap='plasma')\n",
    "plt.title(\"Model Wins by Response Cluster\")\n",
    "plt.xlabel(\"Response Cluster\")\n",
    "plt.ylabel(\"Win Counts\")\n",
    "plt.legend(title=\"Winning Model\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Convert the data for a dot plot\n",
    "box_data = model_wins_by_cluster.reset_index().melt(id_vars=\"winning_response_cluster\", var_name=\"Winning Model\", value_name=\"Win Count\")\n",
    "\n",
    "# Plot box plot using seaborn\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(data=box_data, x=\"Winning Model\", y=\"Win Count\", palette=\"coolwarm\")\n",
    "plt.title(\"Distribution of Model Wins Across Response Clusters\")\n",
    "plt.xlabel(\"Winning Model\")\n",
    "plt.ylabel(\"Win Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40d0f90-f5d0-488e-8a86-5c20b8327214",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "\n",
    "- Grouped Bar Chart: GPT-4 and GPT-3.5-turbo maintain relatively high win counts across multiple clusters, suggesting that users tend to favor these models consistently, regardless of the specific response cluster. Vicuna-13b shows variability, with particularly high counts in certain clusters (like clusters 0 and 4). This may indicate that Vicuna-13b excels at specific types of questions or response styles, making it more likely to be chosen within these clusters.\n",
    "- Box Plot: GPT-4 has a narrow interquartile range and fewer extreme values, indicating stable performance across response clusters. This implies that users consistently choose GPT-4 regardless of the response cluster, pointing to a general preference for this model. GPT-3.5-turbo shows a broader range of win counts, indicating it performs well across most clusters but has more variability than GPT-4. Claude-v1 and Koala-13b show more spread and lower median win counts, indicating they are less likely to be chosen consistently across clusters. This suggests these models may perform better in specific clusters but are less preferred in general.\n",
    "- Cluster-Specific Strengths for Certain Models: Vicuna-13b has a wider range, suggesting it might perform well in particular clusters but is less consistent across all clusters. The variation in counts for Claude-v1 and Koala-13b across clusters indicates that these models may be chosen only under certain conditions or response types, supporting the idea that some models are better suited for specific response clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c64109-07a4-41a7-a02b-3caa49bfb5e2",
   "metadata": {},
   "source": [
    "### Key Observations and Takeaways from Embeddings Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accadcf5-c04a-456f-b948-159dad6de4ec",
   "metadata": {},
   "source": [
    "- **Looking at prompt embeddings, can we say that some models are better at certain clusters of questions?**\n",
    "Yes, prompt embeddings do suggest that certain models stand out in specific question clusters. For example, GPT-4 shows strong performance in Clusters 1 and 4, hinting that it might be better suited for certain types of prompts or topics compared to other models. Similarly, GPT-3.5-Turbo did well in Clusters 0 and 4, and Koala-13B performed strongly in Clusters 1 and 3. Claude-V1 performed poorly in Cluster 2.\n",
    "\n",
    "- **Looking at response embeddings, is a user more likely to pick the same model in the same cluster of responses?**\n",
    "Yes, users tend to favor particular models within specific response clusters. GPT-4, for instance, is consistently the go-to choice across clusters, which suggests it’s generally reliable or appealing in its responses. Other models, like Vicuna-13b, seem to do well in only a few clusters such as Cluster 0. In cluster 1 GPT-3.5-Turbo performed the strongest, whereas Koala-13B was the strongest in Cluster 4.\n",
    "\n",
    "- **Which prompt clusters tend to have higher tie ratios across models?**\n",
    "Some clusters have a higher number of ties, which might mean these questions are more ambiguous or open-ended, making it hard for any model to stand out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f59046-39b9-46c4-b232-fc1511898958",
   "metadata": {},
   "source": [
    "## Auxilliary Data Set - Topic Modeling and Hardness Score Data\n",
    "\n",
    "Now, let's explore the second auxiliary dataset `./chatbot-arena-gpt3-scores.jsonl.gz`, which contains valuable information for later modeling tasks. For each prompt, there are 3 responses, as GPT-3.5 is probabilistic. This means we get multiple labels for a single prompt, similar to how real-world datasets can have multiple annotations.\n",
    "\n",
    "Let's start by loading and inspecting the first 5 rows of this dataset to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7c6014-b955-4bcf-a6be-58605d429c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the data\n",
    "topic_and_hardness.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519278f6-9bb0-4aef-95dc-d47e3a6ca5db",
   "metadata": {},
   "source": [
    "Check the **data types**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72eb315-b8b5-447d-8d82-8a1c89ae7540",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_and_hardness.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ee8e1f-007b-4769-8da1-adc15059a631",
   "metadata": {},
   "source": [
    "Check for **null values** in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d31c52-e3c5-467a-a629-13e9614415f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_and_hardness.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7fd7cf-b1e5-4060-8101-0affbd458541",
   "metadata": {},
   "source": [
    "### Cleaning\n",
    "\n",
    "**Granularity**: the data is on the question-level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58edc211-b7e4-4b40-b4d6-5ebfdfd56f8b",
   "metadata": {},
   "source": [
    "From the above analysis we can see that there is **missing data in our dataframe**. \n",
    "\n",
    "By observing the `openai_scores_raw_choices_nested` variable we can see that it is not missing any data. This variable contains the nested topic_modelling, score_reason, and score_value variables. It is possible that there was an error in the previous attempt in flattening this array. Similar to the flattening we performed above for the main dataset, we can attempt to re-flatten this variable to see if the values are actually missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2bafd3-73e2-4ece-afcb-13e065432b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_and_hardness[\"openai_scores_raw_choices_nested\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd3672c-03d9-44b3-9635-51b74706e173",
   "metadata": {},
   "source": [
    "Obtain the content similar to the Main data set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade5da1-9a47-49b6-ada3-073d09f337c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_1 = topic_and_hardness[\"openai_scores_raw_choices_nested\"].str[0].str[\"message\"].str[\"content\"]\n",
    "ind_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58026826-4df6-48cf-b07e-f8e9b8803ef7",
   "metadata": {},
   "source": [
    "Observe that the above data is dtype object. It is specifically an unparsed JSON object. So let's attempt to parse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a27b67-f981-43be-a5f7-5e895a816723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create a function to parse the valid rows\n",
    "def parse_valid(row):\n",
    "    try:\n",
    "        # Try to parse a row\n",
    "        return json.loads(row)\n",
    "    except (json.JSONDecodeError):\n",
    "        # Return None if the row cannot be parsed\n",
    "        return None\n",
    "\n",
    "# Apply the function to parse the content\n",
    "index_1 = topic_and_hardness[\"openai_scores_raw_choices_nested\"].str[0].str[\"message\"].str[\"content\"].apply(parse_valid)\n",
    "index_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60adba9-443f-46f5-b507-14d50d8f4afd",
   "metadata": {},
   "source": [
    "Check which rows contain **invalid data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046de62c-8d0e-435a-b45a-55dbf0003ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_1[index_1.isnull() == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbde8184-b896-4b7e-9135-b664e068e722",
   "metadata": {},
   "source": [
    "Let's create a variable to store these row values. We will use our parse_valid function for index_2 and index_3 and append any rows which do not contain valid information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e6576-1f8e-4bed-8a70-5e120131f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_rows = []\n",
    "invalid_rows.extend(list(index_1[index_1.isnull() == True].index))\n",
    "\n",
    "# Obtain index_2 and index_3\n",
    "index_2 = topic_and_hardness[\"openai_scores_raw_choices_nested\"].str[1].str[\"message\"].str[\"content\"].apply(parse_valid)\n",
    "index_3 = topic_and_hardness[\"openai_scores_raw_choices_nested\"].str[2].str[\"message\"].str[\"content\"].apply(parse_valid)\n",
    "\n",
    "# Append invalid rows to the list\n",
    "invalid_rows.extend(list(index_2[index_2.isnull() == True].index))\n",
    "invalid_rows.extend(list(index_3[index_3.isnull() == True].index))\n",
    "\n",
    "# Make it a set to only contain unique values\n",
    "invalid_rows = set(invalid_rows)\n",
    "\n",
    "# Check the invalid rows\n",
    "invalid_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9803bdc-9287-4900-b4cd-d6158ceb1a80",
   "metadata": {},
   "source": [
    "Now that we have parsed as much information as possible out of the nested variable. Let's update the topic_and_hardness dataframe with the new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e345c10d-1067-4d3c-abcd-623ef75536ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the values from index_1\n",
    "topic_and_hardness[\"topic_modeling_1\"] = index_1.str[\"topic_modeling\"]\n",
    "topic_and_hardness[\"score_reason_1\"] = index_1.str[\"score_reason\"]\n",
    "topic_and_hardness[\"score_value_1\"] = index_1.str[\"score_value\"]\n",
    "\n",
    "# Update the values from index_2\n",
    "topic_and_hardness[\"topic_modeling_2\"] = index_3.str[\"topic_modeling\"]\n",
    "topic_and_hardness[\"score_reason_2\"] = index_3.str[\"score_reason\"]\n",
    "topic_and_hardness[\"score_value_2\"] = index_3.str[\"score_value\"]\n",
    "\n",
    "# Update the values from index_2\n",
    "topic_and_hardness[\"topic_modeling_3\"] = index_3.str[\"topic_modeling\"]\n",
    "topic_and_hardness[\"score_reason_3\"] = index_3.str[\"score_reason\"]\n",
    "topic_and_hardness[\"score_value_3\"] = index_3.str[\"score_value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd201bc-19f6-4906-bea6-9aa4335f9547",
   "metadata": {},
   "source": [
    "Check for null values now in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a0af18-321e-47ac-b39c-3a659871d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_and_hardness.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442333b8-1714-4ec0-a4ac-846f8983fe11",
   "metadata": {},
   "source": [
    "We have reduced the number of null values in the data, but there is still remaining null values in the nested data. Let's explore this in more detail.\n",
    "\n",
    "Filter the data to view null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4970ccd3-76b4-48fa-b7e0-f25c162bc77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_invalid_data = topic_and_hardness[topic_and_hardness[\"topic_modeling_3\"].isnull() == True]\n",
    "display(len(remaining_invalid_data))\n",
    "display(remaining_invalid_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50afaaf-7025-454f-9051-a712b7dd0845",
   "metadata": {},
   "source": [
    "**Removing Remaining Invalid Data**\n",
    "\n",
    "You can see from the above dataframe that these rows have no meaningful data for topic_modeling, score_reason, or score_value. There are some cases where the first index has meaningful data. However, given the size of the dataset there is not reason to attempt to impute the missing values. Imputing the missing data as an average for the model would skew the results, and since we are only removing a vert small portion of the data, the effect of removing them will be insignificant.\n",
    "\n",
    "Let's clean up our dataframe by removing the `openai_scores_raw_choices_nested` column, now that it is no longer needed, and removing the rows containing the missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86ce636-d83a-423b-9be0-83e9bc6bc6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_and_hardness = topic_and_hardness.drop(list(topic_and_hardness[topic_and_hardness[\"topic_modeling_3\"].isnull() == True].index))\n",
    "topic_and_hardness = topic_and_hardness.drop(columns = \"openai_scores_raw_choices_nested\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d0c049-9f3e-4857-97fa-0b61a007f827",
   "metadata": {},
   "source": [
    "Now check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3cc324-f361-42e1-a962-15b04885069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_and_hardness.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75f9e8b-1f6d-47e9-bbcf-88bf9360fce6",
   "metadata": {},
   "source": [
    "**There are no null values anymore**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56321b3-6fc2-4e3f-8b97-1e5d32ddbea7",
   "metadata": {},
   "source": [
    "**Hardness Score**\n",
    "\n",
    "Now that we have solved the issue with null values, lets evaluate whether the parsed data has inconsistent formatting. Specifically, the data we are concerned about for analysis is the hardness score. So let us determine if the hardness score has inconsistent formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e53f959-9a62-49b4-a8ee-f2a46557d0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_and_hardness[\"score_value_1\"][\n",
    "    topic_and_hardness[\"score_value_1\"].apply(lambda x: isinstance(x, list))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4af432-5904-4c84-92b7-183dfb630696",
   "metadata": {},
   "source": [
    "Let's convert these list of lists to ints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24500470-04a9-4776-9904-92bac3136e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list into an int\n",
    "topic_and_hardness[\"score_value_1\"] = topic_and_hardness[\"score_value_1\"].apply(\n",
    "    # Clean nested list element into an int\n",
    "    lambda x: x[0][0] if isinstance(x, list) and len(x) == 1 and isinstance(x[0], list) and len(x[0]) == 1 else (\n",
    "        # Else clean the list element into an int\n",
    "        x[0] if isinstance(x, list) and len(x) == 1 and isinstance(x[0], (int, float)) \n",
    "        # Else leave it alone\n",
    "        else x\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d906ec-a634-49f9-9eae-1f12a133ec89",
   "metadata": {},
   "source": [
    "Repeat this for score_value_2 and score_value_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf78c1c-0eb9-4abc-ac35-5c27695e37b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_and_hardness[\"score_value_2\"] = topic_and_hardness[\"score_value_2\"].apply(\n",
    "    # Clean nested list element into an int\n",
    "    lambda x: x[0][0] if isinstance(x, list) and len(x) == 1 and isinstance(x[0], list) and len(x[0]) == 1 else (\n",
    "        # Else clean the list element into an int\n",
    "        x[0] if isinstance(x, list) and len(x) == 1 and isinstance(x[0], (int, float)) \n",
    "        # Else leave it alone\n",
    "        else x\n",
    "    )\n",
    ")\n",
    "\n",
    "# Convert the list into an int\n",
    "topic_and_hardness[\"score_value_3\"] = topic_and_hardness[\"score_value_3\"].apply(\n",
    "    # Clean nested list element into an int\n",
    "    lambda x: x[0][0] if isinstance(x, list) and len(x) == 1 and isinstance(x[0], list) and len(x[0]) == 1 else (\n",
    "        # Else clean the list element into an int\n",
    "        x[0] if isinstance(x, list) and len(x) == 1 and isinstance(x[0], (int, float)) \n",
    "        # Else leave it alone\n",
    "        else x\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95774bf-7c83-4218-a19f-83b2e329bc1c",
   "metadata": {},
   "source": [
    "**Topic Modelling**\n",
    "\n",
    "Score value contained nested values because when it was written into the JSON file it was written as a nested list.  Now we need to check the rest of our data for incorrect formatting. Let's check the topic_modeling data next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18ff88-7d4f-4c38-884a-941bc063b90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check topic modeling 1\n",
    "topic_and_hardness[\"topic_modeling_1\"][\n",
    "    topic_and_hardness[\"topic_modeling_1\"].apply(lambda x: isinstance(x, list))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b4abe-d167-4d7d-9dfa-ac4593f44e8c",
   "metadata": {},
   "source": [
    "Since these are only 3 cases, we can take the first element of each list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad97cdb-5650-4d30-8e8c-ea9c1308da30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first value from the list\n",
    "topic_and_hardness[\"topic_modeling_1\"] = topic_and_hardness[\"topic_modeling_1\"].apply(\n",
    "    # Clean nested list element into an string\n",
    "    lambda x: x[0] if isinstance(x, list)\n",
    "    # Else leave it alone\n",
    "    else x\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6714ac-a371-4515-a7be-f51d1a0b4110",
   "metadata": {},
   "source": [
    "Repeat this for topic_modeling_1 and topic_modeling_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d463b0d1-255a-4c29-b986-061520adf24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_and_hardness[\"topic_modeling_2\"] = topic_and_hardness[\"topic_modeling_2\"].apply(\n",
    "    # Clean nested list element into an string\n",
    "    lambda x: x[0] if isinstance(x, list)\n",
    "    # Else leave it alone\n",
    "    else x\n",
    ")\n",
    "\n",
    "topic_and_hardness[\"topic_modeling_3\"] = topic_and_hardness[\"topic_modeling_3\"].apply(\n",
    "    # Clean nested list element into an string\n",
    "    lambda x: x[0] if isinstance(x, list)\n",
    "    # Else leave it alone\n",
    "    else x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5202960d-9875-4754-85c6-c28e1c21cb1b",
   "metadata": {},
   "source": [
    "**Score Reason**\n",
    "\n",
    "Next, let's check the score reason variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56616782-a986-4d17-ae18-92b9dc6c7274",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(topic_and_hardness[\"score_reason_1\"][\n",
    "    topic_and_hardness[\"score_reason_1\"].apply(lambda x: isinstance(x, list))\n",
    "])\n",
    "\n",
    "# Check score response 2\n",
    "display(topic_and_hardness[\"score_reason_2\"][\n",
    "    topic_and_hardness[\"score_reason_2\"].apply(lambda x: isinstance(x, list))\n",
    "])\n",
    "\n",
    "display(topic_and_hardness[\"score_reason_3\"][\n",
    "    topic_and_hardness[\"score_reason_3\"].apply(lambda x: isinstance(x, list))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2806f7b7-30be-432d-924a-56cffdbc635f",
   "metadata": {},
   "source": [
    "Score reason looks clean. It should be noted that the above analysis only checked the columns for lists. It did not check for any other data types. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa1b41-8d24-4827-9a32-eefa3c72cfab",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7922faa-f7d3-46c8-bf28-a16ac077b7c1",
   "metadata": {},
   "source": [
    "First, let's merge the main conversation dataset with the cleaned hardness dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6126b99-e7f1-4c89-b346-8a3e90411162",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_and_topic_hardness = conversation.merge(topic_and_hardness, left_on = \"question_id\", right_on = \"question_id\").drop(columns = [\n",
    "    \"conversation_a\", \"conversation_b\"\n",
    "])\n",
    "display(conversation_and_topic_hardness.head())\n",
    "display(conversation_and_topic_hardness.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a11dce9-0ea9-48d9-9991-a2a8a6871205",
   "metadata": {},
   "source": [
    "Next, calculate the average score and assign it to a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de8ea02-83fd-4fc8-9bb9-d7068486b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_and_topic_hardness[\"avg_score\"] = (conversation_and_topic_hardness[\"score_value_1\"] + conversation_and_topic_hardness[\"score_value_2\"] + conversation_and_topic_hardness[\"score_value_3\"]) / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816a5bcf-2011-409b-b3eb-e76300db6750",
   "metadata": {},
   "source": [
    "From here onwards, we will focus only on the rows where there was no tie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebae141-8e55-4c99-a52f-76fa1aee660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_and_topic_hardness_win = conversation_and_topic_hardness[conversation_and_topic_hardness[\"winner\"].isin([\"model_a\", \"model_b\"])].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f58491-1226-4c50-908b-2a409e7fd82c",
   "metadata": {},
   "source": [
    "Now we will bin the hardness score into 5 categories: Very Easy, Easy, Medium, Hard, Very Hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e376106f-75d2-4406-aaed-abbd2cc41a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 2, 4, 6, 8, 10]\n",
    "labels = ['Very Easy', 'Easy', 'Medium', 'Hard', 'Very Hard']\n",
    "\n",
    "conversation_and_topic_hardness_win[\"hardness_cat\"] = pd.cut(conversation_and_topic_hardness_win[\"avg_score\"], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052ab47d-f8e1-4ade-9a9b-df077d62acad",
   "metadata": {},
   "source": [
    "**Wins In Each Hardness Category**\n",
    "\n",
    "Count the wins for each model in each hardness category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d9abe3-5617-4604-a2ec-1a8f4785cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_count = conversation_and_topic_hardness_win.groupby([\"winner_model\", \"hardness_cat\"]).size().reset_index(name = \"win_count\")\n",
    "win_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b625abc1-e749-480c-bc1a-c8ba3e07d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate match counts for model_a and model_b into a single DataFrame\n",
    "model_a_match_count = conversation_and_topic_hardness_win.groupby([\"model_a\", \"hardness_cat\"]).size().reset_index(name=\"match_count\")\n",
    "model_b_match_count = conversation_and_topic_hardness_win.groupby([\"model_b\", \"hardness_cat\"]).size().reset_index(name=\"match_count\")\n",
    "\n",
    "# Rename columns for consistency\n",
    "model_a_match_count.rename(columns={\"model_a\": \"model\"}, inplace=True)\n",
    "model_b_match_count.rename(columns={\"model_b\": \"model\"}, inplace=True)\n",
    "\n",
    "# Concatenate match counts for both model_a and model_b\n",
    "total_match_count = pd.concat([model_a_match_count, model_b_match_count])\n",
    "\n",
    "# Aggregate total match counts by model and hardness category\n",
    "total_match_count = total_match_count.groupby([\"model\", \"hardness_cat\"]).sum().reset_index()\n",
    "\n",
    "# Merge the total match counts with the win counts on 'winner_model' and 'hardness_cat'\n",
    "df_win_tot = pd.merge(win_count, total_match_count, \n",
    "                      left_on=[\"winner_model\", \"hardness_cat\"], \n",
    "                      right_on=[\"model\", \"hardness_cat\"], \n",
    "                      how=\"inner\")\n",
    "\n",
    "# Compute the win ratio for each model in each hardness category\n",
    "df_win_tot[\"win_ratio\"] = df_win_tot[\"win_count\"] / df_win_tot[\"match_count\"]\n",
    "\n",
    "# Drop the redundant 'model' column from the final DataFrame\n",
    "df_win_tot = df_win_tot.drop(columns=[\"model\"])\n",
    "\n",
    "# Display the result\n",
    "print(df_win_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb5a9aa-2aaf-4afa-aba8-84afb5e84cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results for the top five models\n",
    "df_cat_top_five = df_win_tot[df_win_tot[\"winner_model\"].isin(top_five)]\n",
    "\n",
    "# Plot the win ratio for the top five models\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data = df_cat_top_five, x='hardness_cat', y='win_ratio', hue='winner_model', palette='Set2')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Win Ratio of Top 5 Models Across Hardness Categories')\n",
    "plt.xlabel('Hardness Category')\n",
    "plt.ylabel('Win Ratio')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e06d84-5e6c-4640-a1e0-2e0f03c6517b",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "\n",
    "- **How does the hardness score compare with the results from the top five models from the main data set?**\n",
    "    - The above plot of \"Win Ratio of Top 5 Models Across Hardness Categories\" shows the correlation between the number of times a model won a competition based on the average hardness score. The hardness categories are bins that represent the average hardness score from the 'topic_and_hardness' dataframe. The bins are structured to contain ranges of values, for example, \"Very Easy\" is zero to two, \"Easy\" two to four, and so on. \n",
    "    - From the above plot of we can see that 'gpt-4' outperformed all other models for every category of hardness, except for \"Very Easy\" questions, where 'claude-v1' was the highest performing model. Interestingly, these results would infer that the order of the top five models would be different than that obtained solely based on win count. The plot shows that 'claude-v1' consistently performed better than 'vicuna-13b' even though 'vicuna-13b' took second in rankings based on win count, and 'claude-v1' was fourth. \n",
    "    - Based on this plot, I would maintain 'gpt-4' as the best performing model, but I would switch the order of 'claude-v1' and 'vicuna-13b' due to the reason discussed above. The rest of the ordering remains the same.\n",
    "- **What are the most common topics?**\n",
    "    - By taking the value counts of the \"topic_modeling\" data we can obtain a list of the most common topics in descending order. The top three topics based on this output are: creative writing, factual accuracy, and problem-solving/creativity. Creative writing is the most common with about 600 instances of the topic, which is almost 100 more instances than factual accuracy, and about 200 more instances than problem-solving/creativity. \n",
    "- **What is the hardness score of the most common topics.**\n",
    "    - Based on the above dataframe of topics grouped by the mean of the average hardness score, for the top three topics mentioned above the average hardness score is: 8.2, 7.8, and 6.9 respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be605cac-3fa5-4d49-9556-da987efe7624",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_counts = conversation_and_topic_hardness_win.groupby(['hardness_cat', 'winner_model']).size().unstack(fill_value=0)\n",
    "\n",
    "# Calculate the total matches for each model within each hardness category\n",
    "model_a_counts = conversation_and_topic_hardness_win.groupby(['hardness_cat', 'model_a']).size().unstack(fill_value=0)\n",
    "model_b_counts = conversation_and_topic_hardness_win.groupby(['hardness_cat', 'model_b']).size().unstack(fill_value=0)\n",
    "total_matches = model_a_counts.add(model_b_counts, fill_value=0)\n",
    "\n",
    "# Calculate win rate by dividing win counts by total matches for each model\n",
    "average_win_rate = (win_counts / total_matches).fillna(0)\n",
    "\n",
    "# Reshape the DataFrame for easier plotting\n",
    "average_win_rate_long = average_win_rate.reset_index().melt(id_vars=\"hardness_cat\", var_name=\"Model\", value_name=\"Average Win Rate\")\n",
    "\n",
    "# Plot the average win rate for each model within each hardness category\n",
    "# Define color maps for each hardness level\n",
    "color_maps = {\n",
    "    \"Very Hard\": \"autumn\",\n",
    "    \"Hard\": \"autumn\",\n",
    "    \"Medium\": \"spring\",\n",
    "    \"Easy\": \"summer\",\n",
    "    \"Very Easy\": \"summer\", \n",
    "}\n",
    "\n",
    "# Set up the figure\n",
    "fig, axes = plt.subplots(nrows=len(color_maps), ncols=1, figsize=(12, 8 * len(color_maps)))\n",
    "fig.suptitle(\"Average Win Rate Against All Other Models by Difficulty Level\", fontsize=18, y=1.02)\n",
    "\n",
    "# Plot each hardness category separately\n",
    "for ax, (hardness, cmap) in zip(axes, color_maps.items()):\n",
    "    # Filter data for the current hardness level\n",
    "    subset_data = average_win_rate_long[average_win_rate_long['hardness_cat'] == hardness]\n",
    "    \n",
    "    # Sort models by win rate for cleaner presentation\n",
    "    subset_data = subset_data.sort_values(by=\"Average Win Rate\", ascending=False)\n",
    "    \n",
    "    # Plot the bar chart with a color gradient\n",
    "    bars = sns.barplot(data=subset_data, x=\"Model\", y=\"Average Win Rate\", ax=ax, palette=sns.color_palette(cmap, len(subset_data)))\n",
    "    bars.set_xticklabels(bars.get_xticklabels(), rotation=90)\n",
    "    ax.set_title(f\"{hardness.capitalize()}\", fontsize=18)\n",
    "    ax.set_xlabel(\"Model\")  # Ensure xlabel appears under each plot\n",
    "    \n",
    "    # Add labels on top of bars\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt=\"%.2f\", label_type=\"edge\", fontsize=11, color=\"black\")\n",
    "\n",
    "# Adjust layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8884eeb-500c-429c-b61b-dc2afb4fdd83",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "\n",
    "- **Model Performance by Hardness Level**\n",
    "   - Very Hard: GPT-4 leads with a high win rate, followed by Claude-v1 and Claude-instant-v1.\n",
    "   - Hard: GPT-4 again ranks highest, maintaining strong performance, but the win rate gap between models narrows slightly compared to the \"very hard\" category.\n",
    "   - Medium: GPT-4, Claude-v1, and Gemini-3B still rank at the top, but there’s a more balanced distribution across models, indicating slightly more competition on moderately difficult prompts.\n",
    "   - Easy: GPT-4 and Claude-instant-v1 continue to perform well, but models like Gemini-3B and others gain more ground. This suggests that on easier questions, the performance difference among top models becomes less pronounced.\n",
    "   - Very Easy: Here, the performance is even more leveled. While Claude-v1 and GPT-4 still perform well, other models achieve closer win rates, showing that on very easy tasks, the difference in quality among models is less noticeable.\n",
    "- **Consistency Across Hardness Levels**\n",
    "   - GPT-4 and Claude-v1 are consistently among the top performers across all levels of hardness. This implies these models have strong generalization capabilities, handling both difficult and easy questions effectively.\n",
    "   - Models like GPT-3.5-turbo and Gemini-3B perform well but are more variable, excelling primarily in easier or medium categories compared to very hard questions.\n",
    "- **Specialized vs. Generalized Strengths**\n",
    "   - Some models show strengths only at specific difficulty levels. For instance, certain models that perform decently on \"medium\" or \"easy\" questions drop significantly in win rate as the difficulty increases, indicating a specialization in simpler tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98feecb-ff32-41bd-97a6-824f472a42c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Map hardness categories to numerical values\n",
    "hardness_mapping = {'Very Easy': 1, 'Easy': 2, 'Medium': 3, 'Hard': 4, 'Very Hard': 5}\n",
    "conversation_and_topic_hardness_win['hardness_numeric'] = conversation_and_topic_hardness_win['hardness_cat'].map(hardness_mapping)\n",
    "\n",
    "# Compute embedding distances for model A and model B responses\n",
    "conversation_and_topic_hardness_win['model_a_distance'] = conversation_and_topic_hardness_win.apply(\n",
    "    lambda row: euclidean(row['user_prompt_embedding'], row['model_a_response_embedding']), axis=1\n",
    ")\n",
    "conversation_and_topic_hardness_win['model_b_distance'] = conversation_and_topic_hardness_win.apply(\n",
    "    lambda row: euclidean(row['user_prompt_embedding'], row['model_b_response_embedding']), axis=1\n",
    ")\n",
    "\n",
    "# Create 'winner_model_distance' column based on the winning model\n",
    "conversation_and_topic_hardness_win['winner_model_distance'] = np.where(\n",
    "    conversation_and_topic_hardness_win['winner_model'] == 'model_a', \n",
    "    conversation_and_topic_hardness_win['model_a_distance'], \n",
    "    conversation_and_topic_hardness_win['model_b_distance']\n",
    ")\n",
    "\n",
    "# Melt data for easier plotting (single melt for all plots)\n",
    "distance_df = conversation_and_topic_hardness_win.melt(\n",
    "    id_vars=['hardness_cat', 'hardness_numeric'],\n",
    "    value_vars=['winner_model_distance'],\n",
    "    var_name='Model',\n",
    "    value_name='Embedding Distance'\n",
    ")\n",
    "\n",
    "\n",
    "# Violin Plot of Embedding Distances by Hardness Category\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(data=distance_df, x='hardness_cat', y='Embedding Distance', hue='Model', color='white')\n",
    "plt.title('Distribution of Embedding Distances by Hardness Category')\n",
    "plt.xlabel('Hardness Category')\n",
    "plt.ylabel('Embedding Distance')\n",
    "plt.legend(title='Model')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Jittered Strip Plot with Trend Line\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.stripplot(\n",
    "    data=distance_df,\n",
    "    x='hardness_numeric', \n",
    "    y='Embedding Distance', \n",
    "    hue='Model',\n",
    "    dodge=True, \n",
    "    jitter=0.3, \n",
    "    alpha=0.3\n",
    ")\n",
    "sns.lineplot(\n",
    "    data=distance_df, \n",
    "    x='hardness_numeric', \n",
    "    y='Embedding Distance', \n",
    "    hue='Model', \n",
    "    markers=True, \n",
    "    legend=False,\n",
    "    color='blue'\n",
    ")\n",
    "plt.title('Embedding Distances by Hardness Category with Jitter')\n",
    "plt.xlabel('Hardness Category')\n",
    "plt.ylabel('Embedding Distance')\n",
    "plt.xticks(ticks=[1, 2, 3, 4, 5], labels=['Very Easy', 'Easy', 'Medium', 'Hard', 'Very Hard'])\n",
    "plt.legend(title='Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404e1a8c-10db-4b72-8d26-488e2dbe90a0",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "\n",
    "- Both the violin plot and jittered scatter plot show a stable median embedding distance across all hardness categories. This indicates that the model maintains a similar level of semantic closeness to user prompts, irrespective of question difficulty.\n",
    "- In the violin plot, we see similar shapes and spread across categories, with distances concentrated between roughly 0.6 and 1.0. This suggests that the majority of responses are consistently within this range, regardless of the difficulty level. The scatter plot with jitter reinforces this by displaying a dense clustering around these values, confirming the consistency seen in the violin plot.\n",
    "- The scatter plot trend line shows a slight downward slope as difficulty increases, suggesting a minimal reduction in embedding distance for harder questions. This might imply that as the questions become harder, the responses are slightly more aligned with the prompt embeddings, although the difference is subtle.\n",
    "- Both plots show some responses with lower embedding distances across all categories. These outliers indicate instances where the response was less aligned with the prompt, potentially suggesting less relevant or contextually fitting answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a467de-5185-4fa7-a3f6-587f45e1287b",
   "metadata": {},
   "source": [
    "### Key Observations and Takeaways With Topic & Hardness Dataset\n",
    "\n",
    "- **What is the score distribution and win rate of each chatbot model in each different hardness level?**\n",
    "The win rate distribution across hardness levels shows that GPT-4 consistently outperforms other models across almost all levels of difficulty, with the exception of \"Very Easy\" questions, where Claude-v1 achieves the highest win rate. For \"Very Hard\" questions, GPT-4 remains the top performer, followed by Claude-v1 and Claude-instant-v1. As the hardness decreases (from \"Very Hard\" to \"Very Easy\"), the performance differences among models become less pronounced, with a more balanced distribution of win rates among GPT-4, Claude-v1, GPT-3.5-turbo, and Gemini-3B. This suggests that while GPT-4 excels universally, Claude-v1 and other models are competitive at lower hardness levels.\n",
    "\n",
    "- **How does the hardness score relate to the model that wins? Do some models perform consistently better on harder prompts compared to easier prompts, and vice versa?**\n",
    "The results indicate that GPT-4 and Claude-v1 perform well across all levels of hardness, demonstrating strong generalization capabilities. GPT-4 dominates at higher hardness levels, while Claude-v1 shines particularly on \"Very Easy\" questions. Models like GPT-3.5-turbo and Gemini-3B show more variable performance, excelling on easier or medium-level prompts but falling behind on very hard questions. This indicates that while GPT-4 and Claude-v1 handle both easy and hard prompts effectively, some models are specialized for simpler tasks and struggle with increased difficulty.\n",
    "\n",
    "- **What is the relation between the embedding distances of user prompt, model response, and the hardness score? Do higher hardness scores lead to responses with embeddings that are farther from the prompt embedding?**\n",
    "Analysis of embedding distances shows that the distances remain relatively stable across all hardness levels, with most responses clustered between 0.6 and 1.0. Both the violin plot and jittered scatter plot indicate that hardness levels have little impact on the semantic closeness between user prompts and model responses. There is a slight downward trend in embedding distance as hardness increases, suggesting that responses to harder questions may be slightly more semantically aligned with the prompts, but this trend is minimal and does not indicate a significant correlation between hardness and embedding distance.\n",
    "\n",
    "- **Do certain models demonstrate specialized strengths at particular hardness levels?**\n",
    "Yes, some models perform well only at specific difficulty levels. GPT-4 and Claude-v1 are the most consistent performers across all hardness levels, whereas models like GPT-3.5-turbo and Gemini-3B excel primarily at \"Easy\" and \"Medium\" levels but do not maintain high performance on harder questions. This specialization indicates that GPT-4 and Claude-v1 are better suited for general use, while other models are effective for simpler tasks but less reliable on difficult prompts.\n",
    "\n",
    "- **Do very easy and very hard prompts yield different winning models?**\n",
    "On \"Very Easy\" prompts, Claude-v1 outperforms GPT-4, which is the opposite of the trend observed at higher hardness levels where GPT-4 dominates. This suggests that while GPT-4 is robust across most hardness categories, Claude-v1 is particularly effective for simpler tasks and achieves higher win rates on very easy prompts compared to more challenging ones.\n",
    "\n",
    "- **How does the distribution of embedding distances change as hardness levels increase?**\n",
    "The distribution of embedding distances remains consistent across hardness levels, with a dense clustering between 0.6 and 1.0. This indicates that the model responses maintain a similar level of semantic alignment with the user prompts, regardless of the difficulty level. Both the violin and scatter plots confirm that hardness has a negligible effect on the semantic closeness of model responses to the prompts.\n",
    "\n",
    "- **Are there outliers in embedding distances, and do they vary across hardness levels?**\n",
    "Yes, there are occasional outliers with lower embedding distances across all hardness levels. These responses indicate instances where the response was less aligned with the prompt’s intended meaning, potentially suggesting irrelevant or contextually less fitting answers. However, these outliers are distributed across all hardness levels and do not suggest a strong pattern related to question difficulty.\n",
    "\n",
    "- **Does the hardness level impact the semantic alignment between the prompt and response embeddings?**\n",
    "While there is a very slight downward trend in embedding distances as hardness levels increase, indicating a minor improvement in alignment for harder prompts, the effect is minimal. Both the violin and scatter plots show that the majority of responses are within a stable embedding distance range across all levels of hardness, suggesting that hardness has little impact on the semantic alignment of responses with prompts.\n",
    "\n",
    "- **Which topics are most common, and do they align with specific hardness levels?**\n",
    "The most frequent topics are creative writing, factual accuracy, and problem-solving/creativity. The average hardness scores for these top topics are 8.2, 7.8, and 6.9, respectively, suggesting that creative writing and factual accuracy, the two most common topics, are also among the harder topics. This indicates that popular topics in the dataset tend to be associated with higher hardness scores, suggesting a correlation between topic type and complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7b4d86-2323-4796-aa21-7a25f3ff664e",
   "metadata": {},
   "source": [
    "## Modelling Task A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c0b58-0b7a-4ece-84f9-4defd4a9e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset dataframe \n",
    "conversation_and_topic_hardness = conversation.merge(topic_and_hardness, left_on = \"question_id\", right_on = \"question_id\").drop(columns = [\n",
    "    \"conversation_a\", \"conversation_b\"\n",
    "])\n",
    "\n",
    "conversation_and_topic_hardness.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a17b6a-3710-4cad-bc69-3c2556366157",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0272b600-44f4-49b7-85a9-916b19b0faab",
   "metadata": {},
   "source": [
    "Convert the topics into numerical features by using their normalized frequency. We can't one-hot encode the topics because they are too many i.e. 10k+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6857e9a0-1faa-4b76-ae49-3361c5090f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def ohe_for_models(data):\n",
    "    # Combine unique models from both 'model_a' and 'model_b'\n",
    "    unique_models = pd.concat([data['model_a'], data['model_b']]).unique().reshape(-1, 1)\n",
    "\n",
    "    # Initialize OneHotEncoder with sparse_output=False for compatibility with DataFrame\n",
    "    oh_enc = OneHotEncoder(sparse_output=False)  # sparse_output=False avoids sparse matrix output\n",
    "    oh_enc.fit(unique_models)\n",
    "\n",
    "    # Transform `model_a` and `model_b` separately, ensuring correct array shapes\n",
    "    model_a_encoded = oh_enc.transform(data[['model_a']])\n",
    "    model_b_encoded = oh_enc.transform(data[['model_b']])\n",
    "\n",
    "    # Combine with a Boolean OR to indicate presence in either column\n",
    "    oh_data_combined = (model_a_encoded + model_b_encoded) > 0\n",
    "    oh_data_combined = oh_data_combined.astype(int)  # Convert to integer\n",
    "\n",
    "    # Create DataFrame with appropriate feature names and align with the original index\n",
    "    oh_features = oh_enc.get_feature_names_out(['model'])\n",
    "    oh_df = pd.DataFrame(data=oh_data_combined, columns=oh_features, index=data.index)\n",
    "    \n",
    "    # Join the encoded columns back to the original DataFrame\n",
    "    data = data.join(oh_df)\n",
    "\n",
    "    return data\n",
    "\n",
    "def feature_engineering_pipeline(data):\n",
    "    # Frequency Encoding\n",
    "    topic_modeling_freq1 = data['topic_modeling_1'].value_counts(normalize=True)\n",
    "    data['topic_modeling_1_encoded'] = data['topic_modeling_1'].map(topic_modeling_freq1)\n",
    "\n",
    "    topic_modeling_freq2 = data['topic_modeling_2'].value_counts(normalize=True)\n",
    "    data['topic_modeling_2_encoded'] = data['topic_modeling_2'].map(topic_modeling_freq2)\n",
    "    \n",
    "    topic_modeling_freq3 = data['topic_modeling_3'].value_counts(normalize=True)\n",
    "    data['topic_modeling_3_encoded'] = data['topic_modeling_3'].map(topic_modeling_freq3)\n",
    "\n",
    "    # Expand the 256-dimension embeddings into 256 separate columns\n",
    "    user_prompt_df = pd.DataFrame(data['user_prompt_embedding'].tolist(), index=data.index)\n",
    "    user_prompt_df.columns = [f'user_prompt_embedding_{i}' for i in range(256)]\n",
    "    \n",
    "    model_a_response_df = pd.DataFrame(data['model_a_response_embedding'].tolist(), index=data.index)\n",
    "    model_a_response_df.columns = [f'model_a_response_embedding_{i}' for i in range(256)]\n",
    "    \n",
    "    model_b_response_df = pd.DataFrame(data['model_b_response_embedding'].tolist(), index=data.index)\n",
    "    model_b_response_df.columns = [f'model_b_response_embedding_{i}' for i in range(256)]\n",
    "    \n",
    "    data = pd.concat([data, user_prompt_df, model_a_response_df, model_b_response_df], axis=1)\n",
    "    \n",
    "    data = data.drop(['user_prompt_embedding', 'model_a_response_embedding', 'model_b_response_embedding'], axis=1)\n",
    "\n",
    "    data = ohe_for_models(data)\n",
    "\n",
    "    # Convert winner class from string to int\n",
    "    data['winner_class'] = data.apply(lambda row: 1 if row['winner'] == 'model_b' else  (0 if row['winner'] == 'model_a' else (-1 if row['winner'] == 'tie' else -2)), axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df83e73b-0bcc-416a-8ec5-52acd6a20d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_and_topic_hardness = feature_engineering_pipeline(conversation_and_topic_hardness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb00717-b443-47d1-874f-a1cd1f509304",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(conversation_and_topic_hardness.columns)\n",
    "display(conversation_and_topic_hardness.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b10765-7a80-448e-a627-f7844b23438f",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6320c059-4ee3-4f6b-9e52-2aeacb598502",
   "metadata": {},
   "source": [
    "Approaches tried so far with their accuracy percentage:\n",
    "1. Using avg of vector embeddings as one column - 36%\n",
    "2. Using separate columns for each vector embedding - 48.7%\n",
    "3. Using approach 2 with only data of <= 250 chars in user input - 50%\n",
    "4. Using approach 3 with L2 regularization and 5-fold cross validation - 49.1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f341a-0d7a-476f-8fe9-fa19d14f3400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fd20ca-4313-4028-b49a-49fa5b20f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_and_topic_hardness_250 = conversation_and_topic_hardness[conversation_and_topic_hardness['user_prompt_length'] <= 250]\n",
    "\n",
    "features = [\n",
    "    'topic_modeling_1_encoded',\n",
    "    'topic_modeling_2_encoded',\n",
    "    'topic_modeling_3_encoded',\n",
    "    'score_value_1',\n",
    "    'score_value_2',\n",
    "    'score_value_3',\n",
    "]\n",
    "features += [col for col in conversation_and_topic_hardness_250.columns if col.startswith(\"model_\") and col not in [\"model_a\", \"model_b\"] and not col.startswith(\"model_a_\") and not col.startswith(\"model_b_\")]\n",
    "features += [col for col in conversation_and_topic_hardness_250.columns if col.startswith(\"user_prompt_embedding_\") or col.startswith(\"model_a_response_embedding_\") or col.startswith(\"model_b_response_embedding_\")]\n",
    "\n",
    "X = conversation_and_topic_hardness_250[features]\n",
    "y = conversation_and_topic_hardness_250['winner_class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdba125-d5c3-453d-9e9a-28b5cc2c62f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conversation_and_topic_hardness_250 = conversation_and_topic_hardness[conversation_and_topic_hardness['user_prompt_length'] <= 250]\n",
    "features = [\n",
    "    'topic_modeling_1_encoded',\n",
    "    'topic_modeling_2_encoded',\n",
    "    'topic_modeling_3_encoded',\n",
    "    'score_value_1',\n",
    "    'score_value_2',\n",
    "    'score_value_3',\n",
    "]\n",
    "features += [col for col in conversation_and_topic_hardness.columns if col.startswith(\"model_\") and col not in [\"model_a\", \"model_b\"] and not col.startswith(\"model_a_\") and not col.startswith(\"model_b_\")]\n",
    "features += [col for col in conversation_and_topic_hardness.columns if col.startswith(\"user_prompt_embedding_\") or col.startswith(\"model_a_response_embedding_\") or col.startswith(\"model_b_response_embedding_\")]\n",
    "\n",
    "X = conversation_and_topic_hardness[features]\n",
    "y = conversation_and_topic_hardness['winner_class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegressionCV(\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    max_iter=1000,  # Increased iteration limit for convergence\n",
    "    penalty='l2',  # L2 regularization; you can also test 'l1'\n",
    "    scoring='accuracy',  # Use accuracy as the scoring metric\n",
    "    solver='lbfgs',  # Solver for logistic regression; switch to 'saga' if using L1\n",
    "    verbose=2  # Set verbose level to see progress\n",
    ")\n",
    "\n",
    "# Fit model with training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f12560-c560-4da9-8250-a9964dee58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check out tie cases because the model struggles on them\n",
    "# TODO: check out random forests model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44359622-2a8b-4dce-a761-29418839d4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
